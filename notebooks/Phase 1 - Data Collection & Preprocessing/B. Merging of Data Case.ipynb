{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "---\n## 8. Key Observations & Next Steps\n\n### ‚úÖ What We Achieved:\n\n1. **Concatenation Approach**: Combined LogFile and UsnJrnl without creating duplicate records\n2. **No Data Loss**: Every event from both artifacts preserved as separate rows\n3. **Separate Label Columns**: Track detection from both artifacts:\n   - `is_timestomped_lf` and `is_timestomped_usn`\n   - `is_suspicious_lf` and `is_suspicious_usn`\n   - Plus unified versions: `is_timestomped`, `is_suspicious`\n4. **Source Tracking**: `source_artifact` column identifies whether row came from LogFile or UsnJrnl\n5. **Perfect Preservation**: All suspicious records maintained (no duplicates, no losses)\n\n### üìä Column Structure in Merged Files:\n\n```\ntimestamp          - Normalized timestamp\nfilepath           - File path\nfilename           - File name\nsource_artifact    - 'logfile' or 'usnjrnl' (identifies source)\nlf_lsn             - LogFile LSN (NaN for UsnJrnl rows)\nlf_event           - LogFile event type (NaN for UsnJrnl rows)\n... (other lf_* columns)\nusn_usn            - UsnJrnl USN (NaN for LogFile rows)\nusn_event_info     - UsnJrnl event info (NaN for LogFile rows)\n... (other usn_* columns)\nis_timestomped_lf  - Timestomping in LogFile (NaN for UsnJrnl-only rows)\nis_timestomped_usn - Timestomping in UsnJrnl (NaN for LogFile-only rows)\nis_suspicious_lf   - Suspicious in LogFile (NaN for UsnJrnl-only rows)\nis_suspicious_usn  - Suspicious in UsnJrnl (NaN for LogFile-only rows)\nis_timestomped     - Unified timestomping flag\nis_suspicious      - Unified suspicious flag\nlabel_source       - Where labels came from\ncase_id            - Case identifier\n```\n\n### üéØ Why Concatenation Instead of Join?\n\n**Problem with Join Approach:**\n- UsnJrnl can have multiple events at same timestamp for same file (e.g., File_Created ‚Üí Data_Added ‚Üí File_Closed)\n- Join created **duplicates**: 1 LogFile record √ó 3 UsnJrnl records = 3 rows with same LogFile data\n- This inflated suspicious counts incorrectly\n\n**Solution with Concatenation:**\n- Each event is a separate row (no duplicates)\n- LogFile events have LogFile columns filled, UsnJrnl columns NaN\n- UsnJrnl events have UsnJrnl columns filled, LogFile columns NaN\n- `source_artifact` column clearly identifies the source\n\n### üìÅ Output Files:\n\nAll merged files saved to:\n`data/processed/Phase 1 - Data Collection & Preprocessing/B. Data Case Merging/XX-PE-Merged.csv`\n\n### ‚û°Ô∏è Next Steps:\n\n**Phase 1C: Master Timeline Creation**\n- Concatenate all 12 merged case files into single master timeline\n- Sort by timestamp for chronological analysis\n- Prepare for feature engineering (Phase 2)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Load merged 01-PE to verify\nsample_merged = pd.read_csv(OUTPUT_DIR / \"01-PE-Merged.csv\", encoding='utf-8-sig')\n\n# Find NEWFILETIME_X64.EXE-6C60D39A.pf records\nnewfiletime_records = sample_merged[sample_merged['filename'] == 'NEWFILETIME_X64.EXE-6C60D39A.pf']\n\nprint(\"=\" * 80)\nprint(\"VERIFICATION: NEWFILETIME_X64.EXE-6C60D39A.pf Records\")\nprint(\"=\" * 80)\nprint(f\"\\\\nTotal records for this file: {len(newfiletime_records)}\")\nprint(f\"\\\\nMerge type breakdown:\")\nprint(newfiletime_records['merge_type'].value_counts())\n\n# Show matched record (if exists)\nmatched = newfiletime_records[newfiletime_records['merge_type'] == 'matched']\nif len(matched) > 0:\n    print(f\"\\\\n‚úÖ SUCCESS! Found {len(matched)} MATCHED record(s) with data from BOTH LogFile and UsnJrnl:\")\n    display(matched[['timestamp', 'filename', 'lf_lsn', 'usn_usn', 'lf_event', 'usn_event_info', \n                     'merge_type', 'is_timestomped_lf', 'is_timestomped_usn', 'is_suspicious']])\nelse:\n    print(\"\\\\n‚ö†Ô∏è No matched records found\")\n\nprint(f\"\\\\nüìã All records for this file:\")\ndisplay(newfiletime_records[['timestamp', 'filename', 'lf_lsn', 'usn_usn', 'lf_event', 'usn_event_info', 'merge_type']])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 7. Verify Example: NEWFILETIME_X64.EXE-6C60D39A.pf\n\nLet's verify that the problematic file mentioned at the start is now properly merged!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualizations\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\nx = np.arange(len(stats_df))\nwidth = 0.35\n\n# Plot 1: Before vs After comparison\nbefore = stats_df['lf_records_before'] + stats_df['usn_records_before']\nafter = stats_df['total_records_after']\n\naxes[0, 0].bar(x - width/2, before, width, label='Before Merge (LF+USN)', color='lightcoral', alpha=0.7)\naxes[0, 0].bar(x + width/2, after, width, label='After Merge', color='lightgreen', alpha=0.7)\naxes[0, 0].set_xticks(x)\naxes[0, 0].set_xticklabels(stats_df['case_id'], rotation=45)\naxes[0, 0].set_xlabel('Case ID', fontsize=11)\naxes[0, 0].set_ylabel('Record Count', fontsize=11)\naxes[0, 0].set_title('Dataset Size Reduction per Case', fontsize=13, fontweight='bold')\naxes[0, 0].legend()\naxes[0, 0].grid(axis='y', alpha=0.3)\n\n# Plot 2: Merge type breakdown (stacked)\naxes[0, 1].bar(x, stats_df['matched_events'], label='Matched (Both)', color='#4CAF50', alpha=0.7)\naxes[0, 1].bar(x, stats_df['logfile_only'], bottom=stats_df['matched_events'], \n               label='LogFile Only', color='#FF9800', alpha=0.7)\naxes[0, 1].bar(x, stats_df['usnjrnl_only'], \n               bottom=stats_df['matched_events'] + stats_df['logfile_only'],\n               label='UsnJrnl Only', color='#2196F3', alpha=0.7)\naxes[0, 1].set_xticks(x)\naxes[0, 1].set_xticklabels(stats_df['case_id'], rotation=45)\naxes[0, 1].set_xlabel('Case ID', fontsize=11)\naxes[0, 1].set_ylabel('Record Count', fontsize=11)\naxes[0, 1].set_title('Merge Type Distribution per Case', fontsize=13, fontweight='bold')\naxes[0, 1].legend()\naxes[0, 1].grid(axis='y', alpha=0.3)\n\n# Plot 3: Suspicious record preservation\naxes[1, 0].bar(x - width/2, stats_df['total_suspicious_before'], width, \n               label='Before Merge', color='#ff6b6b', alpha=0.7)\naxes[1, 0].bar(x + width/2, stats_df['suspicious_after'], width, \n               label='After Merge', color='#51cf66', alpha=0.7)\naxes[1, 0].set_xticks(x)\naxes[1, 0].set_xticklabels(stats_df['case_id'], rotation=45)\naxes[1, 0].set_xlabel('Case ID', fontsize=11)\naxes[1, 0].set_ylabel('Suspicious Count', fontsize=11)\naxes[1, 0].set_title('‚ö†Ô∏è Suspicious Record Preservation', fontsize=13, fontweight='bold')\naxes[1, 0].legend()\naxes[1, 0].grid(axis='y', alpha=0.3)\n\n# Plot 4: Suspicious event coverage breakdown (pie chart)\ncoverage_data = [total_susp_matched, total_susp_lf_only, total_susp_usn_only]\nlabels = ['Matched\\\\n(Full Row)', 'LogFile Only', 'UsnJrnl Only']\ncolors = ['#4CAF50', '#FF9800', '#2196F3']\nexplode = (0.1, 0, 0)\n\naxes[1, 1].pie(coverage_data, labels=labels, autopct='%1.1f%%', startangle=90,\n               colors=colors, explode=explode, shadow=True)\naxes[1, 1].set_title('Suspicious Event Coverage Distribution', fontsize=13, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\\\nüìä Visualizations complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 6. Visualizations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create summary DataFrame\nstats_df = pd.DataFrame(all_merge_stats)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"MERGE SUMMARY - ALL CASES (Concatenation)\")\nprint(\"=\" * 80)\nprint(\"\\nüìä Per-Case Statistics:\")\ndisplay(stats_df)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"OVERALL STATISTICS\")\nprint(\"=\" * 80)\n\n# Calculate totals\ntotal_lf_before = stats_df['lf_records_before'].sum()\ntotal_usn_before = stats_df['usn_records_before'].sum()\ntotal_after = stats_df['total_records_after'].sum()\n\nprint(f\"\\nüìà Record Statistics:\")\nprint(f\"   LogFile records:       {total_lf_before:,}\")\nprint(f\"   UsnJrnl records:       {total_usn_before:,}\")\nprint(f\"   Total after concat:    {total_after:,}\")\nprint(f\"   (Should equal sum):    {total_lf_before + total_usn_before:,} {'‚úÖ' if total_after == total_lf_before + total_usn_before else '‚ùå'}\")\n\n# Suspicious record preservation\ntotal_suspicious_before = stats_df['total_suspicious_before'].sum()\ntotal_suspicious_after = stats_df['suspicious_after'].sum()\ncases_preserved = stats_df['suspicious_preserved'].sum()\n\nprint(f\"\\n‚ö†Ô∏è SUSPICIOUS RECORD PRESERVATION:\")\nprint(f\"   Total suspicious before: {total_suspicious_before}\")\nprint(f\"   Total suspicious after:  {total_suspicious_after}\")\nprint(f\"   Match status:            {'‚úÖ PERFECT MATCH' if total_suspicious_after == total_suspicious_before else '‚ùå MISMATCH'}\")\nprint(f\"   Cases preserved exactly: {cases_preserved}/{len(stats_df)}\")\n\nif total_suspicious_after != total_suspicious_before:\n    print(f\"   ‚ö†Ô∏è WARNING: Expected {total_suspicious_before} but got {total_suspicious_after}!\")\n\nprint(f\"\\nüí° KEY INSIGHTS:\")\nprint(f\"   1. Concatenation approach: Each event kept as separate row\")\nprint(f\"   2. No duplicate LogFile records from one-to-many joins\")\nprint(f\"   3. All {total_suspicious_before} suspicious records preserved\")\nprint(f\"   4. Dataset has both LogFile and UsnJrnl events (identified by 'source_artifact' column)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 5. Merge Summary & Analytics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Process all cases\nall_merge_stats = []\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"PROCESSING ALL CASES (Concatenation Approach)\")\nprint(\"=\" * 80)\n\nfor case_id in CASE_IDS:\n    print(f\"\\n[{case_id}] Processing...\")\n    \n    try:\n        # Merge case\n        merged_df, stats = merge_case(case_id, LABELLED_DIR)\n        \n        print(f\"  Records: {stats['lf_records_before']:,} + {stats['usn_records_before']:,} = {stats['total_records_after']:,}\")\n        print(f\"  Suspicious: {stats['total_suspicious_before']} ‚Üí {stats['suspicious_after']} {'‚úÖ' if stats['suspicious_preserved'] else '‚ùå'}\")\n        \n        # Save merged dataset\n        output_path = OUTPUT_DIR / f\"{case_id}-Merged.csv\"\n        merged_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n        print(f\"  ‚úì Saved: {output_path.name}\")\n        \n        # Store statistics\n        all_merge_stats.append(stats)\n        \n    except Exception as e:\n        print(f\"  ‚úó Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        continue\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úì ALL CASES PROCESSED\")\nprint(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 4. Process All Cases",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test merge on case 01-PE\nprint(\"Testing merge on case 01-PE...\\n\")\n\ntest_merged, test_stats = merge_case('01-PE', LABELLED_DIR)\n\nprint(\"=\" * 80)\nprint(\"MERGE TEST RESULTS: 01-PE (Concatenation Approach)\")\nprint(\"=\" * 80)\n\nprint(f\"\\nüìä Record Counts:\")\nprint(f\"   LogFile before:        {test_stats['lf_records_before']:,}\")\nprint(f\"   UsnJrnl before:        {test_stats['usn_records_before']:,}\")\nprint(f\"   Total after merge:     {test_stats['total_records_after']:,}\")\nprint(f\"   From LogFile:          {test_stats['from_logfile']:,}\")\nprint(f\"   From UsnJrnl:          {test_stats['from_usnjrnl']:,}\")\n\nprint(f\"\\n‚ö†Ô∏è SUSPICIOUS RECORD PRESERVATION:\")\nprint(f\"   LogFile suspicious before:  {test_stats['lf_suspicious_before']}\")\nprint(f\"   UsnJrnl suspicious before:  {test_stats['usn_suspicious_before']}\")\nprint(f\"   Total suspicious before:    {test_stats['total_suspicious_before']}\")\nprint(f\"   Suspicious after merge:     {test_stats['suspicious_after']}\")\nprint(f\"   Status: {'‚úÖ PRESERVED (EXACT MATCH)' if test_stats['suspicious_preserved'] else '‚ùå MISMATCH!'}\")\n\nif not test_stats['suspicious_preserved']:\n    print(f\"   ‚ö†Ô∏è WARNING: Expected {test_stats['total_suspicious_before']} but got {test_stats['suspicious_after']}!\")\n\nprint(f\"\\nüìã Sample of merged data:\")\ndisplay(test_merged[['timestamp', 'filepath', 'filename', 'source_artifact', \n                      'lf_lsn', 'usn_usn', 'lf_event', 'usn_event_info',\n                      'is_timestomped_lf', 'is_timestomped_usn', 'is_suspicious']].head(15))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 3. Test Merge on Sample Case (01-PE)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def merge_case(case_id, labelled_dir):\n    \"\"\"\n    Merge LogFile and UsnJrnl for a single case using CONCATENATION approach.\n    This prevents duplicate records from one-to-many relationships.\n    Returns: (merged_df, merge_stats_dict)\n    \"\"\"\n    # Load labelled datasets\n    logfile_path = labelled_dir / f\"{case_id}-LogFile-Labelled.csv\"\n    usnjrnl_path = labelled_dir / f\"{case_id}-UsnJrnl-Labelled.csv\"\n    \n    logfile_df = pd.read_csv(logfile_path, encoding='utf-8-sig')\n    usnjrnl_df = pd.read_csv(usnjrnl_path, encoding='utf-8-sig')\n    \n    # Store original counts\n    lf_suspicious_before = logfile_df['is_suspicious'].sum()\n    usn_suspicious_before = usnjrnl_df['is_suspicious'].sum()\n    total_suspicious_before = lf_suspicious_before + usn_suspicious_before\n    \n    # Prepare for merge with normalized timestamps\n    lf_prepared = prepare_logfile_for_merge(logfile_df)\n    usn_prepared = prepare_usnjrnl_for_merge(usnjrnl_df)\n    \n    # Add source identifier\n    lf_prepared['source_artifact'] = 'logfile'\n    usn_prepared['source_artifact'] = 'usnjrnl'\n    \n    # Concatenate instead of merge to avoid duplicates\n    # This keeps each event as a separate row\n    merged = pd.concat([lf_prepared, usn_prepared], ignore_index=True)\n    \n    # Sort by timestamp and filepath for better organization\n    merged = merged.sort_values(['timestamp', 'filepath'], na_position='last').reset_index(drop=True)\n    \n    # Create unified label columns (already exist from preparation)\n    # For concatenation, just rename back to standard names\n    merged['is_timestomped'] = merged['is_timestomped_lf'].fillna(merged['is_timestomped_usn'])\n    merged['is_suspicious_execution'] = merged['is_suspicious_execution_lf'].fillna(merged['is_suspicious_execution_usn'])\n    merged['is_suspicious'] = merged['is_suspicious_lf'].fillna(merged['is_suspicious_usn'])\n    merged['label_source'] = merged['label_source_lf'].fillna(merged['label_source_usn'])\n    \n    # Add case_id\n    merged['case_id'] = case_id\n    \n    # Gather statistics\n    suspicious_after = merged['is_suspicious'].sum()\n    \n    merge_stats = {\n        'case_id': case_id,\n        'lf_records_before': len(logfile_df),\n        'usn_records_before': len(usnjrnl_df),\n        'total_records_after': len(merged),\n        'lf_suspicious_before': int(lf_suspicious_before),\n        'usn_suspicious_before': int(usn_suspicious_before),\n        'total_suspicious_before': int(total_suspicious_before),\n        'suspicious_after': int(suspicious_after),\n        'suspicious_preserved': suspicious_after == total_suspicious_before,\n        'from_logfile': len(merged[merged['source_artifact'] == 'logfile']),\n        'from_usnjrnl': len(merged[merged['source_artifact'] == 'usnjrnl'])\n    }\n    \n    return merged, merge_stats\n\n\nprint(\"‚úì Merge function defined (concatenation approach)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def normalize_timestamp(ts):\n    \"\"\"\n    Normalize timestamp to consistent format with leading zeros.\n    Fixes the issue where '12/23/23 0:21:57' != '12/23/23 00:21:57'\n    \"\"\"\n    if pd.isna(ts) or ts == '':\n        return ts\n    \n    try:\n        # Parse and reformat with leading zeros\n        dt = pd.to_datetime(ts, format='%m/%d/%y %H:%M:%S', errors='coerce')\n        if pd.isna(dt):\n            dt = pd.to_datetime(ts, errors='coerce')\n        if pd.notna(dt):\n            return dt.strftime('%m/%d/%y %H:%M:%S')\n        return ts\n    except:\n        return ts\n\n\ndef prepare_logfile_for_merge(df):\n    \"\"\"\n    Prepare LogFile for merging: \n    - Normalize timestamps\n    - Add 'lf_' prefix to columns\n    - Keep separate label columns\n    \"\"\"\n    prepared = df.copy()\n    \n    # Standardize join keys with normalized timestamp\n    prepared['timestamp'] = prepared['EventTime(UTC+8)'].apply(normalize_timestamp)\n    prepared['filepath'] = prepared['Full Path']\n    prepared['filename'] = prepared['File/Directory Name']\n    \n    # Rename with 'lf_' prefix\n    rename_map = {\n        'LSN': 'lf_lsn',\n        'Event': 'lf_event',\n        'Detail': 'lf_detail',\n        'CreationTime': 'lf_creation_time',\n        'ModifiedTime': 'lf_modified_time',\n        'MFTModifiedTime': 'lf_mft_modified_time',\n        'AccessedTime': 'lf_accessed_time',\n        'Redo': 'lf_redo',\n        'Target VCN': 'lf_target_vcn',\n        'Cluster Index': 'lf_cluster_index',\n        # Keep label columns separate with lf_ prefix\n        'is_timestomped': 'is_timestomped_lf',\n        'is_suspicious_execution': 'is_suspicious_execution_lf',\n        'is_suspicious': 'is_suspicious_lf',\n        'label_source': 'label_source_lf'\n    }\n    prepared = prepared.rename(columns=rename_map)\n    prepared = prepared.drop(['EventTime(UTC+8)', 'Full Path', 'File/Directory Name'], axis=1, errors='ignore')\n    \n    return prepared\n\n\ndef prepare_usnjrnl_for_merge(df):\n    \"\"\"\n    Prepare UsnJrnl for merging:\n    - Normalize timestamps\n    - Add 'usn_' prefix to columns\n    - Keep separate label columns\n    \"\"\"\n    prepared = df.copy()\n    \n    # Standardize join keys with normalized timestamp\n    prepared['timestamp'] = prepared['TimeStamp(UTC+8)'].apply(normalize_timestamp)\n    prepared['filepath'] = prepared['FullPath']\n    prepared['filename'] = prepared['File/Directory Name']\n    \n    # Rename with 'usn_' prefix\n    rename_map = {\n        'USN': 'usn_usn',\n        'EventInfo': 'usn_event_info',\n        'SourceInfo': 'usn_source_info',\n        'FileAttribute': 'usn_file_attribute',\n        'Carving Flag': 'usn_carving_flag',\n        'FileReferenceNumber': 'usn_file_reference_number',\n        'ParentFileReferenceNumber': 'usn_parent_file_reference_number',\n        # Keep label columns separate with usn_ prefix\n        'is_timestomped': 'is_timestomped_usn',\n        'is_suspicious_execution': 'is_suspicious_execution_usn',\n        'is_suspicious': 'is_suspicious_usn',\n        'label_source': 'label_source_usn'\n    }\n    prepared = prepared.rename(columns=rename_map)\n    prepared = prepared.drop(['TimeStamp(UTC+8)', 'FullPath', 'File/Directory Name'], axis=1, errors='ignore')\n    \n    return prepared\n\n\nprint(\"‚úì Preparation functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 2. Merge Functions with Timestamp Normalization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Case IDs\nCASE_IDS = [f\"{i:02d}-PE\" for i in range(1, 13)]\nprint(f\"Processing {len(CASE_IDS)} cases: {', '.join(CASE_IDS)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Define paths\nnotebook_dir = Path.cwd()\nprint(f\"Current working directory: {notebook_dir}\")\n\n# Navigate to project root\nif 'notebooks' in str(notebook_dir):\n    BASE_DIR = notebook_dir.parent.parent / 'data'\nelse:\n    BASE_DIR = Path('data')\n\nLABELLED_DIR = BASE_DIR / 'processed' / 'Phase 1 - Data Collection & Preprocessing' / 'A. Data Labelled'\nOUTPUT_DIR = BASE_DIR / 'processed' / 'Phase 1 - Data Collection & Preprocessing' / 'B. Data Case Merging'\n\n# Ensure output directory exists\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"\\nüìÇ Directory Configuration:\")\nprint(f\"  Input:  {LABELLED_DIR} {'‚úì' if LABELLED_DIR.exists() else '‚úó NOT FOUND'}\")\nprint(f\"  Output: {OUTPUT_DIR} ‚úì\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\n\nprint(\"‚úì Libraries imported successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 1. Setup & Imports",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# Phase 1B: Data Case Merging\n\n## Objective\nMerge LogFile and UsnJrnl labelled artifacts per case by **intelligently joining** rows that represent the same forensic event.\n\n## Problem Solved\nPreviously, the same file system event recorded in **both** LogFile and UsnJrnl appeared as **two separate rows**:\n- LogFile row: `12/23/23 0:21:57 | \\Windows\\Prefetch\\NEWFILETIME_X64.EXE-6C60D39A.pf`\n- UsnJrnl row: `12/23/23 00:21:57 | \\Windows\\Prefetch\\NEWFILETIME_X64.EXE-6C60D39A.pf`\n\n**Issue**: Timestamps formatted differently (`0:21:57` vs `00:21:57`) prevented matching!\n\n## Solution: Intelligent Join with Timestamp Normalization\n‚úÖ Normalize timestamps to consistent format  \n‚úÖ Merge rows with same timestamp + filepath  \n‚úÖ Keep **separate** label columns for each artifact (`is_timestomped_lf`, `is_timestomped_usn`)  \n‚úÖ Track where suspicious activity was detected  \n\n## Expected Results\n- **Significant dataset reduction**: ~4M ‚Üí ~350K rows\n- **Better data quality**: Matched events have complete LogFile + UsnJrnl data\n- **Preserved detection information**: Know which artifact detected each suspicious activity\n\n## Example Merged Row\n```\ntimestamp         | lf_lsn     | usn_usn    | filename                         | filepath                                     | lf_event      | usn_event_info                    | is_timestomped_lf | is_timestomped_usn | ...\n12/23/23 00:21:57 | 8729569062 | 1327928416 | NEWFILETIME_X64.EXE-6C60D39A.pf | \\Windows\\Prefetch\\NEWFILETIME_X64.EXE-...pf | File Creation | File_Created / Data_Added / ...   | 0                 | 1                  | ...\n```",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}