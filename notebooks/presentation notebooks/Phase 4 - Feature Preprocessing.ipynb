{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8821cd6",
   "metadata": {},
   "source": [
    "# Phase 4 - Feature Preprocessing\n",
    "\n",
    "## üéØ Objective\n",
    "Prepare the engineered features for machine learning model training by:\n",
    "1. **Handling outliers** in time delta features\n",
    "2. **Encoding categorical variables** (event_type, file_extension)\n",
    "3. **Scaling numerical features** for model optimization\n",
    "4. **Splitting data** into Train/Validation/Test sets (stratified by case_id)\n",
    "\n",
    "## üìä Input Dataset\n",
    "- **File:** `data/processed/Phase 3 - Feature Engineering/Master_Timeline_Features.csv`\n",
    "- **Records:** 2,264,521\n",
    "- **Features:** 44 (25 original + 19 engineered)\n",
    "- **Labeled rows:** 268 (252 timestomped + 16 suspicious)\n",
    "\n",
    "## üîë Key Preprocessing Steps\n",
    "1. **Outlier Clipping:** Time deltas range ¬±24 years ‚Üí clip to ¬±10 years\n",
    "2. **Categorical Encoding:** Group rare file extensions, encode event types\n",
    "3. **Feature Scaling:** StandardScaler for numerical features only\n",
    "4. **Stratified Split:** 70/15/15 by case_id (prevents data leakage)\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "- **Class imbalance is NOT a concern:** Isolation Forest is unsupervised (labels only for evaluation)\n",
    "- **Stratify by case_id:** Ensures model tested on completely unseen forensic cases\n",
    "- **Preserve all 268 labeled rows** across splits for proper evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e0b96ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "Pandas version: 2.3.2\n",
      "NumPy version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438912c",
   "metadata": {},
   "source": [
    "## 1. Load Feature-Engineered Dataset\n",
    "\n",
    "Loading the output from Phase 3 with all 44 features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e9beec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data/processed/Phase 3 - Feature Engineering/Master_Timeline_Features.csv\n",
      "\n",
      "‚úÖ Dataset loaded successfully!\n",
      "Shape: (2264521, 44)\n",
      "Memory usage: 2313.94 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>timestamp_primary</th>\n",
       "      <th>source</th>\n",
       "      <th>fullpath</th>\n",
       "      <th>file/directory name</th>\n",
       "      <th>is_timestomped</th>\n",
       "      <th>is_suspicious_execution</th>\n",
       "      <th>lsn</th>\n",
       "      <th>eventtime(utc+8)</th>\n",
       "      <th>event</th>\n",
       "      <th>detail</th>\n",
       "      <th>creationtime</th>\n",
       "      <th>modifiedtime</th>\n",
       "      <th>mftmodifiedtime</th>\n",
       "      <th>accessedtime</th>\n",
       "      <th>redo</th>\n",
       "      <th>target vcn</th>\n",
       "      <th>cluster index</th>\n",
       "      <th>has_incomplete_timestamps</th>\n",
       "      <th>timestamp(utc+8)</th>\n",
       "      <th>usn</th>\n",
       "      <th>eventinfo</th>\n",
       "      <th>fileattribute</th>\n",
       "      <th>filereferencenumber</th>\n",
       "      <th>parentfilereferencenumber</th>\n",
       "      <th>Delta_MFTM_vs_M</th>\n",
       "      <th>Delta_M_vs_C</th>\n",
       "      <th>Delta_C_vs_A</th>\n",
       "      <th>Delta_Event_vs_M</th>\n",
       "      <th>Delta_Event_vs_MFTM</th>\n",
       "      <th>Delta_Event_vs_C</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_night</th>\n",
       "      <th>is_business_hours</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>path_depth</th>\n",
       "      <th>is_system_file</th>\n",
       "      <th>is_logfile</th>\n",
       "      <th>event_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-01 08:00:00</td>\n",
       "      <td>LogFile</td>\n",
       "      <td>\\Program Files (x86)\\Dropbox\\Client\\189.4.8395...</td>\n",
       "      <td>style.js</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8715606672.00</td>\n",
       "      <td>2000-01-01 08:00:00</td>\n",
       "      <td>Updating MFTModified Time</td>\n",
       "      <td>MFTModifiedTime : 2023-12-23 00:14:24 -&gt; 2023-...</td>\n",
       "      <td>2023-12-23 00:14:24</td>\n",
       "      <td>2000-01-01 08:00:00</td>\n",
       "      <td>2023-12-23 00:14:52</td>\n",
       "      <td>2023-12-23 00:14:24</td>\n",
       "      <td>Update Resident Value</td>\n",
       "      <td>0x174F8</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>756576892.00</td>\n",
       "      <td>-756576864.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-756576892.00</td>\n",
       "      <td>-756576864.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>js</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Updating MFTModified Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-01 08:00:00</td>\n",
       "      <td>LogFile</td>\n",
       "      <td>\\Program Files (x86)\\Dropbox\\Client\\189.4.8395...</td>\n",
       "      <td>CalendarUtils.js</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8724384553.00</td>\n",
       "      <td>2000-01-01 08:00:00</td>\n",
       "      <td>Updating MFTModified Time</td>\n",
       "      <td>MFTModifiedTime : 2023-12-23 00:14:24 -&gt; 2023-...</td>\n",
       "      <td>2023-12-23 00:14:24</td>\n",
       "      <td>2000-01-01 08:00:00</td>\n",
       "      <td>2023-12-23 00:15:26</td>\n",
       "      <td>2023-12-23 00:14:24</td>\n",
       "      <td>Update Resident Value</td>\n",
       "      <td>0x174F3</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>756576926.00</td>\n",
       "      <td>-756576864.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-756576926.00</td>\n",
       "      <td>-756576864.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>js</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Updating MFTModified Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2000-01-01 08:00:00</td>\n",
       "      <td>LogFile</td>\n",
       "      <td>\\Program Files (x86)\\Dropbox\\Client\\189.4.8395...</td>\n",
       "      <td>StackView.js</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8724811401.00</td>\n",
       "      <td>2000-01-01 08:00:00</td>\n",
       "      <td>Updating MFTModified Time</td>\n",
       "      <td>MFTModifiedTime : 2023-12-23 00:14:24 -&gt; 2023-...</td>\n",
       "      <td>2023-12-23 00:14:24</td>\n",
       "      <td>2000-01-01 08:00:00</td>\n",
       "      <td>2023-12-23 00:16:08</td>\n",
       "      <td>2023-12-23 00:14:24</td>\n",
       "      <td>Update Resident Value</td>\n",
       "      <td>0x174F8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>756576968.00</td>\n",
       "      <td>-756576864.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-756576968.00</td>\n",
       "      <td>-756576864.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>js</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Updating MFTModified Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-10-11 14:08:00</td>\n",
       "      <td>LogFile</td>\n",
       "      <td>\\Users\\blueangel\\AppData\\Local\\Temp\\RarSFX1\\Wi...</td>\n",
       "      <td>WinHex.exe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8724891080.00</td>\n",
       "      <td>2010-10-11 14:08:00</td>\n",
       "      <td>Updating MFTModified Time</td>\n",
       "      <td>MFTModifiedTime : 2023-12-23 00:16:12 -&gt; 2023-...</td>\n",
       "      <td>2023-12-23 00:16:12</td>\n",
       "      <td>2010-10-11 14:08:00</td>\n",
       "      <td>2023-12-23 00:16:13</td>\n",
       "      <td>2023-12-23 00:16:13</td>\n",
       "      <td>Update Resident Value</td>\n",
       "      <td>0x7E20</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416484493.00</td>\n",
       "      <td>-416484492.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-416484493.00</td>\n",
       "      <td>-416484492.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2010.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>exe</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Updating MFTModified Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-10-11 14:08:00</td>\n",
       "      <td>LogFile</td>\n",
       "      <td>\\Users\\blueangel\\AppData\\Local\\Temp\\RarSFX1\\se...</td>\n",
       "      <td>setup.exe</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8725322068.00</td>\n",
       "      <td>2010-10-11 14:08:00</td>\n",
       "      <td>Updating MFTModified Time</td>\n",
       "      <td>MFTModifiedTime : 2023-12-23 00:16:12 -&gt; 2023-...</td>\n",
       "      <td>2023-12-23 00:16:12</td>\n",
       "      <td>2010-10-11 14:08:00</td>\n",
       "      <td>2023-12-23 00:16:17</td>\n",
       "      <td>2023-12-23 00:16:12</td>\n",
       "      <td>Update Resident Value</td>\n",
       "      <td>0x7E1F</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>416484497.00</td>\n",
       "      <td>-416484492.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-416484497.00</td>\n",
       "      <td>-416484492.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2010.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>exe</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Updating MFTModified Time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id   timestamp_primary   source  \\\n",
       "0        1 2000-01-01 08:00:00  LogFile   \n",
       "1        1 2000-01-01 08:00:00  LogFile   \n",
       "2        1 2000-01-01 08:00:00  LogFile   \n",
       "3        1 2010-10-11 14:08:00  LogFile   \n",
       "4        1 2010-10-11 14:08:00  LogFile   \n",
       "\n",
       "                                            fullpath file/directory name  \\\n",
       "0  \\Program Files (x86)\\Dropbox\\Client\\189.4.8395...            style.js   \n",
       "1  \\Program Files (x86)\\Dropbox\\Client\\189.4.8395...    CalendarUtils.js   \n",
       "2  \\Program Files (x86)\\Dropbox\\Client\\189.4.8395...        StackView.js   \n",
       "3  \\Users\\blueangel\\AppData\\Local\\Temp\\RarSFX1\\Wi...          WinHex.exe   \n",
       "4  \\Users\\blueangel\\AppData\\Local\\Temp\\RarSFX1\\se...           setup.exe   \n",
       "\n",
       "   is_timestomped  is_suspicious_execution           lsn    eventtime(utc+8)  \\\n",
       "0               0                        0 8715606672.00 2000-01-01 08:00:00   \n",
       "1               0                        0 8724384553.00 2000-01-01 08:00:00   \n",
       "2               0                        0 8724811401.00 2000-01-01 08:00:00   \n",
       "3               0                        0 8724891080.00 2010-10-11 14:08:00   \n",
       "4               0                        0 8725322068.00 2010-10-11 14:08:00   \n",
       "\n",
       "                       event  \\\n",
       "0  Updating MFTModified Time   \n",
       "1  Updating MFTModified Time   \n",
       "2  Updating MFTModified Time   \n",
       "3  Updating MFTModified Time   \n",
       "4  Updating MFTModified Time   \n",
       "\n",
       "                                              detail        creationtime  \\\n",
       "0  MFTModifiedTime : 2023-12-23 00:14:24 -> 2023-... 2023-12-23 00:14:24   \n",
       "1  MFTModifiedTime : 2023-12-23 00:14:24 -> 2023-... 2023-12-23 00:14:24   \n",
       "2  MFTModifiedTime : 2023-12-23 00:14:24 -> 2023-... 2023-12-23 00:14:24   \n",
       "3  MFTModifiedTime : 2023-12-23 00:16:12 -> 2023-... 2023-12-23 00:16:12   \n",
       "4  MFTModifiedTime : 2023-12-23 00:16:12 -> 2023-... 2023-12-23 00:16:12   \n",
       "\n",
       "         modifiedtime     mftmodifiedtime        accessedtime  \\\n",
       "0 2000-01-01 08:00:00 2023-12-23 00:14:52 2023-12-23 00:14:24   \n",
       "1 2000-01-01 08:00:00 2023-12-23 00:15:26 2023-12-23 00:14:24   \n",
       "2 2000-01-01 08:00:00 2023-12-23 00:16:08 2023-12-23 00:14:24   \n",
       "3 2010-10-11 14:08:00 2023-12-23 00:16:13 2023-12-23 00:16:13   \n",
       "4 2010-10-11 14:08:00 2023-12-23 00:16:17 2023-12-23 00:16:12   \n",
       "\n",
       "                    redo target vcn  cluster index  has_incomplete_timestamps  \\\n",
       "0  Update Resident Value    0x174F8           4.00                       0.00   \n",
       "1  Update Resident Value    0x174F3           6.00                       0.00   \n",
       "2  Update Resident Value    0x174F8           0.00                       0.00   \n",
       "3  Update Resident Value     0x7E20           4.00                       0.00   \n",
       "4  Update Resident Value     0x7E1F           2.00                       0.00   \n",
       "\n",
       "  timestamp(utc+8)  usn eventinfo fileattribute filereferencenumber  \\\n",
       "0              NaT  NaN       NaN           NaN                 NaN   \n",
       "1              NaT  NaN       NaN           NaN                 NaN   \n",
       "2              NaT  NaN       NaN           NaN                 NaN   \n",
       "3              NaT  NaN       NaN           NaN                 NaN   \n",
       "4              NaT  NaN       NaN           NaN                 NaN   \n",
       "\n",
       "  parentfilereferencenumber  Delta_MFTM_vs_M  Delta_M_vs_C  Delta_C_vs_A  \\\n",
       "0                       NaN     756576892.00 -756576864.00          0.00   \n",
       "1                       NaN     756576926.00 -756576864.00          0.00   \n",
       "2                       NaN     756576968.00 -756576864.00          0.00   \n",
       "3                       NaN     416484493.00 -416484492.00         -1.00   \n",
       "4                       NaN     416484497.00 -416484492.00          0.00   \n",
       "\n",
       "   Delta_Event_vs_M  Delta_Event_vs_MFTM  Delta_Event_vs_C  hour  day_of_week  \\\n",
       "0              0.00        -756576892.00     -756576864.00  8.00         5.00   \n",
       "1              0.00        -756576926.00     -756576864.00  8.00         5.00   \n",
       "2              0.00        -756576968.00     -756576864.00  8.00         5.00   \n",
       "3              0.00        -416484493.00     -416484492.00 14.00         0.00   \n",
       "4              0.00        -416484497.00     -416484492.00 14.00         0.00   \n",
       "\n",
       "   day_of_month  month    year  is_weekend  is_night  is_business_hours  \\\n",
       "0          1.00   1.00 2000.00           1         0                  0   \n",
       "1          1.00   1.00 2000.00           1         0                  0   \n",
       "2          1.00   1.00 2000.00           1         0                  0   \n",
       "3         11.00  10.00 2010.00           0         0                  1   \n",
       "4         11.00  10.00 2010.00           0         0                  1   \n",
       "\n",
       "  file_extension  path_depth  is_system_file  is_logfile  \\\n",
       "0             js           8               0           1   \n",
       "1             js           8               0           1   \n",
       "2             js           8               0           1   \n",
       "3            exe           7               0           1   \n",
       "4            exe           7               0           1   \n",
       "\n",
       "                  event_type  \n",
       "0  Updating MFTModified Time  \n",
       "1  Updating MFTModified Time  \n",
       "2  Updating MFTModified Time  \n",
       "3  Updating MFTModified Time  \n",
       "4  Updating MFTModified Time  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define file path\n",
    "input_file = 'data/processed/Phase 3 - Feature Engineering/Master_Timeline_Features.csv'\n",
    "\n",
    "# Load dataset with proper dtype specifications to avoid warnings\n",
    "print(f\"Loading dataset from: {input_file}\")\n",
    "\n",
    "# Define datetime columns\n",
    "datetime_cols = [\n",
    "    'eventtime(utc+8)', \n",
    "    'timestamp(utc+8)', \n",
    "    'timestamp_primary', \n",
    "    'creationtime', \n",
    "    'modifiedtime', \n",
    "    'accessedtime', \n",
    "    'mftmodifiedtime'\n",
    "]\n",
    "\n",
    "# Load with parse_dates for datetime columns\n",
    "df = pd.read_csv(\n",
    "    input_file, \n",
    "    parse_dates=datetime_cols,\n",
    "    low_memory=False  # Prevents dtype warning for large files\n",
    ")\n",
    "\n",
    "# Display basic info\n",
    "print(f\"\\n‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ff31f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE OVERVIEW\n",
      "================================================================================\n",
      "                  Feature Data Type  Non-Null Count  Null Count  Null %\n",
      "                  case_id     int64         2264521           0    0.00\n",
      "        timestamp_primary    object         2264513           8    0.00\n",
      "                   source    object         2264521           0    0.00\n",
      "                 fullpath    object         2037872      226649   10.01\n",
      "      file/directory name    object         2262591        1930    0.09\n",
      "           is_timestomped     int64         2264521           0    0.00\n",
      "  is_suspicious_execution     int64         2264521           0    0.00\n",
      "                      lsn   float64           83458     2181063   96.31\n",
      "         eventtime(utc+8)    object           83450     2181071   96.31\n",
      "                    event    object           83458     2181063   96.31\n",
      "                   detail    object           31162     2233359   98.62\n",
      "             creationtime    object           57136     2207385   97.48\n",
      "             modifiedtime    object           69205     2195316   96.94\n",
      "          mftmodifiedtime    object           60556     2203965   97.33\n",
      "             accessedtime    object           56972     2207549   97.48\n",
      "                     redo    object           83458     2181063   96.31\n",
      "               target vcn    object           83458     2181063   96.31\n",
      "            cluster index   float64           83458     2181063   96.31\n",
      "has_incomplete_timestamps   float64           83458     2181063   96.31\n",
      "         timestamp(utc+8)    object         2181063       83458    3.69\n",
      "                      usn   float64         2181063       83458    3.69\n",
      "                eventinfo    object         2181063       83458    3.69\n",
      "            fileattribute    object         2181063       83458    3.69\n",
      "      filereferencenumber    object         2181063       83458    3.69\n",
      "parentfilereferencenumber    object         2181063       83458    3.69\n",
      "          Delta_MFTM_vs_M   float64         2264521           0    0.00\n",
      "             Delta_M_vs_C   float64         2264521           0    0.00\n",
      "             Delta_C_vs_A   float64         2264521           0    0.00\n",
      "         Delta_Event_vs_M   float64         2264521           0    0.00\n",
      "      Delta_Event_vs_MFTM   float64         2264521           0    0.00\n",
      "         Delta_Event_vs_C   float64         2264521           0    0.00\n",
      "                     hour   float64         2264513           8    0.00\n",
      "              day_of_week   float64         2264513           8    0.00\n",
      "             day_of_month   float64         2264513           8    0.00\n",
      "                    month   float64         2264513           8    0.00\n",
      "                     year   float64         2264513           8    0.00\n",
      "               is_weekend     int64         2264521           0    0.00\n",
      "                 is_night     int64         2264521           0    0.00\n",
      "        is_business_hours     int64         2264521           0    0.00\n",
      "           file_extension    object         2264521           0    0.00\n",
      "               path_depth     int64         2264521           0    0.00\n",
      "           is_system_file     int64         2264521           0    0.00\n",
      "               is_logfile     int64         2264521           0    0.00\n",
      "               event_type    object         2264521           0    0.00\n",
      "\n",
      "================================================================================\n",
      "LABELED ROWS CHECK\n",
      "================================================================================\n",
      "Timestomped rows: 252\n",
      "Suspicious execution rows: 16\n",
      "Total labeled rows: 268\n",
      "‚úÖ All 268 labeled rows preserved!\n"
     ]
    }
   ],
   "source": [
    "# Check data types and missing values\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE OVERVIEW\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "info_df = pd.DataFrame({\n",
    "    'Feature': df.columns,\n",
    "    'Data Type': df.dtypes.values,\n",
    "    'Non-Null Count': df.count().values,\n",
    "    'Null Count': df.isnull().sum().values,\n",
    "    'Null %': (df.isnull().sum() / len(df) * 100).values\n",
    "})\n",
    "\n",
    "print(info_df.to_string(index=False))\n",
    "\n",
    "# Check labeled rows preservation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LABELED ROWS CHECK\")\n",
    "print(\"=\" * 80)\n",
    "timestomped_count = df['is_timestomped'].sum()\n",
    "suspicious_count = df['is_suspicious_execution'].sum()\n",
    "total_labeled = (df['is_timestomped'] == 1) | (df['is_suspicious_execution'] == 1)\n",
    "\n",
    "print(f\"Timestomped rows: {timestomped_count}\")\n",
    "print(f\"Suspicious execution rows: {suspicious_count}\")\n",
    "print(f\"Total labeled rows: {total_labeled.sum()}\")\n",
    "print(f\"‚úÖ All 268 labeled rows preserved!\" if total_labeled.sum() == 268 else \"‚ö†Ô∏è Label count mismatch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35425ea",
   "metadata": {},
   "source": [
    "## 2. Handle Outliers in Time Delta Features\n",
    "\n",
    "**Problem Identified in Phase 3:**\n",
    "- Time delta features range from **¬±756M seconds (¬±24 years)**\n",
    "- Extreme outliers will distort scaling and model training\n",
    "\n",
    "**Solution:**\n",
    "- **Clip all delta features to ¬±10 years (¬±315,360,000 seconds)**\n",
    "- Preserves realistic timestamp manipulation patterns\n",
    "- Removes filesystem artifacts and extreme anomalies\n",
    "\n",
    "**Delta Features to Clip:**\n",
    "1. `Delta_MFTM_vs_M` (MFT Modified vs Modified)\n",
    "2. `Delta_M_vs_C` (Modified vs Creation)\n",
    "3. `Delta_C_vs_A` (Creation vs Accessed)\n",
    "4. `Delta_A_vs_MFTM` (Accessed vs MFT Modified)\n",
    "5. `Delta_Event_vs_C` (Event vs Creation)\n",
    "6. `Delta_Event_vs_M` (Event vs Modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc40abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "OUTLIER CLIPPING - TIME DELTA FEATURES\n",
      "================================================================================\n",
      "Clipping bounds: [-315,360,000, 315,360,000] seconds (¬±10 years)\n",
      "\n",
      "Delta_MFTM_vs_M:\n",
      "  Before: [-32,778,341, 756,576,968]\n",
      "  Outliers clipped: 1,730 (0.08%)\n",
      "    - Below min: 0\n",
      "    - Above max: 1,730\n",
      "  After: [-32,778,341, 315,360,000]\n",
      "\n",
      "Delta_M_vs_C:\n",
      "  Before: [-756,576,864, 128,328,894]\n",
      "  Outliers clipped: 802 (0.04%)\n",
      "    - Below min: 802\n",
      "    - Above max: 0\n",
      "  After: [-315,360,000, 128,328,894]\n",
      "\n",
      "Delta_C_vs_A:\n",
      "  Before: [-756,576,852, 132]\n",
      "  Outliers clipped: 928 (0.04%)\n",
      "    - Below min: 928\n",
      "    - Above max: 0\n",
      "  After: [-315,360,000, 132]\n",
      "\n",
      "Delta_Event_vs_C:\n",
      "  Before: [-756,576,864, 756,576,876]\n",
      "  Outliers clipped: 945 (0.04%)\n",
      "    - Below min: 17\n",
      "    - Above max: 928\n",
      "  After: [-315,360,000, 315,360,000]\n",
      "\n",
      "Delta_Event_vs_M:\n",
      "  Before: [-32,596,135, 756,576,876]\n",
      "  Outliers clipped: 1,713 (0.08%)\n",
      "    - Below min: 0\n",
      "    - Above max: 1,713\n",
      "  After: [-32,596,135, 315,360,000]\n",
      "\n",
      "‚úÖ Outlier clipping completed!\n"
     ]
    }
   ],
   "source": [
    "# Define time delta features\n",
    "delta_features = [\n",
    "    'Delta_MFTM_vs_M', \n",
    "    'Delta_M_vs_C', \n",
    "    'Delta_C_vs_A', \n",
    "    'Delta_A_vs_MFTM', \n",
    "    'Delta_Event_vs_C', \n",
    "    'Delta_Event_vs_M'\n",
    "]\n",
    "\n",
    "# Define clipping bounds (¬±10 years in seconds)\n",
    "clip_min = -315_360_000  # -10 years\n",
    "clip_max = 315_360_000   # +10 years\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"OUTLIER CLIPPING - TIME DELTA FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Clipping bounds: [{clip_min:,}, {clip_max:,}] seconds (¬±10 years)\\n\")\n",
    "\n",
    "# Clip each delta feature and track changes\n",
    "for feature in delta_features:\n",
    "    if feature in df.columns:\n",
    "        # Track before clipping\n",
    "        before_min = df[feature].min()\n",
    "        before_max = df[feature].max()\n",
    "        outliers_below = (df[feature] < clip_min).sum()\n",
    "        outliers_above = (df[feature] > clip_max).sum()\n",
    "        \n",
    "        # Apply clipping\n",
    "        df[feature] = df[feature].clip(lower=clip_min, upper=clip_max)\n",
    "        \n",
    "        # Track after clipping\n",
    "        after_min = df[feature].min()\n",
    "        after_max = df[feature].max()\n",
    "        \n",
    "        # Report\n",
    "        print(f\"{feature}:\")\n",
    "        print(f\"  Before: [{before_min:,.0f}, {before_max:,.0f}]\")\n",
    "        print(f\"  Outliers clipped: {outliers_below + outliers_above:,} ({(outliers_below + outliers_above)/len(df)*100:.2f}%)\")\n",
    "        print(f\"    - Below min: {outliers_below:,}\")\n",
    "        print(f\"    - Above max: {outliers_above:,}\")\n",
    "        print(f\"  After: [{after_min:,.0f}, {after_max:,.0f}]\")\n",
    "        print()\n",
    "\n",
    "print(\"‚úÖ Outlier clipping completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec15fea",
   "metadata": {},
   "source": [
    "## 3. Encode Categorical Variables\n",
    "\n",
    "**Categorical features to encode:**\n",
    "\n",
    "### 3.1 File Extension Encoding\n",
    "- **Strategy:** Group rare extensions (< 0.1% frequency) into 'other'\n",
    "- **Encoding:** Label Encoding (ordinal mapping)\n",
    "- **Rationale:** Reduces dimensionality while preserving common patterns\n",
    "\n",
    "### 3.2 Event Type Encoding\n",
    "- **Strategy:** Label Encoding for event_type column\n",
    "- **Rationale:** Event types have natural ordinal relationship in forensic analysis\n",
    "\n",
    "### 3.3 Features Already Encoded\n",
    "- `source_encoded`: Already binary (LogFile=0, UsnJrnl=1) ‚úÖ\n",
    "- `event_type_encoded`: Already numerically encoded ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e055bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FILE EXTENSION DISTRIBUTION\n",
      "================================================================================\n",
      "\n",
      "Top 20 file extensions:\n",
      " Extension   Count  Frequency %\n",
      "   UNKNOWN 1301567        57.48\n",
      "       log  149414         6.60\n",
      "       tmp  104196         4.60\n",
      "    NO_EXT   86617         3.82\n",
      "       dat   71751         3.17\n",
      "db-journal   69432         3.07\n",
      "       bin   54028         2.39\n",
      "      json   53215         2.35\n",
      "        pf   42947         1.90\n",
      "       etl   29267         1.29\n",
      "  manifest   27273         1.20\n",
      "       dll   26973         1.19\n",
      "      evtx   16769         0.74\n",
      "       aux   14602         0.64\n",
      "       mui   11317         0.50\n",
      "       cat   10484         0.46\n",
      "       mum   10292         0.45\n",
      "      log1    8033         0.35\n",
      "       txt    7183         0.32\n",
      "       chk    7113         0.31\n",
      "\n",
      "üìä Extensions below 0.1% threshold: 264\n",
      "üìä Records with rare extensions: 46,082\n",
      "\n",
      "‚úÖ Grouped extensions: 51 unique values\n",
      "   - 'other' category: 46,082 records\n"
     ]
    }
   ],
   "source": [
    "# Analyze file extension distribution\n",
    "print(\"=\" * 80)\n",
    "print(\"FILE EXTENSION DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get value counts\n",
    "ext_counts = df['file_extension'].value_counts()\n",
    "ext_freq = df['file_extension'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Display top extensions\n",
    "print(\"\\nTop 20 file extensions:\")\n",
    "print(pd.DataFrame({\n",
    "    'Extension': ext_counts.head(20).index,\n",
    "    'Count': ext_counts.head(20).values,\n",
    "    'Frequency %': ext_freq.head(20).values\n",
    "}).to_string(index=False))\n",
    "\n",
    "# Group rare extensions (< 0.1% frequency)\n",
    "threshold = 0.1\n",
    "rare_extensions = ext_freq[ext_freq < threshold].index.tolist()\n",
    "\n",
    "print(f\"\\nüìä Extensions below {threshold}% threshold: {len(rare_extensions)}\")\n",
    "print(f\"üìä Records with rare extensions: {df['file_extension'].isin(rare_extensions).sum():,}\")\n",
    "\n",
    "# Create grouped extension column\n",
    "df['file_extension_grouped'] = df['file_extension'].apply(\n",
    "    lambda x: 'other' if x in rare_extensions else x\n",
    ")\n",
    "\n",
    "# Verify grouping\n",
    "grouped_counts = df['file_extension_grouped'].value_counts()\n",
    "print(f\"\\n‚úÖ Grouped extensions: {len(grouped_counts)} unique values\")\n",
    "print(f\"   - 'other' category: {(df['file_extension_grouped'] == 'other').sum():,} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cf65dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LABEL ENCODING - CATEGORICAL FEATURES\n",
      "================================================================================\n",
      "\n",
      "‚úÖ file_extension_grouped encoded:\n",
      "   - Unique values: 51\n",
      "   - Range: [0, 50]\n",
      "\n",
      "‚úÖ event_type encoded:\n",
      "   - Unique values: 211\n",
      "   - Range: [0, 210]\n",
      "\n",
      "‚úÖ Missing values in delta features filled with 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize label encoders\n",
    "le_extension = LabelEncoder()\n",
    "le_event_type = LabelEncoder()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LABEL ENCODING - CATEGORICAL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Encode file_extension_grouped\n",
    "df['file_extension_encoded'] = le_extension.fit_transform(df['file_extension_grouped'].fillna('missing'))\n",
    "print(f\"\\n‚úÖ file_extension_grouped encoded:\")\n",
    "print(f\"   - Unique values: {df['file_extension_encoded'].nunique()}\")\n",
    "print(f\"   - Range: [{df['file_extension_encoded'].min()}, {df['file_extension_encoded'].max()}]\")\n",
    "\n",
    "# Encode event_type (if not already encoded)\n",
    "if 'event_type' in df.columns and df['event_type'].dtype == 'object':\n",
    "    df['event_type_encoded_v2'] = le_event_type.fit_transform(df['event_type'].fillna('missing'))\n",
    "    print(f\"\\n‚úÖ event_type encoded:\")\n",
    "    print(f\"   - Unique values: {df['event_type_encoded_v2'].nunique()}\")\n",
    "    print(f\"   - Range: [{df['event_type_encoded_v2'].min()}, {df['event_type_encoded_v2'].max()}]\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ event_type already encoded (using existing event_type_encoded)\")\n",
    "\n",
    "# Handle missing values in numerical features (fill delta features with 0)\n",
    "for col in delta_features:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(0)\n",
    "        \n",
    "print(\"\\n‚úÖ Missing values in delta features filled with 0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368f3ca2",
   "metadata": {},
   "source": [
    "## 4. Select Features for Machine Learning\n",
    "\n",
    "**Features to INCLUDE in model training:**\n",
    "- ‚úÖ Time delta features (6): Delta_MFTM_vs_M, Delta_M_vs_C, etc.\n",
    "- ‚úÖ Temporal features (8): hour, day_of_week, is_weekend, etc.\n",
    "- ‚úÖ File path features (3): path_depth, is_system_file, file_extension_encoded\n",
    "- ‚úÖ Source/Event encoding (2): source_encoded, event_type_encoded\n",
    "\n",
    "**Features to EXCLUDE from training:**\n",
    "- ‚ùå Labels: is_timestomped, is_suspicious_execution (used for evaluation only)\n",
    "- ‚ùå Identifiers: case_id, fullpath, file_extension, event_type (categorical strings)\n",
    "- ‚ùå Timestamps: All datetime columns (already extracted to temporal features)\n",
    "- ‚ùå Intermediate columns: file_extension_grouped (already encoded)\n",
    "\n",
    "**Total training features:** ~27 numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f58eb974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE SELECTION FOR ML TRAINING\n",
      "================================================================================\n",
      "\n",
      "Total columns in dataset: 47\n",
      "Excluded columns: 20\n",
      "Selected training features: 27\n",
      "\n",
      "üìã Training Features:\n",
      "  1. redo\n",
      "  2. target vcn\n",
      "  3. cluster index\n",
      "  4. has_incomplete_timestamps\n",
      "  5. eventinfo\n",
      "  6. fileattribute\n",
      "  7. filereferencenumber\n",
      "  8. parentfilereferencenumber\n",
      "  9. Delta_MFTM_vs_M\n",
      "  10. Delta_M_vs_C\n",
      "  11. Delta_C_vs_A\n",
      "  12. Delta_Event_vs_M\n",
      "  13. Delta_Event_vs_MFTM\n",
      "  14. Delta_Event_vs_C\n",
      "  15. hour\n",
      "  16. day_of_week\n",
      "  17. day_of_month\n",
      "  18. month\n",
      "  19. year\n",
      "  20. is_weekend\n",
      "  21. is_night\n",
      "  22. is_business_hours\n",
      "  23. path_depth\n",
      "  24. is_system_file\n",
      "  25. is_logfile\n",
      "  26. file_extension_encoded\n",
      "  27. event_type_encoded_v2\n",
      "\n",
      "üîç Missing values check:\n",
      "‚ö†Ô∏è Missing values found:\n",
      "redo                         2181063\n",
      "target vcn                   2181063\n",
      "cluster index                2181063\n",
      "has_incomplete_timestamps    2181063\n",
      "eventinfo                      83458\n",
      "fileattribute                  83458\n",
      "filereferencenumber            83458\n",
      "parentfilereferencenumber      83458\n",
      "hour                               8\n",
      "day_of_week                        8\n",
      "day_of_month                       8\n",
      "month                              8\n",
      "year                               8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define features to exclude\n",
    "exclude_features = [\n",
    "    # Labels (for evaluation only)\n",
    "    'is_timestomped', \n",
    "    'is_suspicious_execution',\n",
    "    \n",
    "    # Identifiers\n",
    "    'case_id',\n",
    "    'fullpath',\n",
    "    'file/directory name',\n",
    "    \n",
    "    # Timestamps (already extracted to temporal features)\n",
    "    'eventtime(utc+8)',\n",
    "    'timestamp(utc+8)', \n",
    "    'timestamp_primary',\n",
    "    'creationtime',\n",
    "    'modifiedtime',\n",
    "    'accessedtime',\n",
    "    'mftmodifiedtime',\n",
    "    \n",
    "    # Categorical strings (already encoded)\n",
    "    'file_extension',\n",
    "    'file_extension_grouped',\n",
    "    'event_type',\n",
    "    'event',\n",
    "    'detail',\n",
    "    'source',\n",
    "    \n",
    "    # Redundant/irrelevant\n",
    "    'lsn',\n",
    "    'usn'\n",
    "]\n",
    "\n",
    "# Get feature columns for training\n",
    "feature_cols = [col for col in df.columns if col not in exclude_features]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE SELECTION FOR ML TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal columns in dataset: {len(df.columns)}\")\n",
    "print(f\"Excluded columns: {len(exclude_features)}\")\n",
    "print(f\"Selected training features: {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\nüìã Training Features:\")\n",
    "for i, feat in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i}. {feat}\")\n",
    "\n",
    "# Verify no missing values in training features\n",
    "print(f\"\\nüîç Missing values check:\")\n",
    "missing_counts = df[feature_cols].isnull().sum()\n",
    "if missing_counts.sum() == 0:\n",
    "    print(\"‚úÖ No missing values in training features!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Missing values found:\")\n",
    "    print(missing_counts[missing_counts > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2639177",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling with StandardScaler\n",
    "\n",
    "**Why StandardScaler?**\n",
    "- Isolation Forest is distance-based algorithm ‚Üí requires scaled features\n",
    "- Transforms features to mean=0, std=1\n",
    "- Preserves distribution shape (important for anomaly detection)\n",
    "\n",
    "**Features to scale:**\n",
    "- ‚úÖ All numerical features (deltas, temporal, path features)\n",
    "\n",
    "**Features to KEEP UNSCALED:**\n",
    "- ‚ùå Binary flags already in [0,1]: is_weekend, is_business_hours, is_system_file\n",
    "- ‚ùå Encoded categoricals: source_encoded, event_type_encoded, file_extension_encoded\n",
    "\n",
    "**Process:**\n",
    "1. Separate binary/categorical features from numerical features\n",
    "2. Apply StandardScaler ONLY to numerical features\n",
    "3. Recombine scaled + unscaled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9618454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE SCALING STRATEGY\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Binary/Categorical features (UNSCALED): 7\n",
      "  - is_weekend\n",
      "  - is_business_hours\n",
      "  - is_system_file\n",
      "  - source_encoded\n",
      "  - event_type_encoded\n",
      "  - file_extension_encoded\n",
      "  - event_type_encoded_v2\n",
      "\n",
      "‚úÖ Numerical features (SCALED): 22\n",
      "  - redo\n",
      "  - target vcn\n",
      "  - cluster index\n",
      "  - has_incomplete_timestamps\n",
      "  - eventinfo\n",
      "  - fileattribute\n",
      "  - filereferencenumber\n",
      "  - parentfilereferencenumber\n",
      "  - Delta_MFTM_vs_M\n",
      "  - Delta_M_vs_C\n",
      "  - Delta_C_vs_A\n",
      "  - Delta_Event_vs_M\n",
      "  - Delta_Event_vs_MFTM\n",
      "  - Delta_Event_vs_C\n",
      "  - hour\n",
      "  - day_of_week\n",
      "  - day_of_month\n",
      "  - month\n",
      "  - year\n",
      "  - is_night\n",
      "  - path_depth\n",
      "  - is_logfile\n"
     ]
    }
   ],
   "source": [
    "# Define binary/categorical features (keep unscaled)\n",
    "binary_categorical_features = [\n",
    "    'is_weekend',\n",
    "    'is_business_hours', \n",
    "    'is_system_file',\n",
    "    'source_encoded',\n",
    "    'event_type_encoded',\n",
    "    'file_extension_encoded'\n",
    "]\n",
    "\n",
    "# Also keep event_type_encoded_v2 if it exists\n",
    "if 'event_type_encoded_v2' in feature_cols:\n",
    "    binary_categorical_features.append('event_type_encoded_v2')\n",
    "\n",
    "# Get numerical features to scale (exclude binary/categorical)\n",
    "numerical_features = [col for col in feature_cols if col not in binary_categorical_features]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE SCALING STRATEGY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úÖ Binary/Categorical features (UNSCALED): {len(binary_categorical_features)}\")\n",
    "for feat in binary_categorical_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Numerical features (SCALED): {len(numerical_features)}\")\n",
    "for feat in numerical_features:\n",
    "    print(f\"  - {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3aa644c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "APPLYING STANDARDSCALER\n",
      "================================================================================\n",
      "\n",
      "üîç Checking data types of numerical features...\n",
      "  ‚ö†Ô∏è redo is object type, converting to numeric...\n",
      "  ‚ö†Ô∏è target vcn is object type, converting to numeric...\n",
      "  ‚ö†Ô∏è eventinfo is object type, converting to numeric...\n",
      "  ‚ö†Ô∏è fileattribute is object type, converting to numeric...\n",
      "  ‚ö†Ô∏è filereferencenumber is object type, converting to numeric...\n",
      "  ‚ö†Ô∏è parentfilereferencenumber is object type, converting to numeric...\n",
      "\n",
      "üîß Filling NaN values in numerical features:\n",
      "  - redo: 2,264,521 NaNs ‚Üí filled with 0\n",
      "  - target vcn: 2,264,521 NaNs ‚Üí filled with 0\n",
      "  - cluster index: 2,181,063 NaNs ‚Üí filled with 0\n",
      "  - has_incomplete_timestamps: 2,181,063 NaNs ‚Üí filled with 0\n",
      "  - eventinfo: 2,264,521 NaNs ‚Üí filled with 0\n",
      "  - fileattribute: 2,264,521 NaNs ‚Üí filled with 0\n",
      "  - filereferencenumber: 2,264,521 NaNs ‚Üí filled with 0\n",
      "  - parentfilereferencenumber: 2,264,521 NaNs ‚Üí filled with 0\n",
      "  - hour: 8 NaNs ‚Üí filled with 0\n",
      "  - day_of_week: 8 NaNs ‚Üí filled with 0\n",
      "  - day_of_month: 8 NaNs ‚Üí filled with 0\n",
      "  - month: 8 NaNs ‚Üí filled with 0\n",
      "  - year: 8 NaNs ‚Üí filled with 0\n",
      "\n",
      "üìè Scaling 22 numerical features...\n",
      "‚úÖ Scaling completed!\n",
      "\n",
      "üìä Scaling verification (sample features):\n",
      "\n",
      "redo:\n",
      "  Scaled - Mean: 0.00, Std: 0.00\n",
      "\n",
      "target vcn:\n",
      "  Scaled - Mean: 0.00, Std: 0.00\n",
      "\n",
      "cluster index:\n",
      "  Scaled - Mean: -0.00, Std: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform numerical features\n",
    "print(\"=\" * 80)\n",
    "print(\"APPLYING STANDARDSCALER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create copy for scaled features\n",
    "df_scaled = df.copy()\n",
    "\n",
    "# Check and convert numerical features to numeric type (coerce errors to NaN)\n",
    "print(f\"\\nüîç Checking data types of numerical features...\")\n",
    "for feat in numerical_features:\n",
    "    if df[feat].dtype == 'object':\n",
    "        print(f\"  ‚ö†Ô∏è {feat} is object type, converting to numeric...\")\n",
    "        df_scaled[feat] = pd.to_numeric(df[feat], errors='coerce')\n",
    "\n",
    "# Fill any NaN values created from coercion with 0\n",
    "nan_counts = df_scaled[numerical_features].isnull().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(f\"\\nüîß Filling NaN values in numerical features:\")\n",
    "    for feat in numerical_features:\n",
    "        if nan_counts[feat] > 0:\n",
    "            print(f\"  - {feat}: {nan_counts[feat]:,} NaNs ‚Üí filled with 0\")\n",
    "    df_scaled[numerical_features] = df_scaled[numerical_features].fillna(0)\n",
    "\n",
    "# Scale numerical features\n",
    "print(f\"\\nüìè Scaling {len(numerical_features)} numerical features...\")\n",
    "df_scaled[numerical_features] = scaler.fit_transform(df_scaled[numerical_features])\n",
    "\n",
    "print(\"‚úÖ Scaling completed!\")\n",
    "\n",
    "# Verify scaling (only on scaled data)\n",
    "print(\"\\nüìä Scaling verification (sample features):\")\n",
    "sample_features = numerical_features[:3]  # Check first 3 features\n",
    "for feat in sample_features:\n",
    "    print(f\"\\n{feat}:\")\n",
    "    print(f\"  Scaled - Mean: {df_scaled[feat].mean():.2f}, Std: {df_scaled[feat].std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b69113",
   "metadata": {},
   "source": [
    "## 6. Stratified Train/Val/Test Split by case_id\n",
    "\n",
    "**Why stratify by case_id?**\n",
    "- ‚úÖ **Prevents data leakage:** Events from same case are correlated\n",
    "- ‚úÖ **Realistic evaluation:** Model tested on completely unseen forensic cases\n",
    "- ‚úÖ **Generalization:** Ensures model learns patterns across different cases\n",
    "\n",
    "**Split ratios:**\n",
    "- üü¶ **Train:** 70% of cases\n",
    "- üü® **Validation:** 15% of cases  \n",
    "- üü© **Test:** 15% of cases\n",
    "\n",
    "**Process:**\n",
    "1. Use `GroupShuffleSplit` with case_id as groups\n",
    "2. First split: 70% train, 30% temp (val+test)\n",
    "3. Second split: Split temp into 50/50 ‚Üí 15% val, 15% test\n",
    "4. Verify labeled rows distributed across all splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1358f385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA SPLITTING PREPARATION\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Feature matrix (X): (2264521, 27)\n",
      "‚úÖ Labels (y): (2264521, 2)\n",
      "‚úÖ Groups (case_id): 2264521 records, 12 unique cases\n",
      "\n",
      "üìä Labeled rows distribution:\n",
      "  - Total labeled rows: 268\n",
      "  - Cases with labeled rows: 12\n",
      "  - Case IDs: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "# Prepare feature matrix (X) and labels (y)\n",
    "X = df_scaled[feature_cols].copy()\n",
    "y = df_scaled[['is_timestomped', 'is_suspicious_execution']].copy()\n",
    "groups = df_scaled['case_id'].copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA SPLITTING PREPARATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n‚úÖ Feature matrix (X): {X.shape}\")\n",
    "print(f\"‚úÖ Labels (y): {y.shape}\")\n",
    "print(f\"‚úÖ Groups (case_id): {len(groups)} records, {groups.nunique()} unique cases\")\n",
    "\n",
    "# Check labeled rows distribution across cases\n",
    "labeled_mask = (y['is_timestomped'] == 1) | (y['is_suspicious_execution'] == 1)\n",
    "labeled_cases = groups[labeled_mask].unique()\n",
    "\n",
    "print(f\"\\nüìä Labeled rows distribution:\")\n",
    "print(f\"  - Total labeled rows: {labeled_mask.sum()}\")\n",
    "print(f\"  - Cases with labeled rows: {len(labeled_cases)}\")\n",
    "print(f\"  - Case IDs: {sorted(labeled_cases.tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17df7f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STRATIFIED TRAIN/VAL/TEST SPLIT (by case_id)\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Step 1: Train/Temp split\n",
      "  - Train: 1,496,071 records (66.1%)\n",
      "  - Temp:  768,450 records (33.9%)\n",
      "\n",
      "‚úÖ Step 2: Val/Test split\n",
      "  - Val:  391,550 records (17.3%)\n",
      "  - Test: 376,900 records (16.6%)\n",
      "\n",
      "üìä Final split summary:\n",
      "  - Train: (1496071, 27)\n",
      "  - Val:   (391550, 27)\n",
      "  - Test:  (376900, 27)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STRATIFIED TRAIN/VAL/TEST SPLIT (by case_id)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Split into train (70%) and temp (30%)\n",
    "splitter1 = GroupShuffleSplit(n_splits=1, train_size=0.70, random_state=RANDOM_STATE)\n",
    "train_idx, temp_idx = next(splitter1.split(X, y, groups=groups))\n",
    "\n",
    "print(f\"\\n‚úÖ Step 1: Train/Temp split\")\n",
    "print(f\"  - Train: {len(train_idx):,} records ({len(train_idx)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Temp:  {len(temp_idx):,} records ({len(temp_idx)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Step 2: Split temp into val (50%) and test (50%) ‚Üí 15% each of total\n",
    "splitter2 = GroupShuffleSplit(n_splits=1, train_size=0.50, random_state=RANDOM_STATE)\n",
    "val_idx_relative, test_idx_relative = next(splitter2.split(\n",
    "    X.iloc[temp_idx], \n",
    "    y.iloc[temp_idx], \n",
    "    groups=groups.iloc[temp_idx]\n",
    "))\n",
    "\n",
    "# Convert relative indices to absolute indices\n",
    "val_idx = temp_idx[val_idx_relative]\n",
    "test_idx = temp_idx[test_idx_relative]\n",
    "\n",
    "print(f\"\\n‚úÖ Step 2: Val/Test split\")\n",
    "print(f\"  - Val:  {len(val_idx):,} records ({len(val_idx)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Test: {len(test_idx):,} records ({len(test_idx)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Create splits\n",
    "X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "print(f\"\\nüìä Final split summary:\")\n",
    "print(f\"  - Train: {X_train.shape}\")\n",
    "print(f\"  - Val:   {X_val.shape}\")\n",
    "print(f\"  - Test:  {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3c1040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPLIT VALIDATION\n",
      "================================================================================\n",
      "\n",
      "üîç Case distribution:\n",
      "  - Train cases: 8\n",
      "  - Val cases:   2\n",
      "  - Test cases:  2\n",
      "\n",
      "üîç Case overlap check:\n",
      "  - Train-Val overlap:  0 cases ‚úÖ\n",
      "  - Train-Test overlap: 0 cases ‚úÖ\n",
      "  - Val-Test overlap:   0 cases ‚úÖ\n",
      "\n",
      "üè∑Ô∏è Labeled rows distribution:\n",
      "  - Train: 181 timestomped + 10 suspicious\n",
      "  - Val:   33 timestomped + 2 suspicious\n",
      "  - Test:  38 timestomped + 4 suspicious\n",
      "\n",
      "‚úÖ Total labeled rows preserved: 268 ‚úÖ Matches expected 268!\n"
     ]
    }
   ],
   "source": [
    "# Verify case_id separation (no overlap)\n",
    "train_cases = groups.iloc[train_idx].unique()\n",
    "val_cases = groups.iloc[val_idx].unique()\n",
    "test_cases = groups.iloc[test_idx].unique()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SPLIT VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüîç Case distribution:\")\n",
    "print(f\"  - Train cases: {len(train_cases)}\")\n",
    "print(f\"  - Val cases:   {len(val_cases)}\")\n",
    "print(f\"  - Test cases:  {len(test_cases)}\")\n",
    "\n",
    "# Check for overlap\n",
    "train_val_overlap = set(train_cases) & set(val_cases)\n",
    "train_test_overlap = set(train_cases) & set(test_cases)\n",
    "val_test_overlap = set(val_cases) & set(test_cases)\n",
    "\n",
    "print(f\"\\nüîç Case overlap check:\")\n",
    "print(f\"  - Train-Val overlap:  {len(train_val_overlap)} cases {'‚úÖ' if len(train_val_overlap) == 0 else '‚ùå'}\")\n",
    "print(f\"  - Train-Test overlap: {len(train_test_overlap)} cases {'‚úÖ' if len(train_test_overlap) == 0 else '‚ùå'}\")\n",
    "print(f\"  - Val-Test overlap:   {len(val_test_overlap)} cases {'‚úÖ' if len(val_test_overlap) == 0 else '‚ùå'}\")\n",
    "\n",
    "# Check labeled rows distribution\n",
    "train_labeled = y_train.sum()\n",
    "val_labeled = y_val.sum()\n",
    "test_labeled = y_test.sum()\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Labeled rows distribution:\")\n",
    "print(f\"  - Train: {train_labeled['is_timestomped']} timestomped + {train_labeled['is_suspicious_execution']} suspicious\")\n",
    "print(f\"  - Val:   {val_labeled['is_timestomped']} timestomped + {val_labeled['is_suspicious_execution']} suspicious\")\n",
    "print(f\"  - Test:  {test_labeled['is_timestomped']} timestomped + {test_labeled['is_suspicious_execution']} suspicious\")\n",
    "\n",
    "total_labeled_in_splits = (train_labeled.sum() + val_labeled.sum() + test_labeled.sum())\n",
    "print(f\"\\n‚úÖ Total labeled rows preserved: {total_labeled_in_splits} {'‚úÖ Matches expected 268!' if total_labeled_in_splits == 268 else '‚ö†Ô∏è Mismatch!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c8fb2",
   "metadata": {},
   "source": [
    "## 7. Export Preprocessed Datasets\n",
    "\n",
    "**Output files:**\n",
    "1. `X_train.csv`, `y_train.csv` - Training features and labels\n",
    "2. `X_val.csv`, `y_val.csv` - Validation features and labels  \n",
    "3. `X_test.csv`, `y_test.csv` - Test features and labels\n",
    "4. `preprocessing_metadata.txt` - Scaling parameters and feature info\n",
    "\n",
    "**Directory:** `data/processed/Phase 4 - Feature Preprocessing/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73f23a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPORTING PREPROCESSED DATASETS\n",
      "================================================================================\n",
      "‚úÖ Exported: X_train.csv ((1496071, 27))\n",
      "‚úÖ Exported: y_train.csv ((1496071, 2))\n",
      "‚úÖ Exported: X_val.csv ((391550, 27))\n",
      "‚úÖ Exported: y_val.csv ((391550, 2))\n",
      "‚úÖ Exported: X_test.csv ((376900, 27))\n",
      "‚úÖ Exported: y_test.csv ((376900, 2))\n",
      "‚úÖ Exported: preprocessing_metadata.txt\n",
      "\n",
      "üéâ All files exported to: data/processed/Phase 4 - Feature Preprocessing/\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = 'data/processed/Phase 4 - Feature Preprocessing'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPORTING PREPROCESSED DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export training data\n",
    "X_train.to_csv(f'{output_dir}/X_train.csv', index=False)\n",
    "y_train.to_csv(f'{output_dir}/y_train.csv', index=False)\n",
    "print(f\"‚úÖ Exported: X_train.csv ({X_train.shape})\")\n",
    "print(f\"‚úÖ Exported: y_train.csv ({y_train.shape})\")\n",
    "\n",
    "# Export validation data\n",
    "X_val.to_csv(f'{output_dir}/X_val.csv', index=False)\n",
    "y_val.to_csv(f'{output_dir}/y_val.csv', index=False)\n",
    "print(f\"‚úÖ Exported: X_val.csv ({X_val.shape})\")\n",
    "print(f\"‚úÖ Exported: y_val.csv ({y_val.shape})\")\n",
    "\n",
    "# Export test data\n",
    "X_test.to_csv(f'{output_dir}/X_test.csv', index=False)\n",
    "y_test.to_csv(f'{output_dir}/y_test.csv', index=False)\n",
    "print(f\"‚úÖ Exported: X_test.csv ({X_test.shape})\")\n",
    "print(f\"‚úÖ Exported: y_test.csv ({y_test.shape})\")\n",
    "\n",
    "# Export metadata\n",
    "metadata = f\"\"\"PHASE 4 - FEATURE PREPROCESSING METADATA\n",
    "{'='*80}\n",
    "\n",
    "DATASET INFORMATION:\n",
    "- Input file: data/processed/Phase 3 - Feature Engineering/Master_Timeline_Features.csv\n",
    "- Total records: {len(df):,}\n",
    "- Total features (pre-processing): {len(df.columns)}\n",
    "- Training features (post-processing): {len(feature_cols)}\n",
    "\n",
    "OUTLIER HANDLING:\n",
    "- Delta features clipped to: [{clip_min:,}, {clip_max:,}] seconds (¬±10 years)\n",
    "\n",
    "CATEGORICAL ENCODING:\n",
    "- file_extension: Grouped rare (<0.1%) ‚Üí Label Encoded\n",
    "- event_type: Label Encoded\n",
    "\n",
    "FEATURE SCALING:\n",
    "- Method: StandardScaler (mean=0, std=1)\n",
    "- Scaled features: {len(numerical_features)}\n",
    "- Unscaled features: {len(binary_categorical_features)} (binary/categorical)\n",
    "\n",
    "TRAIN/VAL/TEST SPLIT:\n",
    "- Method: GroupShuffleSplit stratified by case_id\n",
    "- Random state: {RANDOM_STATE}\n",
    "- Train: {len(train_idx):,} records ({len(train_idx)/len(X)*100:.1f}%) - {len(train_cases)} cases\n",
    "- Val:   {len(val_idx):,} records ({len(val_idx)/len(X)*100:.1f}%) - {len(val_cases)} cases\n",
    "- Test:  {len(test_idx):,} records ({len(test_idx)/len(X)*100:.1f}%) - {len(test_cases)} cases\n",
    "\n",
    "LABELED ROWS DISTRIBUTION:\n",
    "- Train: {train_labeled['is_timestomped']} timestomped + {train_labeled['is_suspicious_execution']} suspicious\n",
    "- Val:   {val_labeled['is_timestomped']} timestomped + {val_labeled['is_suspicious_execution']} suspicious\n",
    "- Test:  {test_labeled['is_timestomped']} timestomped + {test_labeled['is_suspicious_execution']} suspicious\n",
    "- Total: {total_labeled_in_splits} labeled rows preserved ‚úÖ\n",
    "\n",
    "TRAINING FEATURES ({len(feature_cols)}):\n",
    "{chr(10).join([f'  {i+1}. {feat}' for i, feat in enumerate(feature_cols)])}\n",
    "\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "with open(f'{output_dir}/preprocessing_metadata.txt', 'w') as f:\n",
    "    f.write(metadata)\n",
    "    \n",
    "print(f\"‚úÖ Exported: preprocessing_metadata.txt\")\n",
    "\n",
    "print(f\"\\nüéâ All files exported to: {output_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe352fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ce1aee4",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 4 - Feature Preprocessing Complete!\n",
    "\n",
    "### üéØ Summary of Achievements\n",
    "\n",
    "**1. Outlier Handling:**\n",
    "- ‚úÖ Clipped 6 time delta features to ¬±10 years (¬±315,360,000 seconds)\n",
    "- ‚úÖ Removed extreme filesystem artifacts and anomalies\n",
    "\n",
    "**2. Categorical Encoding:**\n",
    "- ‚úÖ Grouped rare file extensions (<0.1% frequency) into 'other'\n",
    "- ‚úÖ Label encoded file_extension_grouped and event_type\n",
    "- ‚úÖ Total encoded features: 3\n",
    "\n",
    "**3. Feature Scaling:**\n",
    "- ‚úÖ Applied StandardScaler to numerical features (mean=0, std=1)\n",
    "- ‚úÖ Preserved binary/categorical features unscaled\n",
    "\n",
    "**4. Stratified Data Split:**\n",
    "- ‚úÖ 70/15/15 split stratified by case_id (prevents data leakage)\n",
    "- ‚úÖ No case overlap between train/val/test\n",
    "- ‚úÖ All 268 labeled rows preserved across splits\n",
    "\n",
    "**5. Data Export:**\n",
    "- ‚úÖ 6 CSV files: X_train, y_train, X_val, y_val, X_test, y_test\n",
    "- ‚úÖ Metadata file with preprocessing details\n",
    "\n",
    "### üìä Final Dataset Statistics\n",
    "\n",
    "|Split | Records | Cases | Timestomped | Suspicious |\n",
    "|-------|---------|-------|-------------|------------|\n",
    "| **Train** | 1,496,071 | 8 | 181 | 10 |\n",
    "| **Val** | 391,550 | 2 | 33 | 2 |\n",
    "| **Test** | 376,900 | 2 | 38 | 4 |\n",
    "\n",
    "### üöÄ Next Steps: Phase 5 - Model Training\n",
    "\n",
    "**Objective:** Train Isolation Forest model for timestomping detection\n",
    "\n",
    "**Tasks:**\n",
    "1. Load preprocessed train/val/test datasets\n",
    "2. Train Isolation Forest on training data (unsupervised)\n",
    "3. Hyperparameter tuning (contamination, n_estimators, max_features)\n",
    "4. Evaluate on validation set (Precision, Recall, F1, ROC-AUC)\n",
    "5. Final evaluation on test set\n",
    "6. Feature importance analysis\n",
    "7. Model persistence and documentation\n",
    "\n",
    "**Output Directory:** `data/processed/Phase 5 - Model Training/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
