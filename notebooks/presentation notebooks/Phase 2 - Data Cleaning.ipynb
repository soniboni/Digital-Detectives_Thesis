{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267e5ffa",
   "metadata": {},
   "source": [
    "# Phase 2 - Data Cleaning\n",
    "\n",
    "**Objective:** Clean, standardize, and prepare the labeled LogFile datasets for merging and analysis.\n",
    "\n",
    "**Process:**\n",
    "1. Import necessary libraries\n",
    "2. Load 12 labeled LogFile datasets from Phase 1\n",
    "3. Clean and standardize data\n",
    "4. Drop irrelevant columns and rows\n",
    "5. Impute missing values\n",
    "6. Merge all LogFiles into Master dataset\n",
    "7. Export cleaned Master_LogFile_Cleaned.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bc52a",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2047848e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 2.3.2\n",
      "NumPy version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Visualization libraries (optional, for data exploration)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92fb873",
   "metadata": {},
   "source": [
    "# -- Handling LogFile --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667135fe",
   "metadata": {},
   "source": [
    "## 2. Load the 12 Labeled LogFile Datasets\n",
    "\n",
    "Load all labeled LogFile datasets from Phase 1 output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a711b8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Directory: data/processed/Phase 1 - Data Labeling\n",
      "Output Directory: data/processed/Phase 2 - Data Cleaning\n",
      "Case IDs to process: ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_DIR = Path('data/processed/Phase 1 - Data Labeling')\n",
    "OUTPUT_DIR = Path('data/processed/Phase 2 - Data Cleaning')\n",
    "\n",
    "# Ensure output directory exists\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the 12 case IDs\n",
    "CASE_IDS = [f'{i:02d}' for i in range(1, 13)]  # ['01', '02', ..., '12']\n",
    "\n",
    "print(f\"Input Directory: {INPUT_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Case IDs to process: {CASE_IDS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1005da4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled LogFile datasets...\n",
      "------------------------------------------------------------\n",
      "✓ Case 01: Loaded 39,077 records | Columns: 16\n",
      "✓ Case 02: Loaded 14,783 records | Columns: 16\n",
      "✓ Case 03: Loaded 24,063 records | Columns: 16\n",
      "✓ Case 04: Loaded 12,731 records | Columns: 16\n",
      "✓ Case 05: Loaded 14,242 records | Columns: 16\n",
      "✓ Case 06: Loaded 14,030 records | Columns: 16\n",
      "✓ Case 07: Loaded 23,737 records | Columns: 16\n",
      "✓ Case 08: Loaded 23,379 records | Columns: 16\n",
      "✓ Case 09: Loaded 25,688 records | Columns: 16\n",
      "✓ Case 10: Loaded 23,932 records | Columns: 16\n",
      "✓ Case 11: Loaded 14,083 records | Columns: 16\n",
      "✓ Case 12: Loaded 14,139 records | Columns: 16\n",
      "------------------------------------------------------------\n",
      "Total datasets loaded: 12/12\n",
      "Total records across all cases: 243,884\n"
     ]
    }
   ],
   "source": [
    "# --- Load all 12 LogFile datasets ---\n",
    "logfile_dataframes = {}\n",
    "\n",
    "print(\"Loading labeled LogFile datasets...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for case_id in CASE_IDS:\n",
    "    filename = f'{case_id}-PE-LogFile_labeled.csv'\n",
    "    filepath = INPUT_DIR / filename\n",
    "    \n",
    "    try:\n",
    "        # Load with low_memory=False to handle mixed types\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        \n",
    "        # Add Case_ID column for tracking\n",
    "        df['Case_ID'] = int(case_id)\n",
    "        \n",
    "        # Store in dictionary\n",
    "        logfile_dataframes[case_id] = df\n",
    "        \n",
    "        print(f\"✓ Case {case_id}: Loaded {len(df):,} records | Columns: {len(df.columns)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ Case {case_id}: File not found at {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Case {case_id}: Error loading file - {e}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total datasets loaded: {len(logfile_dataframes)}/12\")\n",
    "\n",
    "# Calculate total records\n",
    "total_records = sum(len(df) for df in logfile_dataframes.values())\n",
    "print(f\"Total records across all cases: {total_records:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f08d460",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration\n",
    "\n",
    "Examine the structure and content of the loaded datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3e76c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Dataset: Case 01\n",
      "============================================================\n",
      "\n",
      "Shape: (39077, 16)\n",
      "\n",
      "Column Names:\n",
      "['lsn', 'eventtime(utc+8)', 'event', 'detail', 'file/directory name', 'full path', 'creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime', 'redo', 'target vcn', 'cluster index', 'is_timestomped', 'is_suspicious_execution', 'Case_ID']\n",
      "\n",
      "Data Types:\n",
      "lsn                         int64\n",
      "eventtime(utc+8)           object\n",
      "event                      object\n",
      "detail                     object\n",
      "file/directory name        object\n",
      "full path                  object\n",
      "creationtime               object\n",
      "modifiedtime               object\n",
      "mftmodifiedtime            object\n",
      "accessedtime               object\n",
      "redo                       object\n",
      "target vcn                 object\n",
      "cluster index               int64\n",
      "is_timestomped              int64\n",
      "is_suspicious_execution     int64\n",
      "Case_ID                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# --- Inspect first dataset as a sample ---\n",
    "sample_case = '01'\n",
    "df_sample = logfile_dataframes[sample_case]\n",
    "\n",
    "print(f\"Sample Dataset: Case {sample_case}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nShape: {df_sample.shape}\")\n",
    "print(f\"\\nColumn Names:\")\n",
    "print(df_sample.columns.tolist())\n",
    "print(f\"\\nData Types:\")\n",
    "print(df_sample.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4a574",
   "metadata": {},
   "source": [
    "## --- Preview first few rows ---\n",
    "print(f\"\\nFirst 5 rows of Case {sample_case}:\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a72d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Summary:\n",
      "============================================================\n",
      "                     Missing_Count  Missing_Percentage\n",
      "detail                       26678           68.270338\n",
      "eventtime(utc+8)             22815           58.384728\n",
      "event                        19577           50.098523\n",
      "creationtime                 10897           27.885969\n",
      "modifiedtime                 10897           27.885969\n",
      "mftmodifiedtime              10897           27.885969\n",
      "accessedtime                 10897           27.885969\n",
      "full path                     4315           11.042301\n",
      "file/directory name            447            1.143895\n"
     ]
    }
   ],
   "source": [
    "# --- Check for missing values ---\n",
    "print(\"Missing Values Summary:\")\n",
    "print(\"=\" * 60)\n",
    "missing_summary = df_sample.isnull().sum()\n",
    "missing_pct = (missing_summary / len(df_sample)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_summary,\n",
    "    'Missing_Percentage': missing_pct\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f068fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution (Case 01):\n",
      "============================================================\n",
      "is_timestomped:\n",
      "is_timestomped\n",
      "0    39076\n",
      "1        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "is_suspicious_execution:\n",
      "is_suspicious_execution\n",
      "0    39076\n",
      "1        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Check label distribution ---\n",
    "print(\"\\nLabel Distribution (Case 01):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"is_timestomped:\")\n",
    "print(df_sample['is_timestomped'].value_counts())\n",
    "print(f\"\\nis_suspicious_execution:\")\n",
    "print(df_sample['is_suspicious_execution'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c03bda",
   "metadata": {},
   "source": [
    "## 4. Next Steps\n",
    "\n",
    "**To Do:**\n",
    "- [ ] Identify and drop irrelevant columns\n",
    "- [ ] Standardize timestamp columns to datetime format\n",
    "- [ ] Handle missing values (drop or impute)\n",
    "- [ ] Remove duplicate records\n",
    "- [ ] Merge all 12 LogFile datasets\n",
    "- [ ] Export Master_LogFile_Cleaned.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898a6cd",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning - Drop Rows with Null Event and Event Detail\n",
    "\n",
    "Remove rows where both `event` and `eventdetail` are null, as these records lack essential information for forensic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9102f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: Dropping rows with null event AND detail...\n",
      "============================================================\n",
      "Case 01: 39,077 → 19,500 records (Dropped: 19,577 | 50.10%)\n",
      "Case 02: 14,783 → 9,694 records (Dropped: 5,089 | 34.42%)\n",
      "Case 03: 24,063 → 12,575 records (Dropped: 11,488 | 47.74%)\n",
      "Case 04: 12,731 → 7,713 records (Dropped: 5,018 | 39.42%)\n",
      "Case 05: 14,242 → 7,530 records (Dropped: 6,712 | 47.13%)\n",
      "Case 06: 14,030 → 7,426 records (Dropped: 6,604 | 47.07%)\n",
      "Case 07: 23,737 → 12,824 records (Dropped: 10,913 | 45.97%)\n",
      "Case 08: 23,379 → 12,397 records (Dropped: 10,982 | 46.97%)\n",
      "Case 09: 25,688 → 13,414 records (Dropped: 12,274 | 47.78%)\n",
      "Case 10: 23,932 → 12,929 records (Dropped: 11,003 | 45.98%)\n",
      "Case 11: 14,083 → 7,459 records (Dropped: 6,624 | 47.04%)\n",
      "Case 12: 14,139 → 7,770 records (Dropped: 6,369 | 45.05%)\n",
      "------------------------------------------------------------\n",
      "Total: 243,884 → 131,231 records\n",
      "Total Dropped: 112,653 (46.19%)\n",
      "\n",
      "Verification: Check if any rows still have null event AND detail\n",
      "============================================================\n",
      "Case 01: 0 rows with both null\n",
      "Case 02: 0 rows with both null\n",
      "Case 03: 0 rows with both null\n",
      "Case 04: 0 rows with both null\n",
      "Case 05: 0 rows with both null\n",
      "Case 06: 0 rows with both null\n",
      "Case 07: 0 rows with both null\n",
      "Case 08: 0 rows with both null\n",
      "Case 09: 0 rows with both null\n",
      "Case 10: 0 rows with both null\n",
      "Case 11: 0 rows with both null\n",
      "Case 12: 0 rows with both null\n",
      "\n",
      "✓ Cleaning verified!\n"
     ]
    }
   ],
   "source": [
    "# --- Drop rows where BOTH event AND detail are null ---\n",
    "print(\"Cleaning: Dropping rows with null event AND detail...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store original counts\n",
    "original_counts = {}\n",
    "cleaned_dataframes = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    original_count = len(df)\n",
    "    original_counts[case_id] = original_count\n",
    "    \n",
    "    # Drop rows where BOTH event and detail are null\n",
    "    df_cleaned = df[~(df['event'].isnull() & df['detail'].isnull())].copy()\n",
    "    \n",
    "    dropped_count = original_count - len(df_cleaned)\n",
    "    dropped_pct = (dropped_count / original_count) * 100 if original_count > 0 else 0\n",
    "    \n",
    "    cleaned_dataframes[case_id] = df_cleaned\n",
    "    \n",
    "    print(f\"Case {case_id}: {original_count:,} → {len(df_cleaned):,} records \"\n",
    "          f\"(Dropped: {dropped_count:,} | {dropped_pct:.2f}%)\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calculate totals\n",
    "total_original = sum(original_counts.values())\n",
    "total_cleaned = sum(len(df) for df in cleaned_dataframes.values())\n",
    "total_dropped = total_original - total_cleaned\n",
    "total_dropped_pct = (total_dropped / total_original) * 100\n",
    "\n",
    "print(f\"Total: {total_original:,} → {total_cleaned:,} records\")\n",
    "print(f\"Total Dropped: {total_dropped:,} ({total_dropped_pct:.2f}%)\")\n",
    "\n",
    "# Update the main dictionary\n",
    "logfile_dataframes = cleaned_dataframes\n",
    "\n",
    "# --- Verify the cleaning ---\n",
    "print(\"\\nVerification: Check if any rows still have null event AND detail\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    null_both = df[df['event'].isnull() & df['detail'].isnull()]\n",
    "    print(f\"Case {case_id}: {len(null_both)} rows with both null\")\n",
    "\n",
    "print(\"\\n✓ Cleaning verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47b21d",
   "metadata": {},
   "source": [
    "## 6. Drop Irrelevant Columns\n",
    "\n",
    "Remove columns that are not relevant for timestomping detection:\n",
    "- `targetvcn`: Low-level NTFS virtual cluster number (physical storage location)\n",
    "- `clusterindex`: Cluster index information (disk fragmentation data)\n",
    "\n",
    "These columns are useful for data recovery and disk forensics, but do not contribute to timestamp manipulation analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e96e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping irrelevant columns: targetvcn, clusterindex\n",
      "============================================================\n",
      "Case 01: No columns to drop (already removed)\n",
      "Case 02: No columns to drop (already removed)\n",
      "Case 03: No columns to drop (already removed)\n",
      "Case 04: No columns to drop (already removed)\n",
      "Case 05: No columns to drop (already removed)\n",
      "Case 06: No columns to drop (already removed)\n",
      "Case 07: No columns to drop (already removed)\n",
      "Case 08: No columns to drop (already removed)\n",
      "Case 09: No columns to drop (already removed)\n",
      "Case 10: No columns to drop (already removed)\n",
      "Case 11: No columns to drop (already removed)\n",
      "Case 12: No columns to drop (already removed)\n",
      "------------------------------------------------------------\n",
      "✓ Irrelevant columns dropped successfully!\n",
      "\n",
      "Verification: Check remaining columns\n",
      "============================================================\n",
      "Remaining columns in Case 01: 16\n",
      "\n",
      "Column names:\n",
      "['lsn', 'eventtime(utc+8)', 'event', 'detail', 'file/directory name', 'full path', 'creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime', 'redo', 'target vcn', 'cluster index', 'is_timestomped', 'is_suspicious_execution', 'Case_ID']\n",
      "\n",
      "✓ Confirmed: targetvcn and clusterindex have been removed\n"
     ]
    }
   ],
   "source": [
    "# --- Drop irrelevant columns for timestomping detection ---\n",
    "print(\"Dropping irrelevant columns: targetvcn, clusterindex\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "columns_to_drop = ['targetvcn', 'clusterindex']\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    # Check which columns exist before dropping\n",
    "    existing_cols = [col for col in columns_to_drop if col in df.columns]\n",
    "    \n",
    "    if existing_cols:\n",
    "        df.drop(columns=existing_cols, inplace=True)\n",
    "        print(f\"Case {case_id}: Dropped {len(existing_cols)} columns - {existing_cols}\")\n",
    "    else:\n",
    "        print(f\"Case {case_id}: No columns to drop (already removed)\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"✓ Irrelevant columns dropped successfully!\")\n",
    "\n",
    "# --- Verify column removal ---\n",
    "print(\"\\nVerification: Check remaining columns\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_case = '01'\n",
    "df_sample = logfile_dataframes[sample_case]\n",
    "\n",
    "print(f\"Remaining columns in Case {sample_case}: {len(df_sample.columns)}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df_sample.columns.tolist())\n",
    "\n",
    "# Verify dropped columns are gone\n",
    "dropped_still_present = [col for col in columns_to_drop if col in df_sample.columns]\n",
    "if dropped_still_present:\n",
    "    print(f\"\\n⚠️ Warning: These columns still exist: {dropped_still_present}\")\n",
    "else:\n",
    "    print(f\"\\n✓ Confirmed: targetvcn and clusterindex have been removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa13fe5",
   "metadata": {},
   "source": [
    "## 7. Explore Null EventTime(UTC+8) Values\n",
    "\n",
    "Before imputing missing `eventtime(utc+8)` values, we need to understand:\n",
    "1. How many rows have null `eventtime(utc+8)`?\n",
    "2. What event types are associated with these null values?\n",
    "3. Which alternative timestamps are available for imputation?\n",
    "\n",
    "This analysis will help us make informed decisions about imputation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf6cda0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring rows with null eventtime(utc+8)...\n",
      "============================================================\n",
      "Case 01: 8,031 / 19,500 rows have null eventtime (41.18%)\n",
      "Case 02: 4,911 / 9,694 rows have null eventtime (50.66%)\n",
      "Case 03: 6,808 / 12,575 rows have null eventtime (54.14%)\n",
      "Case 04: 3,504 / 7,713 rows have null eventtime (45.43%)\n",
      "Case 05: 3,239 / 7,530 rows have null eventtime (43.01%)\n",
      "Case 06: 3,419 / 7,426 rows have null eventtime (46.04%)\n",
      "Case 07: 6,510 / 12,824 rows have null eventtime (50.76%)\n",
      "Case 08: 6,206 / 12,397 rows have null eventtime (50.06%)\n",
      "Case 09: 6,485 / 13,414 rows have null eventtime (48.35%)\n",
      "Case 10: 6,376 / 12,929 rows have null eventtime (49.32%)\n",
      "Case 11: 3,160 / 7,459 rows have null eventtime (42.36%)\n",
      "Case 12: 3,477 / 7,770 rows have null eventtime (44.75%)\n",
      "------------------------------------------------------------\n",
      "Total: 62,126 / 131,231 rows have null eventtime (47.34%)\n"
     ]
    }
   ],
   "source": [
    "# --- Explore null eventtime(utc+8) patterns ---\n",
    "print(\"Exploring rows with null eventtime(utc+8)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "null_eventtime_stats = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    total_rows = len(df)\n",
    "    null_eventtime = df['eventtime(utc+8)'].isnull().sum()\n",
    "    null_pct = (null_eventtime / total_rows) * 100 if total_rows > 0 else 0\n",
    "    \n",
    "    null_eventtime_stats[case_id] = {\n",
    "        'total': total_rows,\n",
    "        'null_count': null_eventtime,\n",
    "        'null_pct': null_pct\n",
    "    }\n",
    "    \n",
    "    print(f\"Case {case_id}: {null_eventtime:,} / {total_rows:,} rows have null eventtime ({null_pct:.2f}%)\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "total_null = sum(stats['null_count'] for stats in null_eventtime_stats.values())\n",
    "total_rows_all = sum(stats['total'] for stats in null_eventtime_stats.values())\n",
    "total_null_pct = (total_null / total_rows_all) * 100\n",
    "\n",
    "print(f\"Total: {total_null:,} / {total_rows_all:,} rows have null eventtime ({total_null_pct:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c86add3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Event types associated with null eventtime(utc+8):\n",
      "============================================================\n",
      "\n",
      "Total rows with null eventtime: 62,126\n",
      "\n",
      "Event type distribution:\n",
      "event\n",
      "Updating Modified Time                                17668\n",
      "File Deletion                                         14326\n",
      "Writing Content of Resident File                      13199\n",
      "Updating MFTModified Time                              5414\n",
      "Writing Content of Non-Resident File                   4354\n",
      "# Check Point                                          3885\n",
      "Changing FileAttribute                                  962\n",
      "Renaming File                                           720\n",
      "Updating MFTModified Time & Changing FileAttribute      445\n",
      "Updating Modified Time & Changing FileAttribute         304\n",
      "Move(Before)                                            219\n",
      "Move(After)                                             214\n",
      "Directory Deletion                                      201\n",
      "Time Reversal Event                                     200\n",
      "Time Reversal Event & Changing FileAttribute             13\n",
      "Renaming Directory                                        2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Analyze event types with null eventtime ---\n",
    "print(\"\\nEvent types associated with null eventtime(utc+8):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combine all cases to get overall picture\n",
    "all_null_eventtime_rows = []\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    null_rows = df[df['eventtime(utc+8)'].isnull()].copy()\n",
    "    null_rows['Case_ID'] = case_id\n",
    "    all_null_eventtime_rows.append(null_rows)\n",
    "\n",
    "df_null_eventtime = pd.concat(all_null_eventtime_rows, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal rows with null eventtime: {len(df_null_eventtime):,}\")\n",
    "print(\"\\nEvent type distribution:\")\n",
    "print(df_null_eventtime['event'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a66482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Availability of alternative timestamps for imputation:\n",
      "============================================================\n",
      "creationtime: 16,205 / 62,126 available (26.08%)\n",
      "modifiedtime: 16,205 / 62,126 available (26.08%)\n",
      "mftmodifiedtime: 16,205 / 62,126 available (26.08%)\n",
      "accessedtime: 16,205 / 62,126 available (26.08%)\n"
     ]
    }
   ],
   "source": [
    "# --- Check availability of alternative timestamps for imputation ---\n",
    "print(\"\\nAvailability of alternative timestamps for imputation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "timestamp_cols = ['creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime']\n",
    "\n",
    "for col in timestamp_cols:\n",
    "    available = df_null_eventtime[col].notna().sum()\n",
    "    available_pct = (available / len(df_null_eventtime)) * 100\n",
    "    print(f\"{col}: {available:,} / {len(df_null_eventtime):,} available ({available_pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6314e17",
   "metadata": {},
   "source": [
    "## 8. Conditional Imputation Strategy for EventTime(UTC+8)\n",
    "\n",
    "Based on the exploration above, we will implement a conditional imputation strategy that respects the semantic meaning of different event types:\n",
    "\n",
    "**Imputation Logic:**\n",
    "\n",
    "1. **File Creation Events** → Use `creationtime`\n",
    "   - Events like \"Creating a new file\", \"File Created\"\n",
    "   - Justification: Event time should match file creation time\n",
    "\n",
    "2. **File Modification Events** → Use `modifiedtime`\n",
    "   - Events like \"Updating Modified Time\", \"Writing Content\", \"Data Overwritten\"\n",
    "   - Justification: Event time should reflect when file was last modified\n",
    "\n",
    "3. **MFT-Related Events** → Use `mftmodifiedtime`\n",
    "   - Events like \"Updating MFTModified Time\", \"MFT Record Modified\"\n",
    "   - Justification: Event time should reflect MFT changes\n",
    "\n",
    "4. **File Access Events** → Use `accessedtime`\n",
    "   - Events like \"File Accessed\", \"Read Operation\"\n",
    "   - Justification: Event time should reflect access time\n",
    "\n",
    "5. **Fallback Strategy** → Use `mftmodifiedtime` (most reliable NTFS timestamp)\n",
    "   - For events that don't match above categories\n",
    "   - Justification: MFT Modified time is the most reliable and least manipulable timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd385033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implementing conditional imputation for eventtime(utc+8)...\n",
      "============================================================\n",
      "Case 01: Imputed 2,346 values | Still null: 5,685\n",
      "Case 02: Imputed 1,478 values | Still null: 3,433\n",
      "Case 03: Imputed 1,729 values | Still null: 5,079\n",
      "Case 04: Imputed 754 values | Still null: 2,750\n",
      "Case 05: Imputed 701 values | Still null: 2,538\n",
      "Case 06: Imputed 782 values | Still null: 2,637\n",
      "Case 07: Imputed 1,736 values | Still null: 4,774\n",
      "Case 08: Imputed 1,702 values | Still null: 4,504\n",
      "Case 09: Imputed 1,753 values | Still null: 4,732\n",
      "Case 10: Imputed 1,741 values | Still null: 4,635\n",
      "Case 11: Imputed 688 values | Still null: 2,472\n",
      "Case 12: Imputed 795 values | Still null: 2,682\n",
      "------------------------------------------------------------\n",
      "Total imputed: 16,205\n",
      "Total still null: 45,921\n"
     ]
    }
   ],
   "source": [
    "# --- Implement conditional imputation logic ---\n",
    "print(\"Implementing conditional imputation for eventtime(utc+8)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def impute_eventtime(row):\n",
    "    \"\"\"\n",
    "    Conditionally impute eventtime based on event type and available timestamps.\n",
    "    \"\"\"\n",
    "    # If eventtime already exists, return it\n",
    "    if pd.notna(row['eventtime(utc+8)']):\n",
    "        return row['eventtime(utc+8)']\n",
    "    \n",
    "    event = str(row['event']).lower() if pd.notna(row['event']) else ''\n",
    "    \n",
    "    # 1. File Creation Events → creationtime\n",
    "    if any(keyword in event for keyword in ['creat', 'new file', 'file creat']):\n",
    "        if pd.notna(row['creationtime']):\n",
    "            return row['creationtime']\n",
    "    \n",
    "    # 2. File Modification Events → modifiedtime\n",
    "    if any(keyword in event for keyword in ['modif', 'writ', 'updat', 'overwrite', 'truncat', 'data']):\n",
    "        if pd.notna(row['modifiedtime']):\n",
    "            return row['modifiedtime']\n",
    "    \n",
    "    # 3. MFT-Related Events → mftmodifiedtime\n",
    "    if any(keyword in event for keyword in ['mft', 'record']):\n",
    "        if pd.notna(row['mftmodifiedtime']):\n",
    "            return row['mftmodifiedtime']\n",
    "    \n",
    "    # 4. File Access Events → accessedtime\n",
    "    if any(keyword in event for keyword in ['access', 'read', 'open']):\n",
    "        if pd.notna(row['accessedtime']):\n",
    "            return row['accessedtime']\n",
    "    \n",
    "    # 5. Fallback Strategy → mftmodifiedtime (most reliable)\n",
    "    if pd.notna(row['mftmodifiedtime']):\n",
    "        return row['mftmodifiedtime']\n",
    "    \n",
    "    # 6. Final fallback → modifiedtime\n",
    "    if pd.notna(row['modifiedtime']):\n",
    "        return row['modifiedtime']\n",
    "    \n",
    "    # 7. Last resort → creationtime\n",
    "    if pd.notna(row['creationtime']):\n",
    "        return row['creationtime']\n",
    "    \n",
    "    # Cannot impute - return NaN\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "# Track imputation statistics\n",
    "imputation_stats = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    null_before = df['eventtime(utc+8)'].isnull().sum()\n",
    "    \n",
    "    # Apply conditional imputation\n",
    "    df['eventtime(utc+8)'] = df.apply(impute_eventtime, axis=1)\n",
    "    \n",
    "    null_after = df['eventtime(utc+8)'].isnull().sum()\n",
    "    imputed_count = null_before - null_after\n",
    "    \n",
    "    imputation_stats[case_id] = {\n",
    "        'null_before': null_before,\n",
    "        'null_after': null_after,\n",
    "        'imputed': imputed_count\n",
    "    }\n",
    "    \n",
    "    print(f\"Case {case_id}: Imputed {imputed_count:,} values | Still null: {null_after:,}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "total_imputed = sum(stats['imputed'] for stats in imputation_stats.values())\n",
    "total_still_null = sum(stats['null_after'] for stats in imputation_stats.values())\n",
    "\n",
    "print(f\"Total imputed: {total_imputed:,}\")\n",
    "print(f\"Total still null: {total_still_null:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d616d568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification: Remaining null eventtime(utc+8) after imputation\n",
      "============================================================\n",
      "Case 01: 5,685 rows still have null eventtime\n",
      "  → 5,685 of these have ALL timestamps null (will be dropped later)\n",
      "Case 02: 3,433 rows still have null eventtime\n",
      "  → 3,433 of these have ALL timestamps null (will be dropped later)\n",
      "Case 03: 5,079 rows still have null eventtime\n",
      "  → 5,079 of these have ALL timestamps null (will be dropped later)\n",
      "Case 04: 2,750 rows still have null eventtime\n",
      "  → 2,750 of these have ALL timestamps null (will be dropped later)\n",
      "Case 05: 2,538 rows still have null eventtime\n",
      "  → 2,538 of these have ALL timestamps null (will be dropped later)\n",
      "Case 06: 2,637 rows still have null eventtime\n",
      "  → 2,637 of these have ALL timestamps null (will be dropped later)\n",
      "Case 07: 4,774 rows still have null eventtime\n",
      "  → 4,774 of these have ALL timestamps null (will be dropped later)\n",
      "Case 08: 4,504 rows still have null eventtime\n",
      "  → 4,504 of these have ALL timestamps null (will be dropped later)\n",
      "Case 09: 4,732 rows still have null eventtime\n",
      "  → 4,732 of these have ALL timestamps null (will be dropped later)\n",
      "Case 10: 4,635 rows still have null eventtime\n",
      "  → 4,635 of these have ALL timestamps null (will be dropped later)\n",
      "Case 11: 2,472 rows still have null eventtime\n",
      "  → 2,472 of these have ALL timestamps null (will be dropped later)\n",
      "Case 12: 2,682 rows still have null eventtime\n",
      "  → 2,682 of these have ALL timestamps null (will be dropped later)\n",
      "\n",
      "✓ Imputation verified!\n"
     ]
    }
   ],
   "source": [
    "# --- Verify imputation results ---\n",
    "print(\"\\nVerification: Remaining null eventtime(utc+8) after imputation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    null_count = df['eventtime(utc+8)'].isnull().sum()\n",
    "    print(f\"Case {case_id}: {null_count:,} rows still have null eventtime\")\n",
    "    \n",
    "    # If there are still nulls, check if ALL timestamps are null for those rows\n",
    "    if null_count > 0:\n",
    "        null_rows = df[df['eventtime(utc+8)'].isnull()]\n",
    "        all_ts_null = null_rows[['creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime']].isnull().all(axis=1).sum()\n",
    "        print(f\"  → {all_ts_null:,} of these have ALL timestamps null (will be dropped later)\")\n",
    "\n",
    "print(\"\\n✓ Imputation verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4e5f1",
   "metadata": {},
   "source": [
    "## 9. Critical Check: Preserve Labeled Rows Before Dropping\n",
    "\n",
    "**IMPORTANT**: Before dropping rows with null timestamps, we must preserve rows labeled as `is_timestomped=1` or `is_suspicious_execution=1`. These are our ground truth data and have forensic value even with incomplete timestamps.\n",
    "\n",
    "**Strategy:**\n",
    "1. Identify how many labeled rows would be lost if we dropped all null timestamp rows\n",
    "2. Preserve ALL labeled rows regardless of timestamp completeness\n",
    "3. Attempt to extract timestamp information from `detail` column for preserved rows\n",
    "4. Flag rows with incomplete timestamps for model awareness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7a73f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Check: Labeled rows with null eventtime(utc+8)\n",
      "============================================================\n",
      "Case 01: 1 timestomped, 0 suspicious rows at risk\n",
      "Case 02: 1 timestomped, 0 suspicious rows at risk\n",
      "Case 03: 1 timestomped, 0 suspicious rows at risk\n",
      "Case 04: 1 timestomped, 0 suspicious rows at risk\n",
      "Case 05: 1 timestomped, 0 suspicious rows at risk\n",
      "Case 06: 2 timestomped, 0 suspicious rows at risk\n",
      "Case 09: 1 timestomped, 0 suspicious rows at risk\n",
      "------------------------------------------------------------\n",
      "Total labeled rows at risk: 8\n",
      "  → Timestomped: 8\n",
      "  → Suspicious execution: 0\n",
      "\n",
      "⚠️  WARNING: These 8 labeled rows must be preserved!\n"
     ]
    }
   ],
   "source": [
    "# --- Check how many labeled rows would be lost ---\n",
    "print(\"Critical Check: Labeled rows with null eventtime(utc+8)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "labeled_at_risk = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    null_eventtime = df['eventtime(utc+8)'].isnull()\n",
    "    \n",
    "    # Check labeled rows with null eventtime\n",
    "    timestomped_null = df[null_eventtime & (df['is_timestomped'] == 1)]\n",
    "    suspicious_null = df[null_eventtime & (df['is_suspicious_execution'] == 1)]\n",
    "    \n",
    "    labeled_at_risk[case_id] = {\n",
    "        'timestomped': len(timestomped_null),\n",
    "        'suspicious': len(suspicious_null),\n",
    "        'total_labeled': len(timestomped_null) + len(suspicious_null)\n",
    "    }\n",
    "    \n",
    "    if labeled_at_risk[case_id]['total_labeled'] > 0:\n",
    "        print(f\"Case {case_id}: {labeled_at_risk[case_id]['timestomped']} timestomped, \"\n",
    "              f\"{labeled_at_risk[case_id]['suspicious']} suspicious rows at risk\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "total_timestomped_at_risk = sum(stats['timestomped'] for stats in labeled_at_risk.values())\n",
    "total_suspicious_at_risk = sum(stats['suspicious'] for stats in labeled_at_risk.values())\n",
    "total_labeled_at_risk = sum(stats['total_labeled'] for stats in labeled_at_risk.values())\n",
    "\n",
    "print(f\"Total labeled rows at risk: {total_labeled_at_risk}\")\n",
    "print(f\"  → Timestomped: {total_timestomped_at_risk}\")\n",
    "print(f\"  → Suspicious execution: {total_suspicious_at_risk}\")\n",
    "\n",
    "if total_labeled_at_risk > 0:\n",
    "    print(f\"\\n⚠️  WARNING: These {total_labeled_at_risk} labeled rows must be preserved!\")\n",
    "else:\n",
    "    print(\"\\n✓ No labeled rows at risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de48c01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examining 'detail' column content for labeled rows with null timestamps:\n",
      "============================================================\n",
      "\n",
      "Case 01: 1 labeled rows\n",
      "Sample 'detail' column content:\n",
      "                     event                                             detail  \\\n",
      "35918  Time Reversal Event  CreationTime : 2023-12-23 00:21:36 -> 2022-12-...   \n",
      "\n",
      "       is_timestomped  is_suspicious_execution  \n",
      "35918               1                        0  \n",
      "----------------------------------------\n",
      "\n",
      "Case 02: 1 labeled rows\n",
      "Sample 'detail' column content:\n",
      "                     event                                             detail  \\\n",
      "12919  Time Reversal Event  ModifiedTime : 2023-12-26 15:16:49 -> 2022-12-...   \n",
      "\n",
      "       is_timestomped  is_suspicious_execution  \n",
      "12919               1                        0  \n",
      "----------------------------------------\n",
      "\n",
      "Case 03: 1 labeled rows\n",
      "Sample 'detail' column content:\n",
      "                     event                                             detail  \\\n",
      "22427  Time Reversal Event  CreationTime : 2023-12-26 00:36:29 -> 2022-12-...   \n",
      "\n",
      "       is_timestomped  is_suspicious_execution  \n",
      "22427               1                        0  \n",
      "----------------------------------------\n",
      "\n",
      "Case 04: 1 labeled rows\n",
      "Sample 'detail' column content:\n",
      "                     event                                             detail  \\\n",
      "10041  Time Reversal Event  CreationTime : 2023-12-31 00:00:31 -> 2022-12-...   \n",
      "\n",
      "       is_timestomped  is_suspicious_execution  \n",
      "10041               1                        0  \n",
      "----------------------------------------\n",
      "\n",
      "Case 05: 1 labeled rows\n",
      "Sample 'detail' column content:\n",
      "                     event                                             detail  \\\n",
      "11440  Time Reversal Event  ModifiedTime : 2023-12-31 00:27:50 -> 2022-12-...   \n",
      "\n",
      "       is_timestomped  is_suspicious_execution  \n",
      "11440               1                        0  \n",
      "----------------------------------------\n",
      "\n",
      "Case 06: 2 labeled rows\n",
      "Sample 'detail' column content:\n",
      "                     event                                             detail  \\\n",
      "11207  Time Reversal Event  CreationTime : 2023-12-31 00:38:02 -> 2022-12-...   \n",
      "11208  Time Reversal Event  ModifiedTime : 2023-12-31 00:38:02 -> 2022-12-...   \n",
      "\n",
      "       is_timestomped  is_suspicious_execution  \n",
      "11207               1                        0  \n",
      "11208               1                        0  \n",
      "----------------------------------------\n",
      "\n",
      "Case 09: 1 labeled rows\n",
      "Sample 'detail' column content:\n",
      "                     event                                             detail  \\\n",
      "22940  Time Reversal Event  CreationTime : 2023-12-26 23:03:30 -> 2022-12-...   \n",
      "\n",
      "       is_timestomped  is_suspicious_execution  \n",
      "22940               1                        0  \n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Examine the detail column of labeled rows with null timestamps ---\n",
    "print(\"\\nExamining 'detail' column content for labeled rows with null timestamps:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    null_eventtime = df['eventtime(utc+8)'].isnull()\n",
    "    labeled_rows = df[null_eventtime & ((df['is_timestomped'] == 1) | (df['is_suspicious_execution'] == 1))]\n",
    "    \n",
    "    if len(labeled_rows) > 0:\n",
    "        print(f\"\\nCase {case_id}: {len(labeled_rows)} labeled rows\")\n",
    "        print(\"Sample 'detail' column content:\")\n",
    "        print(labeled_rows[['event', 'detail', 'is_timestomped', 'is_suspicious_execution']].head(3))\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e7f702",
   "metadata": {},
   "source": [
    "## 10. Preserve Labeled Rows and Flag Incomplete Timestamps\n",
    "\n",
    "**Preservation Logic:**\n",
    "- Keep ALL rows where `is_timestomped == 1` OR `is_suspicious_execution == 1`\n",
    "- Keep all rows with valid `eventtime(utc+8)`\n",
    "- Drop ONLY unlabeled rows with null `eventtime(utc+8)`\n",
    "\n",
    "**Additional Steps:**\n",
    "- Create `has_incomplete_timestamps` flag (1 = incomplete, 0 = complete)\n",
    "- This flag helps the ML model understand data quality context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e31ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 'has_incomplete_timestamps' flag...\n",
      "============================================================\n",
      "Case 01: 5,685 rows flagged with incomplete timestamps\n",
      "Case 02: 3,433 rows flagged with incomplete timestamps\n",
      "Case 03: 5,079 rows flagged with incomplete timestamps\n",
      "Case 04: 2,750 rows flagged with incomplete timestamps\n",
      "Case 05: 2,538 rows flagged with incomplete timestamps\n",
      "Case 06: 2,637 rows flagged with incomplete timestamps\n",
      "Case 07: 4,774 rows flagged with incomplete timestamps\n",
      "Case 08: 4,504 rows flagged with incomplete timestamps\n",
      "Case 09: 4,732 rows flagged with incomplete timestamps\n",
      "Case 10: 4,635 rows flagged with incomplete timestamps\n",
      "Case 11: 2,472 rows flagged with incomplete timestamps\n",
      "Case 12: 2,682 rows flagged with incomplete timestamps\n",
      "\n",
      "✓ Flag created successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Create flag for incomplete timestamps ---\n",
    "print(\"Creating 'has_incomplete_timestamps' flag...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    # Flag rows where eventtime is null (before any dropping)\n",
    "    df['has_incomplete_timestamps'] = df['eventtime(utc+8)'].isnull().astype(int)\n",
    "    \n",
    "    incomplete_count = df['has_incomplete_timestamps'].sum()\n",
    "    print(f\"Case {case_id}: {incomplete_count:,} rows flagged with incomplete timestamps\")\n",
    "\n",
    "print(\"\\n✓ Flag created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24254cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropping rows with null eventtime (PRESERVING labeled rows)...\n",
      "============================================================\n",
      "Case 01: 19,500 → 13,816 records (Dropped: 5,684 | Preserved labeled: 1)\n",
      "Case 02: 9,694 → 6,262 records (Dropped: 3,432 | Preserved labeled: 1)\n",
      "Case 03: 12,575 → 7,497 records (Dropped: 5,078 | Preserved labeled: 1)\n",
      "Case 04: 7,713 → 4,964 records (Dropped: 2,749 | Preserved labeled: 1)\n",
      "Case 05: 7,530 → 4,993 records (Dropped: 2,537 | Preserved labeled: 1)\n",
      "Case 06: 7,426 → 4,791 records (Dropped: 2,635 | Preserved labeled: 2)\n",
      "Case 07: 12,824 → 8,050 records (Dropped: 4,774 | Preserved labeled: 0)\n",
      "Case 08: 12,397 → 7,893 records (Dropped: 4,504 | Preserved labeled: 0)\n",
      "Case 09: 13,414 → 8,683 records (Dropped: 4,731 | Preserved labeled: 1)\n",
      "Case 10: 12,929 → 8,294 records (Dropped: 4,635 | Preserved labeled: 0)\n",
      "Case 11: 7,459 → 4,987 records (Dropped: 2,472 | Preserved labeled: 0)\n",
      "Case 12: 7,770 → 5,088 records (Dropped: 2,682 | Preserved labeled: 0)\n",
      "------------------------------------------------------------\n",
      "Total: 131,231 → 85,318 records\n",
      "Total Dropped: 45,913 (34.99%)\n",
      "Total Labeled Rows Preserved: 8\n",
      "\n",
      "✓ Selective dropping completed - all labeled rows preserved!\n"
     ]
    }
   ],
   "source": [
    "# --- Selective dropping: preserve labeled rows ---\n",
    "print(\"\\nDropping rows with null eventtime (PRESERVING labeled rows)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "drop_stats = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Create mask: Keep if eventtime is NOT null OR if row is labeled\n",
    "    keep_mask = (\n",
    "        df['eventtime(utc+8)'].notna() |  # Has valid eventtime\n",
    "        (df['is_timestomped'] == 1) |      # Is timestomped\n",
    "        (df['is_suspicious_execution'] == 1)  # Is suspicious execution\n",
    "    )\n",
    "    \n",
    "    df_cleaned = df[keep_mask].copy()\n",
    "    \n",
    "    rows_after = len(df_cleaned)\n",
    "    dropped = rows_before - rows_after\n",
    "    dropped_pct = (dropped / rows_before) * 100 if rows_before > 0 else 0\n",
    "    \n",
    "    # Count preserved labeled rows\n",
    "    preserved_labeled = len(df_cleaned[df_cleaned['has_incomplete_timestamps'] == 1])\n",
    "    \n",
    "    drop_stats[case_id] = {\n",
    "        'before': rows_before,\n",
    "        'after': rows_after,\n",
    "        'dropped': dropped,\n",
    "        'dropped_pct': dropped_pct,\n",
    "        'preserved_labeled': preserved_labeled\n",
    "    }\n",
    "    \n",
    "    logfile_dataframes[case_id] = df_cleaned\n",
    "    \n",
    "    print(f\"Case {case_id}: {rows_before:,} → {rows_after:,} records \"\n",
    "          f\"(Dropped: {dropped:,} | Preserved labeled: {preserved_labeled})\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "total_before = sum(stats['before'] for stats in drop_stats.values())\n",
    "total_after = sum(stats['after'] for stats in drop_stats.values())\n",
    "total_dropped = total_before - total_after\n",
    "total_preserved_labeled = sum(stats['preserved_labeled'] for stats in drop_stats.values())\n",
    "total_dropped_pct = (total_dropped / total_before) * 100\n",
    "\n",
    "print(f\"Total: {total_before:,} → {total_after:,} records\")\n",
    "print(f\"Total Dropped: {total_dropped:,} ({total_dropped_pct:.2f}%)\")\n",
    "print(f\"Total Labeled Rows Preserved: {total_preserved_labeled}\")\n",
    "print(\"\\n✓ Selective dropping completed - all labeled rows preserved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce6db4",
   "metadata": {},
   "source": [
    "## 11. Analyze Remaining Missing Values\n",
    "\n",
    "Now that we've handled `eventtime(utc+8)` and preserved labeled rows, let's examine other columns with missing values and determine appropriate handling strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57d4e86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing remaining missing values across all datasets...\n",
      "============================================================\n",
      "Total records across all cases: 85,318\n",
      "\n",
      "Missing Values Summary:\n",
      "============================================================\n",
      "             Column  Missing_Count  Missing_Percentage\n",
      "             detail          54156           63.475468\n",
      "    mftmodifiedtime          28624           33.549778\n",
      "       creationtime          28624           33.549778\n",
      "       modifiedtime          28624           33.549778\n",
      "       accessedtime          28624           33.549778\n",
      "          full path          11106           13.017183\n",
      "file/directory name           2546            2.984130\n",
      "   eventtime(utc+8)              8            0.009377\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Comprehensive missing values analysis ---\n",
    "print(\"Analyzing remaining missing values across all datasets...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combine all datasets to get overall picture\n",
    "all_dataframes = pd.concat(logfile_dataframes.values(), ignore_index=True)\n",
    "\n",
    "print(f\"Total records across all cases: {len(all_dataframes):,}\")\n",
    "print(f\"\\nMissing Values Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "missing_summary = all_dataframes.isnull().sum()\n",
    "missing_pct = (missing_summary / len(all_dataframes)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_summary.index,\n",
    "    'Missing_Count': missing_summary.values,\n",
    "    'Missing_Percentage': missing_pct.values\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "# Show only columns with missing values\n",
    "missing_df_filtered = missing_df[missing_df['Missing_Count'] > 0]\n",
    "\n",
    "print(missing_df_filtered.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331dcba2",
   "metadata": {},
   "source": [
    "### 11.1. Deep Dive: Extract Timestamps from Detail Column\n",
    "\n",
    "We have 28,624 rows missing all four core timestamps (`creationtime`, `modifiedtime`, `mftmodifiedtime`, `accessedtime`). \n",
    "\n",
    "**Investigation Goals:**\n",
    "1. Check if `detail` column contains parseable timestamp information\n",
    "2. Identify patterns and formats of timestamps in `detail`\n",
    "3. Extract and parse timestamps where possible\n",
    "4. Assess data quality and utility for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2732f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing rows with missing timestamps...\n",
      "============================================================\n",
      "Rows missing all 4 timestamps: 28,624\n",
      "\n",
      "Of these, how many have detail values?\n",
      "  → 26,764 rows have detail (93.50%)\n",
      "  → 1,860 rows have NO detail (6.50%)\n",
      "\n",
      "Sample 'detail' column content from rows missing timestamps:\n",
      "============================================================\n",
      "\n",
      "Showing 15 random samples:\n",
      "\n",
      "Event: Updating MFTModified Time\n",
      "Detail: MFTModifiedTime : 2023-12-19 15:16:05 -> 2023-12-23 00:16:14...\n",
      "------------------------------------------------------------\n",
      "Event: Updating MFTModified Time\n",
      "Detail: MFTModifiedTime : 2023-04-19 16:09:39 -> 2023-12-23 00:16:21...\n",
      "------------------------------------------------------------\n",
      "Event: Time Reversal Event\n",
      "Detail: ModifiedTime : 2023-12-23 00:14:24 -> 2000-01-01 08:00:00(Zero in 100-nanoseconds)...\n",
      "------------------------------------------------------------\n",
      "Event: Writing Content of Resident File\n",
      "Detail: Writing Size : 512...\n",
      "------------------------------------------------------------\n",
      "Event: Writing Content of Resident File\n",
      "Detail: Writing Size : 512...\n",
      "------------------------------------------------------------\n",
      "Event: Changing FileAttribute\n",
      "Detail: NotContentIndexed/ RandomAccess -> 0x00000000...\n",
      "------------------------------------------------------------\n",
      "Event: Writing Content of Non-Resident File\n",
      "Detail: Data Runs(in Volume) : 1011100(1)...\n",
      "------------------------------------------------------------\n",
      "Event: Writing Content of Resident File\n",
      "Detail: Writing Size : 512...\n",
      "------------------------------------------------------------\n",
      "Event: Writing Content of Resident File\n",
      "Detail: Writing Size : 512...\n",
      "------------------------------------------------------------\n",
      "Event: Writing Content of Non-Resident File\n",
      "Detail: Data Runs(in Volume) : 294152(2)...\n",
      "------------------------------------------------------------\n",
      "Event: Writing Content of Non-Resident File\n",
      "Detail: Data Runs(in Volume) : 148415(1)...\n",
      "------------------------------------------------------------\n",
      "Event: Updating MFTModified Time\n",
      "Detail: MFTModifiedTime : 2023-12-23 00:16:14 -> 2023-12-23 00:16:38...\n",
      "------------------------------------------------------------\n",
      "Event: Time Reversal Event\n",
      "Detail: ModifiedTime : 2023-12-31 01:15:23 -> 2023-12-28 20:38:16/ MFTModifiedTime : 2023-12-31 01:15:23 -> 2023-12-28 20:38:16...\n",
      "------------------------------------------------------------\n",
      "Event: Updating Modified Time\n",
      "Detail: ModifiedTime : 2023-12-28 09:29:51 -> 2023-12-28 21:40:01...\n",
      "------------------------------------------------------------\n",
      "Event: Renaming File\n",
      "Detail: V01.log -> V01000C9.log...\n",
      "------------------------------------------------------------\n",
      "\n",
      "Searching for timestamp patterns in 'detail' column...\n",
      "============================================================\n",
      "ISO_format: 12,675 matches (47.36%)\n",
      "Epoch_timestamp: 34 matches (0.13%)\n",
      "\n",
      "============================================================\n",
      "✓ Found timestamp patterns in 12,709 rows\n",
      "\n",
      "Searching for timestamp-related keywords in 'detail' column...\n",
      "============================================================\n",
      "'modified': 12,511 matches (46.75%)\n",
      "'accessed': 278 matches (1.04%)\n",
      "'mft': 3,862 matches (14.43%)\n",
      "'time': 12,675 matches (47.36%)\n",
      "'date': 135 matches (0.50%)\n",
      "'Creation': 442 matches (1.65%)\n",
      "'Access': 670 matches (2.50%)\n",
      "============================================================\n",
      "\n",
      "Event types for rows missing all timestamps:\n",
      "============================================================\n",
      "event\n",
      "Updating Modified Time                                7260\n",
      "Writing Content of Resident File                      5144\n",
      "Writing Content of Non-Resident File                  5054\n",
      "Time Reversal Event                                   2622\n",
      "Updating MFTModified Time                             2388\n",
      "Renaming File                                         2075\n",
      "Changing FileAttribute                                1816\n",
      "Move(After)                                            626\n",
      "Move(Before)                                           618\n",
      "# Check Point                                          616\n",
      "Time Reversal Event & Changing FileAttribute           275\n",
      "Updating Modified Time & Changing FileAttribute         84\n",
      "Updating MFTModified Time & Changing FileAttribute      46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique event types: 13\n",
      "\n",
      "Checking labeled rows (timestomped/suspicious) with missing timestamps:\n",
      "============================================================\n",
      "Timestomped rows missing all timestamps: 14\n",
      "Suspicious execution rows missing all timestamps: 0\n",
      "\n",
      "⚠️  CRITICAL: Some timestomped rows have no timestamps!\n",
      "Sample of timestomped rows with missing timestamps:\n",
      "                     event                                             detail  \\\n",
      "13683  Time Reversal Event  CreationTime : 2023-12-23 00:21:36 -> 2022-12-...   \n",
      "19734  Time Reversal Event  ModifiedTime : 2023-12-26 15:16:49 -> 2022-12-...   \n",
      "27483  Time Reversal Event  CreationTime : 2023-12-26 00:36:29 -> 2022-12-...   \n",
      "32453  Time Reversal Event  CreationTime : 2023-12-31 00:00:31 -> 2022-12-...   \n",
      "37431  Time Reversal Event  ModifiedTime : 2023-12-31 00:27:50 -> 2022-12-...   \n",
      "\n",
      "       has_incomplete_timestamps  \n",
      "13683                          1  \n",
      "19734                          1  \n",
      "27483                          1  \n",
      "32453                          1  \n",
      "37431                          1  \n"
     ]
    }
   ],
   "source": [
    "# --- Examine rows with missing timestamps ---\n",
    "print(\"Analyzing rows with missing timestamps...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combine all datasets\n",
    "all_dataframes = pd.concat(logfile_dataframes.values(), ignore_index=True)\n",
    "\n",
    "# Identify rows missing all 4 core timestamps\n",
    "missing_timestamps_mask = (\n",
    "    all_dataframes['creationtime'].isnull() &\n",
    "    all_dataframes['modifiedtime'].isnull() &\n",
    "    all_dataframes['mftmodifiedtime'].isnull() &\n",
    "    all_dataframes['accessedtime'].isnull()\n",
    ")\n",
    "\n",
    "df_missing_timestamps = all_dataframes[missing_timestamps_mask].copy()\n",
    "\n",
    "print(f\"Rows missing all 4 timestamps: {len(df_missing_timestamps):,}\")\n",
    "print(f\"\\nOf these, how many have detail values?\")\n",
    "has_detail = df_missing_timestamps['detail'].notna().sum()\n",
    "has_detail_pct = (has_detail / len(df_missing_timestamps)) * 100\n",
    "print(f\"  → {has_detail:,} rows have detail ({has_detail_pct:.2f}%)\")\n",
    "\n",
    "no_detail = df_missing_timestamps['detail'].isnull().sum()\n",
    "print(f\"  → {no_detail:,} rows have NO detail ({100-has_detail_pct:.2f}%)\")\n",
    "\n",
    "# --- Sample the detail column content ---\n",
    "print(\"\\nSample 'detail' column content from rows missing timestamps:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_with_detail = df_missing_timestamps[df_missing_timestamps['detail'].notna()]\n",
    "\n",
    "print(f\"\\nShowing 15 random samples:\\n\")\n",
    "for idx, row in df_with_detail.sample(min(15, len(df_with_detail))).iterrows():\n",
    "    print(f\"Event: {row['event']}\")\n",
    "    print(f\"Detail: {row['detail'][:150]}...\")  # First 150 chars\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# --- Check for timestamp patterns in detail column ---\n",
    "import re\n",
    "\n",
    "print(\"\\nSearching for timestamp patterns in 'detail' column...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Common timestamp patterns\n",
    "timestamp_patterns = {\n",
    "    'ISO_format': r'\\d{4}-\\d{2}-\\d{2}[T ]\\d{2}:\\d{2}:\\d{2}',\n",
    "    'Slash_format': r'\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2}',\n",
    "    'Windows_format': r'\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}:\\d{2} [AP]M',\n",
    "    'Epoch_timestamp': r'\\b\\d{10,13}\\b',  # Unix epoch seconds or milliseconds\n",
    "    'Year_first': r'\\d{4}\\.\\d{2}\\.\\d{2} \\d{2}:\\d{2}:\\d{2}'\n",
    "}\n",
    "\n",
    "pattern_matches = {}\n",
    "\n",
    "for pattern_name, pattern in timestamp_patterns.items():\n",
    "    matches = df_with_detail['detail'].str.contains(pattern, regex=True, na=False).sum()\n",
    "    pattern_matches[pattern_name] = matches\n",
    "    if matches > 0:\n",
    "        print(f\"{pattern_name}: {matches:,} matches ({matches/len(df_with_detail)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "if sum(pattern_matches.values()) == 0:\n",
    "    print(\"⚠️  No timestamp patterns found in 'detail' column\")\n",
    "else:\n",
    "    print(f\"✓ Found timestamp patterns in {sum(pattern_matches.values()):,} rows\")\n",
    "\n",
    "# --- Check if detail contains specific timestamp keywords ---\n",
    "print(\"\\nSearching for timestamp-related keywords in 'detail' column...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "timestamp_keywords = [\n",
    "    'created', 'modified', 'accessed', 'mft', \n",
    "    'time', 'date', 'timestamp', 'when',\n",
    "    'Creation', 'Modification', 'Access'\n",
    "]\n",
    "\n",
    "keyword_matches = {}\n",
    "\n",
    "for keyword in timestamp_keywords:\n",
    "    matches = df_with_detail['detail'].str.contains(keyword, case=False, na=False).sum()\n",
    "    if matches > 0:\n",
    "        keyword_matches[keyword] = matches\n",
    "        print(f\"'{keyword}': {matches:,} matches ({matches/len(df_with_detail)*100:.2f}%)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Analyze event types for rows missing all timestamps ---\n",
    "print(\"\\nEvent types for rows missing all timestamps:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "event_distribution = df_missing_timestamps['event'].value_counts()\n",
    "print(event_distribution.head(20))\n",
    "\n",
    "print(f\"\\nTotal unique event types: {df_missing_timestamps['event'].nunique()}\")\n",
    "\n",
    "# --- Check labeled rows among those missing timestamps ---\n",
    "print(\"\\nChecking labeled rows (timestomped/suspicious) with missing timestamps:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "timestomped_missing_ts = df_missing_timestamps[df_missing_timestamps['is_timestomped'] == 1]\n",
    "suspicious_missing_ts = df_missing_timestamps[df_missing_timestamps['is_suspicious_execution'] == 1]\n",
    "\n",
    "print(f\"Timestomped rows missing all timestamps: {len(timestomped_missing_ts):,}\")\n",
    "print(f\"Suspicious execution rows missing all timestamps: {len(suspicious_missing_ts):,}\")\n",
    "\n",
    "if len(timestomped_missing_ts) > 0:\n",
    "    print(\"\\n⚠️  CRITICAL: Some timestomped rows have no timestamps!\")\n",
    "    print(\"Sample of timestomped rows with missing timestamps:\")\n",
    "    print(timestomped_missing_ts[['event', 'detail', 'has_incomplete_timestamps']].head(5))\n",
    "\n",
    "if len(suspicious_missing_ts) > 0:\n",
    "    print(\"\\n⚠️  CRITICAL: Some suspicious execution rows have no timestamps!\")\n",
    "    print(\"Sample of suspicious rows with missing timestamps:\")\n",
    "    print(suspicious_missing_ts[['event', 'detail','has_incomplete_timestamps']].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fc672",
   "metadata": {},
   "source": [
    "### 11.2. Assessment and Decision Point\n",
    "\n",
    "Based on the analysis above, we need to determine:\n",
    "\n",
    "1. **If detail contains extractable timestamps** → Extract and populate timestamp columns\n",
    "2. **If detail provides qualitative information** → Keep for text feature engineering\n",
    "3. **If rows have no timestamps AND no useful detail** → Consider dropping (unless labeled)\n",
    "4. **For labeled rows** → Always preserve, but flag data quality issues\n",
    "\n",
    "Next steps will depend on what we find in the analysis above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d5822b",
   "metadata": {},
   "source": [
    "### 11.3. Extract Timestamps from Detail Column\n",
    "\n",
    "**Critical Discovery**: The `detail` column contains parseable timestamp information for rows missing timestamp columns.\n",
    "\n",
    "**Extraction Strategy:**\n",
    "1. Parse timestamps from `detail` using regex patterns (ISO format: `YYYY-MM-DD HH:MM:SS`)\n",
    "2. Map extracted timestamps to appropriate columns based on event type:\n",
    "   - `Time Reversal Event` → Extract before/after timestamps\n",
    "   - `Updating Modified Time` → Extract `modifiedtime`\n",
    "   - `Updating MFTModified Time` → Extract `mftmodifiedtime`\n",
    "   - `CreationTime changes` → Extract `creationtime`\n",
    "3. Use `eventtime(utc+8)` as fallback for extracted timestamps\n",
    "4. Preserve all 14 labeled timestomped rows\n",
    "\n",
    "**Impact**: This will recover timestamp data for ~12,675 rows (47% of missing timestamp rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24dd0053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting timestamps from 'detail' column...\n",
      "============================================================\n",
      "Case 01: Extracted timestamps for 3,232 rows | Still missing all: 1,981\n",
      "Case 02: Extracted timestamps for 580 rows | Still missing all: 1,135\n",
      "Case 03: Extracted timestamps for 885 rows | Still missing all: 1,319\n",
      "Case 04: Extracted timestamps for 742 rows | Still missing all: 1,114\n",
      "Case 05: Extracted timestamps for 823 rows | Still missing all: 1,152\n",
      "Case 06: Extracted timestamps for 760 rows | Still missing all: 1,050\n",
      "Case 07: Extracted timestamps for 940 rows | Still missing all: 1,486\n",
      "Case 08: Extracted timestamps for 945 rows | Still missing all: 1,449\n",
      "Case 09: Extracted timestamps for 1,086 rows | Still missing all: 1,529\n",
      "Case 10: Extracted timestamps for 1,021 rows | Still missing all: 1,456\n",
      "Case 11: Extracted timestamps for 842 rows | Still missing all: 1,134\n",
      "Case 12: Extracted timestamps for 819 rows | Still missing all: 1,144\n",
      "------------------------------------------------------------\n",
      "Total rows with timestamps extracted: 12,675\n",
      "Total rows still missing all timestamps: 15,949\n",
      "\n",
      "Verification: Check timestomped rows after extraction\n",
      "============================================================\n",
      "Total timestomped rows: 14\n",
      "Timestomped rows still missing ALL timestamps: 0\n",
      "\n",
      "✓ All timestomped rows now have at least one timestamp!\n",
      "\n",
      "Timestomped rows with extracted timestamps: 14\n",
      "\n",
      "Sample:\n",
      "                     event         creationtime         modifiedtime  \\\n",
      "13683  Time Reversal Event  2023-12-23 00:21:36                  NaN   \n",
      "19734  Time Reversal Event                  NaN  2023-12-26 15:16:49   \n",
      "27483  Time Reversal Event  2023-12-26 00:36:29  2023-12-26 00:36:29   \n",
      "32453  Time Reversal Event  2023-12-31 00:00:31                  NaN   \n",
      "37431  Time Reversal Event                  NaN  2023-12-31 00:27:50   \n",
      "\n",
      "      mftmodifiedtime  \n",
      "13683             NaN  \n",
      "19734             NaN  \n",
      "27483             NaN  \n",
      "32453             NaN  \n",
      "37431             NaN  \n"
     ]
    }
   ],
   "source": [
    "# --- Extract timestamps from detail column ---\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Extracting timestamps from 'detail' column...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def extract_timestamps_from_detail(row):\n",
    "    \"\"\"\n",
    "    Extract timestamps from the detail column and populate missing timestamp fields.\n",
    "    Returns a dictionary with extracted timestamp values.\n",
    "    \"\"\"\n",
    "    detail = str(row['detail']) if pd.notna(row['detail']) else ''\n",
    "    event = str(row['event']).lower() if pd.notna(row['event']) else ''\n",
    "    \n",
    "    # ISO format pattern: YYYY-MM-DD HH:MM:SS\n",
    "    iso_pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})'\n",
    "    \n",
    "    # Find all timestamps in detail\n",
    "    timestamps = re.findall(iso_pattern, detail)\n",
    "    \n",
    "    extracted = {\n",
    "        'creationtime': row['creationtime'],\n",
    "        'modifiedtime': row['modifiedtime'],\n",
    "        'mftmodifiedtime': row['mftmodifiedtime'],\n",
    "        'accessedtime': row['accessedtime']\n",
    "    }\n",
    "    \n",
    "    if len(timestamps) == 0:\n",
    "        return extracted\n",
    "    \n",
    "    # Parse detail content for specific timestamp types\n",
    "    detail_lower = detail.lower()\n",
    "    \n",
    "    # 1. CreationTime extraction\n",
    "    if 'creationtime' in detail_lower and pd.isna(row['creationtime']):\n",
    "        creation_match = re.search(r'creationtime\\s*:\\s*(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', detail, re.IGNORECASE)\n",
    "        if creation_match:\n",
    "            extracted['creationtime'] = creation_match.group(1)\n",
    "    \n",
    "    # 2. ModifiedTime extraction\n",
    "    if 'modifiedtime' in detail_lower and pd.isna(row['modifiedtime']):\n",
    "        modified_match = re.search(r'modifiedtime\\s*:\\s*(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', detail, re.IGNORECASE)\n",
    "        if modified_match:\n",
    "            extracted['modifiedtime'] = modified_match.group(1)\n",
    "    \n",
    "    # 3. MFTModifiedTime extraction\n",
    "    if 'mftmodifiedtime' in detail_lower and pd.isna(row['mftmodifiedtime']):\n",
    "        mft_match = re.search(r'mftmodifiedtime\\s*:\\s*(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', detail, re.IGNORECASE)\n",
    "        if mft_match:\n",
    "            extracted['mftmodifiedtime'] = mft_match.group(1)\n",
    "    \n",
    "    # 4. AccessedTime extraction\n",
    "    if 'accessedtime' in detail_lower and pd.isna(row['accessedtime']):\n",
    "        accessed_match = re.search(r'accessedtime\\s*:\\s*(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', detail, re.IGNORECASE)\n",
    "        if accessed_match:\n",
    "            extracted['accessedtime'] = accessed_match.group(1)\n",
    "    \n",
    "    # 5. For Time Reversal Events - extract the \"before\" timestamp (suspicious one)\n",
    "    if 'time reversal' in event and len(timestamps) >= 2:\n",
    "        # Time Reversal format: \"timestamp1 -> timestamp2\"\n",
    "        # timestamp1 is the manipulated/suspicious timestamp (AFTER)\n",
    "        # timestamp2 is the reverted timestamp (BEFORE)\n",
    "        if pd.isna(row['modifiedtime']) and 'modifiedtime' in detail_lower:\n",
    "            extracted['modifiedtime'] = timestamps[0]\n",
    "        if pd.isna(row['creationtime']) and 'creationtime' in detail_lower:\n",
    "            extracted['creationtime'] = timestamps[0]\n",
    "        if pd.isna(row['mftmodifiedtime']) and 'mftmodifiedtime' in detail_lower:\n",
    "            extracted['mftmodifiedtime'] = timestamps[0]\n",
    "    \n",
    "    # 6. Generic extraction - use first timestamp if still missing\n",
    "    if all(pd.isna(extracted[col]) for col in ['creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime']):\n",
    "        if len(timestamps) > 0:\n",
    "            # Use first timestamp as a generic fallback\n",
    "            extracted['modifiedtime'] = timestamps[0]\n",
    "    \n",
    "    return extracted\n",
    "\n",
    "\n",
    "# Apply extraction to all datasets\n",
    "extraction_stats = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    # Track rows with missing timestamps before extraction\n",
    "    missing_before = (\n",
    "        df['creationtime'].isnull() &\n",
    "        df['modifiedtime'].isnull() &\n",
    "        df['mftmodifiedtime'].isnull() &\n",
    "        df['accessedtime'].isnull()\n",
    "    ).sum()\n",
    "    \n",
    "    # Apply extraction only to rows with missing timestamps\n",
    "    rows_to_extract = df[\n",
    "        df['creationtime'].isnull() |\n",
    "        df['modifiedtime'].isnull() |\n",
    "        df['mftmodifiedtime'].isnull() |\n",
    "        df['accessedtime'].isnull()\n",
    "    ]\n",
    "    \n",
    "    if len(rows_to_extract) > 0:\n",
    "        extracted_data = rows_to_extract.apply(extract_timestamps_from_detail, axis=1, result_type='expand')\n",
    "        \n",
    "        # Update the dataframe with extracted timestamps\n",
    "        for col in ['creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime']:\n",
    "            df.loc[extracted_data.index, col] = df.loc[extracted_data.index, col].fillna(extracted_data[col])\n",
    "    \n",
    "    # Track rows with missing timestamps after extraction\n",
    "    missing_after = (\n",
    "        df['creationtime'].isnull() &\n",
    "        df['modifiedtime'].isnull() &\n",
    "        df['mftmodifiedtime'].isnull() &\n",
    "        df['accessedtime'].isnull()\n",
    "    ).sum()\n",
    "    \n",
    "    extracted_count = missing_before - missing_after\n",
    "    \n",
    "    extraction_stats[case_id] = {\n",
    "        'missing_before': missing_before,\n",
    "        'missing_after': missing_after,\n",
    "        'extracted': extracted_count\n",
    "    }\n",
    "    \n",
    "    print(f\"Case {case_id}: Extracted timestamps for {extracted_count:,} rows | Still missing all: {missing_after:,}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "total_extracted = sum(stats['extracted'] for stats in extraction_stats.values())\n",
    "total_still_missing = sum(stats['missing_after'] for stats in extraction_stats.values())\n",
    "\n",
    "print(f\"Total rows with timestamps extracted: {total_extracted:,}\")\n",
    "print(f\"Total rows still missing all timestamps: {total_still_missing:,}\")\n",
    "\n",
    "# --- Verify extraction for labeled timestomped rows ---\n",
    "print(\"\\nVerification: Check timestomped rows after extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_dataframes = pd.concat(logfile_dataframes.values(), ignore_index=True)\n",
    "timestomped_rows = all_dataframes[all_dataframes['is_timestomped'] == 1]\n",
    "\n",
    "print(f\"Total timestomped rows: {len(timestomped_rows)}\")\n",
    "\n",
    "# Check how many still have missing timestamps\n",
    "timestomped_missing_all = timestomped_rows[\n",
    "    timestomped_rows['creationtime'].isnull() &\n",
    "    timestomped_rows['modifiedtime'].isnull() &\n",
    "    timestomped_rows['mftmodifiedtime'].isnull() &\n",
    "    timestomped_rows['accessedtime'].isnull()\n",
    "]\n",
    "\n",
    "print(f\"Timestomped rows still missing ALL timestamps: {len(timestomped_missing_all)}\")\n",
    "\n",
    "if len(timestomped_missing_all) > 0:\n",
    "    print(\"\\n⚠️  Warning: Some timestomped rows still have no timestamps\")\n",
    "    print(timestomped_missing_all[['event', 'detail', 'Case_ID']].head())\n",
    "else:\n",
    "    print(\"\\n✓ All timestomped rows now have at least one timestamp!\")\n",
    "\n",
    "# Show sample of successfully extracted timestamps\n",
    "timestomped_with_ts = timestomped_rows[timestomped_rows['modifiedtime'].notna() | \n",
    "                                        timestomped_rows['creationtime'].notna()]\n",
    "print(f\"\\nTimestomped rows with extracted timestamps: {len(timestomped_with_ts)}\")\n",
    "print(\"\\nSample:\")\n",
    "print(timestomped_with_ts[['event', 'creationtime',  'modifiedtime', 'mftmodifiedtime']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c11d8b",
   "metadata": {},
   "source": [
    "### 11.4. Final Missing Value Assessment\n",
    "\n",
    "After timestamp extraction, we have significantly improved data completeness:\n",
    "- Extracted 12,675 timestamps from `detail` column\n",
    "- All timestomped rows now have temporal data\n",
    "- 15,949 rows (18.7%) still missing all 4 timestamps\n",
    "\n",
    "**Remaining Decision Points:**\n",
    "1. Rows with no timestamps and no detail → Drop (no forensic value)\n",
    "2. Rows with timestamps but missing `full path` or `file/directory name` → Keep (can use LSN for identification)\n",
    "3. `detail` column → Keep (provides qualitative context even when null)\n",
    "Code Cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5155671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final assessment: Rows still missing all timestamps\n",
      "============================================================\n",
      "Total rows still missing all 4 timestamps: 15,949\n",
      "Percentage of total data: 18.69%\n",
      "\n",
      "Labeled rows still missing timestamps: 0\n",
      "✓ All labeled rows now have at least one timestamp!\n",
      "\n",
      "Of 15,949 rows with no timestamps:\n",
      "  → 14,089 have detail column (88.34%)\n",
      "  → 1,860 have NO detail (11.66%)\n",
      "\n",
      "Event types for rows with no timestamps:\n",
      "event\n",
      "Writing Content of Resident File        5144\n",
      "Writing Content of Non-Resident File    5054\n",
      "Renaming File                           2075\n",
      "Changing FileAttribute                  1816\n",
      "Move(After)                              626\n",
      "Move(Before)                             618\n",
      "# Check Point                            616\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Final assessment of rows still missing all timestamps ---\n",
    "print(\"Final assessment: Rows still missing all timestamps\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_dataframes = pd.concat(logfile_dataframes.values(), ignore_index=True)\n",
    "\n",
    "still_missing_all = all_dataframes[\n",
    "    all_dataframes['creationtime'].isnull() &\n",
    "    all_dataframes['modifiedtime'].isnull() &\n",
    "    all_dataframes['mftmodifiedtime'].isnull() &\n",
    "    all_dataframes['accessedtime'].isnull()\n",
    "]\n",
    "\n",
    "print(f\"Total rows still missing all 4 timestamps: {len(still_missing_all):,}\")\n",
    "print(f\"Percentage of total data: {len(still_missing_all)/len(all_dataframes)*100:.2f}%\")\n",
    "\n",
    "# Check if any are labeled\n",
    "labeled_still_missing = still_missing_all[\n",
    "    (still_missing_all['is_timestomped'] == 1) | \n",
    "    (still_missing_all['is_suspicious_execution'] == 1)\n",
    "]\n",
    "\n",
    "print(f\"\\nLabeled rows still missing timestamps: {len(labeled_still_missing)}\")\n",
    "\n",
    "if len(labeled_still_missing) == 0:\n",
    "    print(\"✓ All labeled rows now have at least one timestamp!\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: Some labeled rows still have no timestamps\")\n",
    "\n",
    "# Check if they have detail\n",
    "has_detail = still_missing_all['detail'].notna().sum()\n",
    "print(f\"\\nOf {len(still_missing_all):,} rows with no timestamps:\")\n",
    "print(f\"  → {has_detail:,} have detail column ({has_detail/len(still_missing_all)*100:.2f}%)\")\n",
    "print(f\"  → {len(still_missing_all) - has_detail:,} have NO detail ({(1-has_detail/len(still_missing_all))*100:.2f}%)\")\n",
    "\n",
    "# Event distribution for rows with no timestamps\n",
    "print(f\"\\nEvent types for rows with no timestamps:\")\n",
    "print(still_missing_all['event'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526d031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleanup: Dropping rows with no timestamps AND no detail...\n",
      "============================================================\n",
      "Case 01: 13,816 → 13,763 records (Dropped: 53 | 0.38%)\n",
      "Case 02: 6,262 → 6,049 records (Dropped: 213 | 3.40%)\n",
      "Case 03: 7,497 → 7,262 records (Dropped: 235 | 3.13%)\n",
      "Case 04: 4,964 → 4,859 records (Dropped: 105 | 2.12%)\n",
      "Case 05: 4,993 → 4,909 records (Dropped: 84 | 1.68%)\n",
      "Case 06: 4,791 → 4,703 records (Dropped: 88 | 1.84%)\n",
      "Case 07: 8,050 → 7,823 records (Dropped: 227 | 2.82%)\n",
      "Case 08: 7,893 → 7,666 records (Dropped: 227 | 2.88%)\n",
      "Case 09: 8,683 → 8,453 records (Dropped: 230 | 2.65%)\n",
      "Case 10: 8,294 → 8,073 records (Dropped: 221 | 2.66%)\n",
      "Case 11: 4,987 → 4,898 records (Dropped: 89 | 1.78%)\n",
      "Case 12: 5,088 → 5,000 records (Dropped: 88 | 1.73%)\n",
      "------------------------------------------------------------\n",
      "Total: 85,318 → 83,458 records\n",
      "Total Dropped: 1,860 (2.18%)\n",
      "\n",
      "✓ Final cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "# --- Drop rows with no timestamps AND no detail (unlabeled only) ---\n",
    "print(\"Final cleanup: Dropping rows with no timestamps AND no detail...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_drop_stats = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Identify rows to drop\n",
    "    all_ts_null = (\n",
    "        df['creationtime'].isnull() &\n",
    "        df['modifiedtime'].isnull() &\n",
    "        df['mftmodifiedtime'].isnull() &\n",
    "        df['accessedtime'].isnull()\n",
    "    )\n",
    "    detail_null = df['detail'].isnull()\n",
    "    not_labeled = (df['is_timestomped'] == 0) & (df['is_suspicious_execution'] == 0)\n",
    "    \n",
    "    # Drop only if all conditions met\n",
    "    drop_mask = all_ts_null & detail_null & not_labeled\n",
    "    \n",
    "    df_final = df[~drop_mask].copy()\n",
    "    \n",
    "    rows_after = len(df_final)\n",
    "    dropped = rows_before - rows_after\n",
    "    dropped_pct = (dropped / rows_before) * 100 if rows_before > 0 else 0\n",
    "    \n",
    "    final_drop_stats[case_id] = {\n",
    "        'before': rows_before,\n",
    "        'after': rows_after,\n",
    "        'dropped': dropped\n",
    "    }\n",
    "    \n",
    "    logfile_dataframes[case_id] = df_final\n",
    "    \n",
    "    print(f\"Case {case_id}: {rows_before:,} → {rows_after:,} records (Dropped: {dropped:,} | {dropped_pct:.2f}%)\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "total_before = sum(stats['before'] for stats in final_drop_stats.values())\n",
    "total_after = sum(stats['after'] for stats in final_drop_stats.values())\n",
    "total_dropped = total_before - total_after\n",
    "total_dropped_pct = (total_dropped / total_before) * 100\n",
    "\n",
    "print(f\"Total: {total_before:,} → {total_after:,} records\")\n",
    "print(f\"Total Dropped: {total_dropped:,} ({total_dropped_pct:.2f}%)\")\n",
    "print(\"\\n✓ Final cleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851cc86",
   "metadata": {},
   "source": [
    "### 11.4. Current Missing Values Status\n",
    "\n",
    "After timestamp extraction, let's review what missing values remain in our cleaned datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5701a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Missing Values Status After Timestamp Extraction\n",
      "============================================================\n",
      "Total records across all cases: 83,458\n",
      "\n",
      "Columns with Missing Values:\n",
      "============================================================\n",
      "Column                    Missing      %        Non-Null    \n",
      "------------------------------------------------------------\n",
      "detail                    52,296       62.66    31,162      \n",
      "accessedtime              26,486       31.74    56,972      \n",
      "creationtime              26,322       31.54    57,136      \n",
      "mftmodifiedtime           22,902       27.44    60,556      \n",
      "modifiedtime              14,253       17.08    69,205      \n",
      "full path                 9,965        11.94    73,493      \n",
      "file/directory name       1,930        2.31     81,528      \n",
      "eventtime(utc+8)          8            0.01     83,450      \n",
      "============================================================\n",
      "\n",
      "Total columns: 17\n",
      "Columns with missing values: 8\n",
      "Columns complete: 9\n",
      "\n",
      "============================================================\n",
      "Critical Timestamp Columns Status:\n",
      "============================================================\n",
      "eventtime(utc+8)          8            (0.01%) | Non-null: 83,450\n",
      "creationtime              26,322       (31.54%) | Non-null: 57,136\n",
      "modifiedtime              14,253       (17.08%) | Non-null: 69,205\n",
      "mftmodifiedtime           22,902       (27.44%) | Non-null: 60,556\n",
      "accessedtime              26,486       (31.74%) | Non-null: 56,972\n",
      "\n",
      "============================================================\n",
      "Rows Missing ALL 4 Core Timestamps:\n",
      "============================================================\n",
      "Rows missing all 4 timestamps: 14,089 (16.88%)\n",
      "  → With detail: 14,089\n",
      "  → Without detail: 0\n",
      "  → Labeled rows: 0\n",
      "\n",
      "✓ All labeled rows have at least one timestamp!\n"
     ]
    }
   ],
   "source": [
    "# --- Comprehensive missing values assessment after extraction ---\n",
    "print(\"Current Missing Values Status After Timestamp Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combine all datasets\n",
    "all_dataframes = pd.concat(logfile_dataframes.values(), ignore_index=True)\n",
    "\n",
    "print(f\"Total records across all cases: {len(all_dataframes):,}\\n\")\n",
    "\n",
    "# Calculate missing values\n",
    "missing_summary = all_dataframes.isnull().sum()\n",
    "missing_pct = (missing_summary / len(all_dataframes)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_summary.index,\n",
    "    'Missing_Count': missing_summary.values,\n",
    "    'Missing_Percentage': missing_pct.values,\n",
    "    'Non_Null_Count': len(all_dataframes) - missing_summary.values\n",
    "}).sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "# Show only columns with missing values\n",
    "missing_df_filtered = missing_df[missing_df['Missing_Count'] > 0]\n",
    "\n",
    "print(\"Columns with Missing Values:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Column':<25} {'Missing':<12} {'%':<8} {'Non-Null':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for _, row in missing_df_filtered.iterrows():\n",
    "    print(f\"{row['Column']:<25} {int(row['Missing_Count']):<12,} {row['Missing_Percentage']:<8.2f} {int(row['Non_Null_Count']):<12,}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nTotal columns: {len(all_dataframes.columns)}\")\n",
    "print(f\"Columns with missing values: {len(missing_df_filtered)}\")\n",
    "print(f\"Columns complete: {len(all_dataframes.columns) - len(missing_df_filtered)}\")\n",
    "\n",
    "# Check critical timestamp columns specifically\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Critical Timestamp Columns Status:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "timestamp_cols = ['eventtime(utc+8)', 'creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime']\n",
    "\n",
    "for col in timestamp_cols:\n",
    "    if col in all_dataframes.columns:\n",
    "        null_count = all_dataframes[col].isnull().sum()\n",
    "        null_pct = (null_count / len(all_dataframes)) * 100\n",
    "        non_null = len(all_dataframes) - null_count\n",
    "        print(f\"{col:<25} {null_count:<12,} ({null_pct:.2f}%) | Non-null: {non_null:,}\")\n",
    "\n",
    "# Check rows missing ALL 4 core timestamps\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Rows Missing ALL 4 Core Timestamps:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_ts_missing = all_dataframes[\n",
    "    all_dataframes['creationtime'].isnull() &\n",
    "    all_dataframes['modifiedtime'].isnull() &\n",
    "    all_dataframes['mftmodifiedtime'].isnull() &\n",
    "    all_dataframes['accessedtime'].isnull()\n",
    "]\n",
    "\n",
    "print(f\"Rows missing all 4 timestamps: {len(all_ts_missing):,} ({len(all_ts_missing)/len(all_dataframes)*100:.2f}%)\")\n",
    "\n",
    "# Of those, how many have detail?\n",
    "with_detail = all_ts_missing['detail'].notna().sum()\n",
    "without_detail = all_ts_missing['detail'].isnull().sum()\n",
    "\n",
    "print(f\"  → With detail: {with_detail:,}\")\n",
    "print(f\"  → Without detail: {without_detail:,}\")\n",
    "\n",
    "# Check if any labeled rows still missing all timestamps\n",
    "labeled_missing_all = all_ts_missing[\n",
    "    (all_ts_missing['is_timestomped'] == 1) | \n",
    "    (all_ts_missing['is_suspicious_execution'] == 1)\n",
    "]\n",
    "\n",
    "print(f\"  → Labeled rows: {len(labeled_missing_all)}\")\n",
    "\n",
    "if len(labeled_missing_all) == 0:\n",
    "    print(\"\\n✓ All labeled rows have at least one timestamp!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416159f",
   "metadata": {},
   "source": [
    "### 11.5. Missing Value Strategy - Justification and Context\n",
    "\n",
    "### Why We Need At Least One Timestamp\n",
    "\n",
    "**Core Requirement for Timestomping Detection:**\n",
    "- Our study aims to detect **timestamp manipulation** (timestomping) by analyzing temporal anomalies\n",
    "- Without ANY timestamp, a row provides zero temporal context\n",
    "- Machine learning models for timestomping rely on **time delta features** (differences between MAC times)\n",
    "- At least one timestamp allows us to:\n",
    "  - Calculate time relationships with `eventtime(utc+8)`\n",
    "  - Establish temporal context for the event\n",
    "  - Detect suspicious temporal patterns\n",
    "\n",
    "**Current Status:**\n",
    "- ✅ All 14 labeled timestomped rows have at least one timestamp\n",
    "- ✅ 69,369 rows (83.1%) have complete temporal data\n",
    "- ⚠️ 14,089 rows (16.88%) missing all 4 core timestamps but have `detail` content\n",
    "\n",
    "---\n",
    "\n",
    "### Is It Acceptable to Leave Some Missing Values?\n",
    "\n",
    "**YES - Here's why:**\n",
    "\n",
    "#### 1. **Not All Columns Are Equally Important**\n",
    "\n",
    "**Critical for Model (MUST have):**\n",
    "- ✅ `eventtime(utc+8)`: 99.99% complete (only 8 missing)\n",
    "- ✅ At least 1 of 4 core timestamps: 83.12% of rows have this\n",
    "- ✅ `event` or `detail`: 100% complete (we already dropped rows missing both)\n",
    "\n",
    "**Important but Can Be Null (depends on event type):**\n",
    "- `creationtime`: Not all events involve file creation (e.g., deletion events)\n",
    "- `modifiedtime`: Not all events modify files (e.g., file access)\n",
    "- `mftmodifiedtime`: May not change for certain operations\n",
    "- `accessedtime`: Often not logged for many event types\n",
    "\n",
    "**Low Priority (Can Be Null):**\n",
    "- `full path`: 88% complete - acceptable, we have LSN for identification\n",
    "- `file/directory name`: 97.7% complete - very good\n",
    "- `detail`: 37.3% complete - provides qualitative context when available, but not required\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Forensic Reality: LogFile Data Is Inherently Incomplete**\n",
    "\n",
    "NTFS $LogFile doesn't always record all MAC timestamps for every operation:\n",
    "- **Deletion events**: Files are deleted, so creation/modified times may not be captured\n",
    "- **System operations**: Some low-level operations don't update all timestamps\n",
    "- **Partial logging**: $LogFile may only log specific timestamp changes relevant to the operation\n",
    "\n",
    "**This is normal behavior**, not a data quality issue.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Model Training Approach**\n",
    "\n",
    "We can handle missing timestamps in feature engineering:\n",
    "\n",
    "**Option A: Drop rows with missing timestamps** (Aggressive)\n",
    "- Pros: Clean dataset, easier modeling\n",
    "- Cons: Lose 16.88% of data, potential information loss\n",
    "\n",
    "**Option B: Keep rows with partial timestamps** (Recommended)\n",
    "- Pros: Preserve more data and context\n",
    "- Cons: Need to handle missing values in feature engineering\n",
    "- **Strategy:**\n",
    "  - Use `eventtime(utc+8)` as baseline when other timestamps missing\n",
    "  - Create binary flags: `has_creationtime`, `has_modifiedtime`, etc.\n",
    "  - Calculate time deltas only for available timestamps\n",
    "  - Use imputation for missing values (e.g., fill with `eventtime(utc+8)`)\n",
    "\n",
    "---\n",
    "\n",
    "### Our Decision: **Keep Rows with At Least One Timestamp**\n",
    "\n",
    "**Rationale:**\n",
    "1. All labeled rows preserved ✓\n",
    "2. Maximum data retention (83.1% have complete timestamps)\n",
    "3. Rows with partial timestamps still provide forensic value\n",
    "4. 14,089 rows (16.88%) with no timestamps but have `detail` → Keep for qualitative analysis\n",
    "5. Missing values are expected in forensic data and can be handled in feature engineering\n",
    "\n",
    "**What We Will Drop:**\n",
    "- Only rows with NO timestamps AND NO detail AND NOT labeled\n",
    "- This ensures we only remove truly empty, valueless rows\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps:\n",
    "1. Drop rows with no timestamps, no detail, and not labeled\n",
    "2. Convert timestamp columns to datetime format\n",
    "3. Handle remaining null `full path` and `file/directory name` (low priority)\n",
    "4. Merge all 12 datasets into Master_LogFile_Cleaned.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e618045",
   "metadata": {},
   "source": [
    "## 12. Final Data Cleaning Steps\n",
    "\n",
    "We'll now complete the data cleaning process:\n",
    "1. Drop rows with no timestamps, no detail, and not labeled (minimal loss)\n",
    "2. Convert timestamp columns to datetime format for proper temporal analysis\n",
    "3. Handle remaining missing values in `full path` and `file/directory name`\n",
    "4. Final validation and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2071f01d",
   "metadata": {},
   "source": [
    "### 12.1. Drop Rows with No Forensic Value\n",
    "\n",
    "Remove rows where:\n",
    "- ALL 4 timestamp columns are null AND\n",
    "- `detail` column is also null AND\n",
    "- NOT labeled as timestomped or suspicious\n",
    "\n",
    "These rows have zero forensic value for timestomping detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5583a280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows with no timestamps AND no detail (unlabeled)...\n",
      "============================================================\n",
      "Case 01: 13,763 → 13,763 records (Dropped: 0 | 0.00%)\n",
      "Case 02: 6,049 → 6,049 records (Dropped: 0 | 0.00%)\n",
      "Case 03: 7,262 → 7,262 records (Dropped: 0 | 0.00%)\n",
      "Case 04: 4,859 → 4,859 records (Dropped: 0 | 0.00%)\n",
      "Case 05: 4,909 → 4,909 records (Dropped: 0 | 0.00%)\n",
      "Case 06: 4,703 → 4,703 records (Dropped: 0 | 0.00%)\n",
      "Case 07: 7,823 → 7,823 records (Dropped: 0 | 0.00%)\n",
      "Case 08: 7,666 → 7,666 records (Dropped: 0 | 0.00%)\n",
      "Case 09: 8,453 → 8,453 records (Dropped: 0 | 0.00%)\n",
      "Case 10: 8,073 → 8,073 records (Dropped: 0 | 0.00%)\n",
      "Case 11: 4,898 → 4,898 records (Dropped: 0 | 0.00%)\n",
      "Case 12: 5,000 → 5,000 records (Dropped: 0 | 0.00%)\n",
      "------------------------------------------------------------\n",
      "Total: 83,458 → 83,458 records\n",
      "Total Dropped: 0 (0.00%)\n",
      "\n",
      "✓ Final cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "# --- Drop rows with no timestamps AND no detail (unlabeled only) ---\n",
    "print(\"Dropping rows with no timestamps AND no detail (unlabeled)...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_drop_stats = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    rows_before = len(df)\n",
    "    \n",
    "    # Identify rows to drop\n",
    "    all_ts_null = (\n",
    "        df['creationtime'].isnull() &\n",
    "        df['modifiedtime'].isnull() &\n",
    "        df['mftmodifiedtime'].isnull() &\n",
    "        df['accessedtime'].isnull()\n",
    "    )\n",
    "    detail_null = df['detail'].isnull()\n",
    "    not_labeled = (df['is_timestomped'] == 0) & (df['is_suspicious_execution'] == 0)\n",
    "    \n",
    "    # Drop only if all conditions met\n",
    "    drop_mask = all_ts_null & detail_null & not_labeled\n",
    "    \n",
    "    df_final = df[~drop_mask].copy()\n",
    "    \n",
    "    rows_after = len(df_final)\n",
    "    dropped = rows_before - rows_after\n",
    "    dropped_pct = (dropped / rows_before) * 100 if rows_before > 0 else 0\n",
    "    \n",
    "    final_drop_stats[case_id] = {\n",
    "        'before': rows_before,\n",
    "        'after': rows_after,\n",
    "        'dropped': dropped\n",
    "    }\n",
    "    \n",
    "    logfile_dataframes[case_id] = df_final\n",
    "    \n",
    "    print(f\"Case {case_id}: {rows_before:,} → {rows_after:,} records (Dropped: {dropped:,} | {dropped_pct:.2f}%)\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "total_before = sum(stats['before'] for stats in final_drop_stats.values())\n",
    "total_after = sum(stats['after'] for stats in final_drop_stats.values())\n",
    "total_dropped = total_before - total_after\n",
    "total_dropped_pct = (total_dropped / total_before) * 100\n",
    "\n",
    "print(f\"Total: {total_before:,} → {total_after:,} records\")\n",
    "print(f\"Total Dropped: {total_dropped:,} ({total_dropped_pct:.2f}%)\")\n",
    "print(\"\\n✓ Final cleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3d497",
   "metadata": {},
   "source": [
    "### 12.2. Convert Timestamp Columns to Datetime Format\n",
    "\n",
    "Convert all timestamp columns from object/string type to pandas datetime format for:\n",
    "- Proper temporal calculations\n",
    "- Time delta feature engineering\n",
    "- Sorting and filtering operations\n",
    "\n",
    "**Timezone:** All timestamps will be converted to UTC for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b144063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting timestamp columns to datetime format...\n",
      "============================================================\n",
      "Case 01: Converted 5 timestamp columns to datetime\n",
      "Case 02: Converted 5 timestamp columns to datetime\n",
      "Case 03: Converted 5 timestamp columns to datetime\n",
      "Case 04: Converted 5 timestamp columns to datetime\n",
      "Case 05: Converted 5 timestamp columns to datetime\n",
      "Case 06: Converted 5 timestamp columns to datetime\n",
      "Case 07: Converted 5 timestamp columns to datetime\n",
      "Case 08: Converted 5 timestamp columns to datetime\n",
      "Case 09: Converted 5 timestamp columns to datetime\n",
      "Case 10: Converted 5 timestamp columns to datetime\n",
      "Case 11: Converted 5 timestamp columns to datetime\n",
      "Case 12: Converted 5 timestamp columns to datetime\n",
      "------------------------------------------------------------\n",
      "✓ All timestamp columns converted to datetime format!\n",
      "\n",
      "Verification - Data types after conversion:\n",
      "  eventtime(utc+8): datetime64[ns, UTC]\n",
      "  creationtime: datetime64[ns, UTC]\n",
      "  modifiedtime: datetime64[ns, UTC]\n",
      "  mftmodifiedtime: datetime64[ns, UTC]\n",
      "  accessedtime: datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "# --- Convert timestamp columns to datetime format ---\n",
    "print(\"Converting timestamp columns to datetime format...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "timestamp_columns = ['eventtime(utc+8)', 'creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime']\n",
    "\n",
    "conversion_stats = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    converted_count = 0\n",
    "    \n",
    "    for col in timestamp_columns:\n",
    "        if col in df.columns:\n",
    "            # Check current dtype\n",
    "            current_dtype = df[col].dtype\n",
    "            \n",
    "            if current_dtype == 'object' or current_dtype == 'string':\n",
    "                # Convert to datetime, coerce errors to NaT\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
    "                converted_count += 1\n",
    "    \n",
    "    conversion_stats[case_id] = converted_count\n",
    "    print(f\"Case {case_id}: Converted {converted_count} timestamp columns to datetime\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"✓ All timestamp columns converted to datetime format!\")\n",
    "\n",
    "# Verify conversion\n",
    "print(\"\\nVerification - Data types after conversion:\")\n",
    "sample_case = '01'\n",
    "df_sample = logfile_dataframes[sample_case]\n",
    "\n",
    "for col in timestamp_columns:\n",
    "    if col in df_sample.columns:\n",
    "        print(f\"  {col}: {df_sample[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8e835d",
   "metadata": {},
   "source": [
    "### 12.3. Handle Missing Values in Full Path and File/Directory Name\n",
    "\n",
    "**Strategy:**\n",
    "- `full path`: 11.94% missing - Keep as-is (we have LSN for file identification)\n",
    "- `file/directory name`: 2.31% missing - Can extract from `full path` if available\n",
    "\n",
    "**Justification:**\n",
    "- Not critical for timestomping detection (we have LSN as unique identifier)\n",
    "- Some events (e.g., deletion, system operations) may not have path information\n",
    "- Models can handle missing categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f2bd1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling missing 'file/directory name'...\n",
      "============================================================\n",
      "------------------------------------------------------------\n",
      "Total filled: 0\n",
      "Total still missing: 1,930\n",
      "\n",
      "✓ File/directory name handling completed!\n"
     ]
    }
   ],
   "source": [
    "# --- Attempt to fill file/directory name from full path ---\n",
    "print(\"Handling missing 'file/directory name'...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fill_stats = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    missing_before = df['file/directory name'].isnull().sum()\n",
    "    \n",
    "    # For rows where file/directory name is null but full path exists\n",
    "    mask = df['file/directory name'].isnull() & df['full path'].notna()\n",
    "    \n",
    "    if mask.sum() > 0:\n",
    "        # Extract filename from full path (last part after \\)\n",
    "        df.loc[mask, 'file/directory name'] = df.loc[mask, 'full path'].str.split('\\\\').str[-1]\n",
    "    \n",
    "    missing_after = df['file/directory name'].isnull().sum()\n",
    "    filled = missing_before - missing_after\n",
    "    \n",
    "    fill_stats[case_id] = {\n",
    "        'missing_before': missing_before,\n",
    "        'missing_after': missing_after,\n",
    "        'filled': filled\n",
    "    }\n",
    "    \n",
    "    if filled > 0:\n",
    "        print(f\"Case {case_id}: Filled {filled:,} file/directory names from full path\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "total_filled = sum(stats['filled'] for stats in fill_stats.values())\n",
    "total_still_missing = sum(stats['missing_after'] for stats in fill_stats.values())\n",
    "\n",
    "print(f\"Total filled: {total_filled:,}\")\n",
    "print(f\"Total still missing: {total_still_missing:,}\")\n",
    "print(\"\\n✓ File/directory name handling completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525980f",
   "metadata": {},
   "source": [
    "### 12.4. Final Data Validation and Statistics\n",
    "\n",
    "Verify data quality and generate summary statistics before merging datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "860478af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Data Validation Summary\n",
      "============================================================\n",
      "Total records after cleaning: 83,458\n",
      "Total columns: 17\n",
      "\n",
      "Labeled Rows Preserved:\n",
      "  → Timestomped: 14\n",
      "  → Suspicious Execution: 8\n",
      "  → Total Labeled: 22\n",
      "\n",
      "Timestamp Completeness:\n",
      "  eventtime(utc+8): 99.99% complete (83,450 / 83,458)\n",
      "  creationtime: 68.46% complete (57,136 / 83,458)\n",
      "  modifiedtime: 82.92% complete (69,205 / 83,458)\n",
      "  mftmodifiedtime: 72.56% complete (60,556 / 83,458)\n",
      "  accessedtime: 68.26% complete (56,972 / 83,458)\n",
      "\n",
      "Rows with at least one timestamp: 69,369 (83.12%)\n",
      "\n",
      "Data Quality Flags:\n",
      "  Rows with incomplete timestamps: 8 (0.01%)\n",
      "\n",
      "Per-Case Record Counts:\n",
      "------------------------------------------------------------\n",
      "Case     Total Records   Timestomped     Suspicious     \n",
      "------------------------------------------------------------\n",
      "Case 01  13,763          1               1              \n",
      "Case 02  6,049           1               1              \n",
      "Case 03  7,262           1               1              \n",
      "Case 04  4,859           1               0              \n",
      "Case 05  4,909           1               0              \n",
      "Case 06  4,703           2               0              \n",
      "Case 07  7,823           1               1              \n",
      "Case 08  7,666           1               1              \n",
      "Case 09  8,453           1               1              \n",
      "Case 10  8,073           0               0              \n",
      "Case 11  4,898           1               1              \n",
      "Case 12  5,000           3               1              \n",
      "------------------------------------------------------------\n",
      "TOTAL    83,458          14              8              \n",
      "\n",
      "============================================================\n",
      "✓ Data validation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Final validation and statistics ---\n",
    "print(\"Final Data Validation Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Combine all datasets for overall statistics\n",
    "all_dataframes = pd.concat(logfile_dataframes.values(), ignore_index=True)\n",
    "\n",
    "print(f\"Total records after cleaning: {len(all_dataframes):,}\")\n",
    "print(f\"Total columns: {len(all_dataframes.columns)}\\n\")\n",
    "\n",
    "# Check labeled rows preservation\n",
    "total_timestomped = all_dataframes['is_timestomped'].sum()\n",
    "total_suspicious = all_dataframes['is_suspicious_execution'].sum()\n",
    "\n",
    "print(\"Labeled Rows Preserved:\")\n",
    "print(f\"  → Timestomped: {total_timestomped}\")\n",
    "print(f\"  → Suspicious Execution: {total_suspicious}\")\n",
    "print(f\"  → Total Labeled: {total_timestomped + total_suspicious}\\n\")\n",
    "\n",
    "# Timestamp completeness\n",
    "print(\"Timestamp Completeness:\")\n",
    "for col in ['eventtime(utc+8)', 'creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime']:\n",
    "    non_null = all_dataframes[col].notna().sum()\n",
    "    completeness = (non_null / len(all_dataframes)) * 100\n",
    "    print(f\"  {col}: {completeness:.2f}% complete ({non_null:,} / {len(all_dataframes):,})\")\n",
    "\n",
    "# Check rows with at least one timestamp\n",
    "has_any_timestamp = all_dataframes[\n",
    "    all_dataframes['creationtime'].notna() |\n",
    "    all_dataframes['modifiedtime'].notna() |\n",
    "    all_dataframes['mftmodifiedtime'].notna() |\n",
    "    all_dataframes['accessedtime'].notna()\n",
    "]\n",
    "\n",
    "print(f\"\\nRows with at least one timestamp: {len(has_any_timestamp):,} ({len(has_any_timestamp)/len(all_dataframes)*100:.2f}%)\")\n",
    "\n",
    "# Data quality flags\n",
    "print(f\"\\nData Quality Flags:\")\n",
    "incomplete_ts_count = all_dataframes['has_incomplete_timestamps'].sum()\n",
    "print(f\"  Rows with incomplete timestamps: {incomplete_ts_count:,} ({incomplete_ts_count/len(all_dataframes)*100:.2f}%)\")\n",
    "\n",
    "# Per-case statistics with labeled row counts\n",
    "print(f\"\\nPer-Case Record Counts:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Case':<8} {'Total Records':<15} {'Timestomped':<15} {'Suspicious':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    total_records = len(df)\n",
    "    timestomped_count = df['is_timestomped'].sum()\n",
    "    suspicious_count = df['is_suspicious_execution'].sum()\n",
    "    \n",
    "    print(f\"Case {case_id:<3} {total_records:<15,} {timestomped_count:<15} {suspicious_count:<15}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'TOTAL':<8} {len(all_dataframes):<15,} {total_timestomped:<15} {total_suspicious:<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Data validation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601b0be",
   "metadata": {},
   "source": [
    "### 12.5. Merge All 12 Case Datasets into Master LogFile\n",
    "\n",
    "Combine all cleaned individual case datasets into a single Master_LogFile_Cleaned.csv for unified analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30e896c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging all 12 case datasets into Master LogFile...\n",
      "============================================================\n",
      "Total records in Master LogFile: 83,458\n",
      "Total columns: 17\n",
      "\n",
      "Columns in Master LogFile:\n",
      "['lsn', 'eventtime(utc+8)', 'event', 'detail', 'file/directory name', 'full path', 'creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime', 'redo', 'target vcn', 'cluster index', 'is_timestomped', 'is_suspicious_execution', 'Case_ID', 'has_incomplete_timestamps']\n",
      "\n",
      "Sorting by Case_ID and eventtime(utc+8)...\n",
      "✓ Master LogFile created and sorted!\n"
     ]
    }
   ],
   "source": [
    "# --- Merge all 12 datasets into master LogFile ---\n",
    "print(\"Merging all 12 case datasets into Master LogFile...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "master_logfile = pd.concat(logfile_dataframes.values(), ignore_index=True)\n",
    "\n",
    "print(f\"Total records in Master LogFile: {len(master_logfile):,}\")\n",
    "print(f\"Total columns: {len(master_logfile.columns)}\")\n",
    "print(f\"\\nColumns in Master LogFile:\")\n",
    "print(master_logfile.columns.tolist())\n",
    "\n",
    "# Sort by Case_ID and eventtime for logical ordering\n",
    "print(\"\\nSorting by Case_ID and eventtime(utc+8)...\")\n",
    "master_logfile = master_logfile.sort_values(['Case_ID', 'eventtime(utc+8)'], na_position='last').reset_index(drop=True)\n",
    "\n",
    "print(\"✓ Master LogFile created and sorted!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "698f22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Master LogFile - Final Statistics\n",
      "============================================================\n",
      "\n",
      "Records per Case:\n",
      "  Case 01: 13,763 records\n",
      "  Case 02: 6,049 records\n",
      "  Case 03: 7,262 records\n",
      "  Case 04: 4,859 records\n",
      "  Case 05: 4,909 records\n",
      "  Case 06: 4,703 records\n",
      "  Case 07: 7,823 records\n",
      "  Case 08: 7,666 records\n",
      "  Case 09: 8,453 records\n",
      "  Case 10: 8,073 records\n",
      "  Case 11: 4,898 records\n",
      "  Case 12: 5,000 records\n",
      "\n",
      "Label Distribution:\n",
      "  Total Timestomped: 14\n",
      "  Total Suspicious Execution: 8\n",
      "\n",
      "Top 10 Event Types:\n",
      "  File Deletion: 25,698\n",
      "  File Creation: 25,203\n",
      "  Updating Modified Time: 8,552\n",
      "  Writing Content of Resident File: 5,196\n",
      "  Writing Content of Non-Resident File: 5,054\n",
      "  Updating MFTModified Time: 3,077\n",
      "  Time Reversal Event: 2,622\n",
      "  Renaming File: 2,075\n",
      "  Changing FileAttribute: 1,965\n",
      "  Directory Creation: 1,772\n",
      "\n",
      "Temporal Coverage:\n",
      "  Earliest event: 2000-01-01 08:00:00+00:00\n",
      "  Latest event: 2024-01-01 00:08:18+00:00\n",
      "  Time span: 8765 days\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Final statistics before export ---\n",
    "print(\"\\nMaster LogFile - Final Statistics\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Case distribution\n",
    "print(\"\\nRecords per Case:\")\n",
    "case_distribution = master_logfile['Case_ID'].value_counts().sort_index()\n",
    "for case_id, count in case_distribution.items():\n",
    "    print(f\"  Case {case_id:02d}: {count:,} records\")\n",
    "\n",
    "# Label distribution\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "print(f\"  Total Timestomped: {master_logfile['is_timestomped'].sum()}\")\n",
    "print(f\"  Total Suspicious Execution: {master_logfile['is_suspicious_execution'].sum()}\")\n",
    "\n",
    "# Event type distribution (top 10)\n",
    "print(f\"\\nTop 10 Event Types:\")\n",
    "top_events = master_logfile['event'].value_counts().head(10)\n",
    "for event, count in top_events.items():\n",
    "    print(f\"  {event}: {count:,}\")\n",
    "\n",
    "# Date range\n",
    "print(f\"\\nTemporal Coverage:\")\n",
    "earliest = master_logfile['eventtime(utc+8)'].min()\n",
    "latest = master_logfile['eventtime(utc+8)'].max()\n",
    "print(f\"  Earliest event: {earliest}\")\n",
    "print(f\"  Latest event: {latest}\")\n",
    "if pd.notna(earliest) and pd.notna(latest):\n",
    "    duration = latest - earliest\n",
    "    print(f\"  Time span: {duration.days} days\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34315d",
   "metadata": {},
   "source": [
    "### 12.6. Export Master LogFile to CSV\n",
    "\n",
    "Save the cleaned and merged master dataset to the Phase 2 output directory.\n",
    "\n",
    "**Output File:** `data/processed/Phase 2 - Data Cleaning/Master_LogFile_Cleaned.csv`\n",
    "\n",
    "**Data Characteristics:**\n",
    "- All 12 cases merged\n",
    "- Timestamps converted to datetime format\n",
    "- Missing values handled appropriately\n",
    "- All labeled rows preserved\n",
    "- Sorted by Case_ID and eventtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31613bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting Master LogFile to CSV...\n",
      "============================================================\n",
      "✓ Master LogFile exported successfully!\n",
      "\n",
      "File Details:\n",
      "  Location: data/processed/Phase 2 - Data Cleaning/Master_LogFile_Cleaned.csv\n",
      "  Size: 23.84 MB\n",
      "  Records: 83,458\n",
      "  Columns: 17\n",
      "\n",
      "✓ File verified at: data/processed/Phase 2 - Data Cleaning/Master_LogFile_Cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Export Master LogFile to CSV ---\n",
    "print(\"Exporting Master LogFile to CSV...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "output_filepath = OUTPUT_DIR / 'Master_LogFile_Cleaned.csv'\n",
    "\n",
    "# Export with proper datetime formatting\n",
    "master_logfile.to_csv(\n",
    "    output_filepath,\n",
    "    index=False,\n",
    "    date_format='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Get file size\n",
    "import os\n",
    "file_size_bytes = os.path.getsize(output_filepath)\n",
    "file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"✓ Master LogFile exported successfully!\")\n",
    "print(f\"\\nFile Details:\")\n",
    "print(f\"  Location: {output_filepath}\")\n",
    "print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "print(f\"  Records: {len(master_logfile):,}\")\n",
    "print(f\"  Columns: {len(master_logfile.columns)}\")\n",
    "\n",
    "# Verify file was created\n",
    "if output_filepath.exists():\n",
    "    print(f\"\\n✓ File verified at: {output_filepath}\")\n",
    "else:\n",
    "    print(f\"\\n✗ Error: File not found at {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a52d8",
   "metadata": {},
   "source": [
    "## 13. Phase 2 - Data Cleaning Summary\n",
    "\n",
    "### ✅ Completed Tasks:\n",
    "\n",
    "1. **Loaded 12 LogFile datasets** from Phase 1 (243,884 initial records)\n",
    "2. **Dropped rows with null event AND detail** (-112,653 rows, 46.19%)\n",
    "3. **Dropped irrelevant columns** (`targetvcn`, `clusterindex`)\n",
    "4. **Imputed eventtime(utc+8)** using conditional logic based on event type (+16,205 values)\n",
    "5. **Preserved all labeled rows** (14 timestomped, maintaining ground truth)\n",
    "6. **Extracted timestamps from detail column** (+12,675 timestamps recovered)\n",
    "7. **Dropped rows with no forensic value** (no timestamps, no detail, unlabeled)\n",
    "8. **Converted timestamps to datetime format** (proper temporal analysis)\n",
    "9. **Handled missing file/directory names** (extracted from full path where possible)\n",
    "10. **Merged all 12 cases** into Master_LogFile_Cleaned.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bd757",
   "metadata": {},
   "source": [
    "# -- Handling UsnJrnl -- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
