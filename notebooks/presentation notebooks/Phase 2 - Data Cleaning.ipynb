{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267e5ffa",
   "metadata": {},
   "source": [
    "# Phase 2 - Data Cleaning\n",
    "\n",
    "**Objective:** Clean, standardize, and prepare the labeled LogFile datasets for merging and analysis.\n",
    "\n",
    "**Process:**\n",
    "1. Import necessary libraries\n",
    "2. Load 12 labeled LogFile datasets from Phase 1\n",
    "3. Clean and standardize data\n",
    "4. Drop irrelevant columns and rows\n",
    "5. Impute missing values\n",
    "6. Merge all LogFiles into Master dataset\n",
    "7. Export cleaned Master_LogFile_Cleaned.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5bc52a",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2047848e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Pandas version: 2.3.2\n",
      "NumPy version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Visualization libraries (optional, for data exploration)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667135fe",
   "metadata": {},
   "source": [
    "## 2. Load the 12 Labeled LogFile Datasets\n",
    "\n",
    "Load all labeled LogFile datasets from Phase 1 output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a711b8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Directory: data/processed/Phase 1 - Data Labeling\n",
      "Output Directory: data/processed/Phase 2 - Data Cleaning\n",
      "Case IDs to process: ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "INPUT_DIR = Path('data/processed/Phase 1 - Data Labeling')\n",
    "OUTPUT_DIR = Path('data/processed/Phase 2 - Data Cleaning')\n",
    "\n",
    "# Ensure output directory exists\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the 12 case IDs\n",
    "CASE_IDS = [f'{i:02d}' for i in range(1, 13)]  # ['01', '02', ..., '12']\n",
    "\n",
    "print(f\"Input Directory: {INPUT_DIR}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Case IDs to process: {CASE_IDS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1005da4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled LogFile datasets...\n",
      "------------------------------------------------------------\n",
      "✓ Case 01: Loaded 39,077 records | Columns: 16\n",
      "✓ Case 02: Loaded 14,783 records | Columns: 16\n",
      "✓ Case 03: Loaded 24,063 records | Columns: 16\n",
      "✓ Case 04: Loaded 12,731 records | Columns: 16\n",
      "✓ Case 05: Loaded 14,242 records | Columns: 16\n",
      "✓ Case 06: Loaded 14,030 records | Columns: 16\n",
      "✓ Case 07: Loaded 23,737 records | Columns: 16\n",
      "✓ Case 08: Loaded 23,379 records | Columns: 16\n",
      "✓ Case 09: Loaded 25,688 records | Columns: 16\n",
      "✓ Case 10: Loaded 23,932 records | Columns: 16\n",
      "✓ Case 11: Loaded 14,083 records | Columns: 16\n",
      "✓ Case 12: Loaded 14,139 records | Columns: 16\n",
      "------------------------------------------------------------\n",
      "Total datasets loaded: 12/12\n",
      "Total records across all cases: 243,884\n"
     ]
    }
   ],
   "source": [
    "# --- Load all 12 LogFile datasets ---\n",
    "logfile_dataframes = {}\n",
    "\n",
    "print(\"Loading labeled LogFile datasets...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for case_id in CASE_IDS:\n",
    "    filename = f'{case_id}-PE-LogFile_labeled.csv'\n",
    "    filepath = INPUT_DIR / filename\n",
    "    \n",
    "    try:\n",
    "        # Load with low_memory=False to handle mixed types\n",
    "        df = pd.read_csv(filepath, low_memory=False)\n",
    "        \n",
    "        # Add Case_ID column for tracking\n",
    "        df['Case_ID'] = int(case_id)\n",
    "        \n",
    "        # Store in dictionary\n",
    "        logfile_dataframes[case_id] = df\n",
    "        \n",
    "        print(f\"✓ Case {case_id}: Loaded {len(df):,} records | Columns: {len(df.columns)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"✗ Case {case_id}: File not found at {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Case {case_id}: Error loading file - {e}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total datasets loaded: {len(logfile_dataframes)}/12\")\n",
    "\n",
    "# Calculate total records\n",
    "total_records = sum(len(df) for df in logfile_dataframes.values())\n",
    "print(f\"Total records across all cases: {total_records:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f08d460",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration\n",
    "\n",
    "Examine the structure and content of the loaded datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3e76c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Dataset: Case 01\n",
      "============================================================\n",
      "\n",
      "Shape: (39077, 16)\n",
      "\n",
      "Column Names:\n",
      "['lsn', 'eventtime(utc+8)', 'event', 'detail', 'file/directory name', 'full path', 'creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime', 'redo', 'target vcn', 'cluster index', 'is_timestomped', 'is_suspicious_execution', 'Case_ID']\n",
      "\n",
      "Data Types:\n",
      "lsn                         int64\n",
      "eventtime(utc+8)           object\n",
      "event                      object\n",
      "detail                     object\n",
      "file/directory name        object\n",
      "full path                  object\n",
      "creationtime               object\n",
      "modifiedtime               object\n",
      "mftmodifiedtime            object\n",
      "accessedtime               object\n",
      "redo                       object\n",
      "target vcn                 object\n",
      "cluster index               int64\n",
      "is_timestomped              int64\n",
      "is_suspicious_execution     int64\n",
      "Case_ID                     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# --- Inspect first dataset as a sample ---\n",
    "sample_case = '01'\n",
    "df_sample = logfile_dataframes[sample_case]\n",
    "\n",
    "print(f\"Sample Dataset: Case {sample_case}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nShape: {df_sample.shape}\")\n",
    "print(f\"\\nColumn Names:\")\n",
    "print(df_sample.columns.tolist())\n",
    "print(f\"\\nData Types:\")\n",
    "print(df_sample.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4a574",
   "metadata": {},
   "source": [
    "# --- Preview first few rows ---\n",
    "print(f\"\\nFirst 5 rows of Case {sample_case}:\")\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a72d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values Summary:\n",
      "============================================================\n",
      "                     Missing_Count  Missing_Percentage\n",
      "detail                       26678           68.270338\n",
      "eventtime(utc+8)             22815           58.384728\n",
      "event                        19577           50.098523\n",
      "creationtime                 10897           27.885969\n",
      "modifiedtime                 10897           27.885969\n",
      "mftmodifiedtime              10897           27.885969\n",
      "accessedtime                 10897           27.885969\n",
      "full path                     4315           11.042301\n",
      "file/directory name            447            1.143895\n"
     ]
    }
   ],
   "source": [
    "# --- Check for missing values ---\n",
    "print(\"Missing Values Summary:\")\n",
    "print(\"=\" * 60)\n",
    "missing_summary = df_sample.isnull().sum()\n",
    "missing_pct = (missing_summary / len(df_sample)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_summary,\n",
    "    'Missing_Percentage': missing_pct\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f068fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution (Case 01):\n",
      "============================================================\n",
      "is_timestomped:\n",
      "is_timestomped\n",
      "0    39076\n",
      "1        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "is_suspicious_execution:\n",
      "is_suspicious_execution\n",
      "0    39076\n",
      "1        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Check label distribution ---\n",
    "print(\"\\nLabel Distribution (Case 01):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"is_timestomped:\")\n",
    "print(df_sample['is_timestomped'].value_counts())\n",
    "print(f\"\\nis_suspicious_execution:\")\n",
    "print(df_sample['is_suspicious_execution'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c03bda",
   "metadata": {},
   "source": [
    "## 4. Next Steps\n",
    "\n",
    "**To Do:**\n",
    "- [ ] Identify and drop irrelevant columns\n",
    "- [ ] Standardize timestamp columns to datetime format\n",
    "- [ ] Handle missing values (drop or impute)\n",
    "- [ ] Remove duplicate records\n",
    "- [ ] Merge all 12 LogFile datasets\n",
    "- [ ] Export Master_LogFile_Cleaned.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898a6cd",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning - Drop Rows with Null Event and Event Detail\n",
    "\n",
    "Remove rows where both `event` and `eventdetail` are null, as these records lack essential information for forensic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f9102f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: Dropping rows with null event AND detail...\n",
      "============================================================\n",
      "Case 01: 39,077 → 19,500 records (Dropped: 19,577 | 50.10%)\n",
      "Case 02: 14,783 → 9,694 records (Dropped: 5,089 | 34.42%)\n",
      "Case 03: 24,063 → 12,575 records (Dropped: 11,488 | 47.74%)\n",
      "Case 04: 12,731 → 7,713 records (Dropped: 5,018 | 39.42%)\n",
      "Case 05: 14,242 → 7,530 records (Dropped: 6,712 | 47.13%)\n",
      "Case 06: 14,030 → 7,426 records (Dropped: 6,604 | 47.07%)\n",
      "Case 07: 23,737 → 12,824 records (Dropped: 10,913 | 45.97%)\n",
      "Case 08: 23,379 → 12,397 records (Dropped: 10,982 | 46.97%)\n",
      "Case 09: 25,688 → 13,414 records (Dropped: 12,274 | 47.78%)\n",
      "Case 10: 23,932 → 12,929 records (Dropped: 11,003 | 45.98%)\n",
      "Case 11: 14,083 → 7,459 records (Dropped: 6,624 | 47.04%)\n",
      "Case 12: 14,139 → 7,770 records (Dropped: 6,369 | 45.05%)\n",
      "------------------------------------------------------------\n",
      "Total: 243,884 → 131,231 records\n",
      "Total Dropped: 112,653 (46.19%)\n",
      "\n",
      "Verification: Check if any rows still have null event AND detail\n",
      "============================================================\n",
      "Case 01: 0 rows with both null\n",
      "Case 02: 0 rows with both null\n",
      "Case 03: 0 rows with both null\n",
      "Case 04: 0 rows with both null\n",
      "Case 05: 0 rows with both null\n",
      "Case 06: 0 rows with both null\n",
      "Case 07: 0 rows with both null\n",
      "Case 08: 0 rows with both null\n",
      "Case 09: 0 rows with both null\n",
      "Case 10: 0 rows with both null\n",
      "Case 11: 0 rows with both null\n",
      "Case 12: 0 rows with both null\n",
      "\n",
      "✓ Cleaning verified!\n"
     ]
    }
   ],
   "source": [
    "# --- Drop rows where BOTH event AND detail are null ---\n",
    "print(\"Cleaning: Dropping rows with null event AND detail...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store original counts\n",
    "original_counts = {}\n",
    "cleaned_dataframes = {}\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    original_count = len(df)\n",
    "    original_counts[case_id] = original_count\n",
    "    \n",
    "    # Drop rows where BOTH event and detail are null\n",
    "    df_cleaned = df[~(df['event'].isnull() & df['detail'].isnull())].copy()\n",
    "    \n",
    "    dropped_count = original_count - len(df_cleaned)\n",
    "    dropped_pct = (dropped_count / original_count) * 100 if original_count > 0 else 0\n",
    "    \n",
    "    cleaned_dataframes[case_id] = df_cleaned\n",
    "    \n",
    "    print(f\"Case {case_id}: {original_count:,} → {len(df_cleaned):,} records \"\n",
    "          f\"(Dropped: {dropped_count:,} | {dropped_pct:.2f}%)\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calculate totals\n",
    "total_original = sum(original_counts.values())\n",
    "total_cleaned = sum(len(df) for df in cleaned_dataframes.values())\n",
    "total_dropped = total_original - total_cleaned\n",
    "total_dropped_pct = (total_dropped / total_original) * 100\n",
    "\n",
    "print(f\"Total: {total_original:,} → {total_cleaned:,} records\")\n",
    "print(f\"Total Dropped: {total_dropped:,} ({total_dropped_pct:.2f}%)\")\n",
    "\n",
    "# Update the main dictionary\n",
    "logfile_dataframes = cleaned_dataframes\n",
    "\n",
    "# --- Verify the cleaning ---\n",
    "print(\"\\nVerification: Check if any rows still have null event AND detail\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    null_both = df[df['event'].isnull() & df['detail'].isnull()]\n",
    "    print(f\"Case {case_id}: {len(null_both)} rows with both null\")\n",
    "\n",
    "print(\"\\n✓ Cleaning verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47b21d",
   "metadata": {},
   "source": [
    "## 6. Drop Irrelevant Columns\n",
    "\n",
    "Remove columns that are not relevant for timestomping detection:\n",
    "- `targetvcn`: Low-level NTFS virtual cluster number (physical storage location)\n",
    "- `clusterindex`: Cluster index information (disk fragmentation data)\n",
    "\n",
    "These columns are useful for data recovery and disk forensics, but do not contribute to timestamp manipulation analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e96e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping irrelevant columns: targetvcn, clusterindex\n",
      "============================================================\n",
      "Case 01: No columns to drop (already removed)\n",
      "Case 02: No columns to drop (already removed)\n",
      "Case 03: No columns to drop (already removed)\n",
      "Case 04: No columns to drop (already removed)\n",
      "Case 05: No columns to drop (already removed)\n",
      "Case 06: No columns to drop (already removed)\n",
      "Case 07: No columns to drop (already removed)\n",
      "Case 08: No columns to drop (already removed)\n",
      "Case 09: No columns to drop (already removed)\n",
      "Case 10: No columns to drop (already removed)\n",
      "Case 11: No columns to drop (already removed)\n",
      "Case 12: No columns to drop (already removed)\n",
      "------------------------------------------------------------\n",
      "✓ Irrelevant columns dropped successfully!\n",
      "\n",
      "Verification: Check remaining columns\n",
      "============================================================\n",
      "Remaining columns in Case 01: 16\n",
      "\n",
      "Column names:\n",
      "['lsn', 'eventtime(utc+8)', 'event', 'detail', 'file/directory name', 'full path', 'creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime', 'redo', 'target vcn', 'cluster index', 'is_timestomped', 'is_suspicious_execution', 'Case_ID']\n",
      "\n",
      "✓ Confirmed: targetvcn and clusterindex have been removed\n"
     ]
    }
   ],
   "source": [
    "# --- Drop irrelevant columns for timestomping detection ---\n",
    "print(\"Dropping irrelevant columns: targetvcn, clusterindex\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "columns_to_drop = ['targetvcn', 'clusterindex']\n",
    "\n",
    "for case_id, df in logfile_dataframes.items():\n",
    "    # Check which columns exist before dropping\n",
    "    existing_cols = [col for col in columns_to_drop if col in df.columns]\n",
    "    \n",
    "    if existing_cols:\n",
    "        df.drop(columns=existing_cols, inplace=True)\n",
    "        print(f\"Case {case_id}: Dropped {len(existing_cols)} columns - {existing_cols}\")\n",
    "    else:\n",
    "        print(f\"Case {case_id}: No columns to drop (already removed)\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"✓ Irrelevant columns dropped successfully!\")\n",
    "\n",
    "# --- Verify column removal ---\n",
    "print(\"\\nVerification: Check remaining columns\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_case = '01'\n",
    "df_sample = logfile_dataframes[sample_case]\n",
    "\n",
    "print(f\"Remaining columns in Case {sample_case}: {len(df_sample.columns)}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df_sample.columns.tolist())\n",
    "\n",
    "# Verify dropped columns are gone\n",
    "dropped_still_present = [col for col in columns_to_drop if col in df_sample.columns]\n",
    "if dropped_still_present:\n",
    "    print(f\"\\n⚠️ Warning: These columns still exist: {dropped_still_present}\")\n",
    "else:\n",
    "    print(f\"\\n✓ Confirmed: targetvcn and clusterindex have been removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cda0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
