{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb6ec09",
   "metadata": {},
   "source": [
    "# 2.1 Merging All Sub-Folders \n",
    "Now that we've combined the CSVs from LogFile and UsnJrnl according to their Case ID (01, 02, 03, ..., 12), we are now going to merge them all to make one master timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771f019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Phase 2.1: Comprehensive Aggregation ---\n",
      "\n",
      "Processing Case ID 01...\n",
      "  ✅ Loaded 325,420 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 02...\n",
      "  ✅ Loaded 251,933 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 03...\n",
      "  ✅ Loaded 250,718 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 04...\n",
      "  ✅ Loaded 266,559 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 05...\n",
      "  ✅ Loaded 273,890 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 06...\n",
      "  ✅ Loaded 267,499 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 07...\n",
      "  ✅ Loaded 253,532 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 08...\n",
      "  ✅ Loaded 254,103 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 09...\n",
      "  ✅ Loaded 255,627 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 10...\n",
      "  ✅ Loaded 255,255 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 11...\n",
      "  ✅ Loaded 267,443 rows. Case_ID column added.\n",
      "\n",
      "Processing Case ID 12...\n",
      "  ✅ Loaded 268,746 rows. Case_ID column added.\n",
      "\n",
      "--- Concatenating all individual cases into Master Timeline ---\n",
      "\n",
      "--- Row Count Verification ---\n",
      "Total rows loaded from all individual files: 3,190,725\n",
      "Total rows in final df_master_all:          3,190,725\n",
      "✅ QC PASS: The aggregation was successful. No rows were lost.\n",
      "\n",
      "--- Final Reordering, Sorting, and Export ---\n",
      "✅ Columns reordered: 'Case_ID' is now the first column.\n",
      "✅ Rows sorted by 'Case_ID' then 'timestamp_primary'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Configuration ---\n",
    "# Directory containing the 12 individual merged case files (01-PE-Merged.csv, etc.)\n",
    "INPUT_DIR = Path('data/processed/phase 2 - data merged') \n",
    "# New directory for the final, consolidated output\n",
    "OUTPUT_DIR = 'data/processed/phase 2.1 - data merged (all sub-folders)'\n",
    "OUTPUT_FILENAME = 'MASTER_TIMELINE_ALL_CASES.csv'\n",
    "OUTPUT_FILEPATH = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
    "\n",
    "# --- Aggregation Logic ---\n",
    "all_cases_data = []\n",
    "total_rows_input = 0\n",
    "case_numbers = range(1, 13) # Corresponds to 01 through 12\n",
    "\n",
    "print(\"--- Starting Phase 2.1: Comprehensive Aggregation ---\")\n",
    "\n",
    "for case_num in case_numbers:\n",
    "    # Format the case number to be 01, 02, 03, etc.\n",
    "    case_id = str(case_num).zfill(2)\n",
    "    filename = f'{case_id}-PE-Merged.csv'\n",
    "    filepath = INPUT_DIR / filename\n",
    "    \n",
    "    print(f\"\\nProcessing Case ID {case_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the individual merged file\n",
    "        # low_memory=False is essential for large files\n",
    "        df_case = pd.read_csv(filepath, low_memory=False)\n",
    "        \n",
    "        # --- CRUCIAL STEP: Add the Case Identifier ---\n",
    "        # This column is vital for later filtering and model evaluation\n",
    "        df_case['Case_ID'] = case_id \n",
    "        \n",
    "        rows_loaded = len(df_case)\n",
    "        total_rows_input += rows_loaded\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        all_cases_data.append(df_case)\n",
    "        \n",
    "        print(f\"  ✅ Loaded {rows_loaded:,} rows. Case_ID column added.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"  ❌ WARNING: File not found for Case ID {case_id} at: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ ERROR processing Case ID {case_id}: {e}\")\n",
    "\n",
    "# --- Final Concatenation ---\n",
    "print(\"\\n--- Concatenating all individual cases into Master Timeline ---\")\n",
    "\n",
    "if all_cases_data:\n",
    "    # Vertical merge all DataFrames in the list\n",
    "    df_master_all = pd.concat(all_cases_data, ignore_index=True)\n",
    "    \n",
    "    final_rows = len(df_master_all)\n",
    "\n",
    "    # --- Quality Control Check (QC) ---\n",
    "    print(\"\\n--- Row Count Verification ---\")\n",
    "    print(f\"Total rows loaded from all individual files: {total_rows_input:,}\")\n",
    "    print(f\"Total rows in final df_master_all:          {final_rows:,}\")\n",
    "\n",
    "    if total_rows_input == final_rows:\n",
    "        print(\"✅ QC PASS: The aggregation was successful. No rows were lost.\")\n",
    "    else:\n",
    "        print(\"❌ QC FAIL: Rows were lost during concatenation. Check the input count and concatenation logic.\")\n",
    "\n",
    "    print(\"\\n--- Final Reordering, Sorting, and Export ---\")\n",
    "    \n",
    "    # --- 1. Column Reordering: Move Case_ID to the first position ---\n",
    "    cols = df_master_all.columns.tolist()\n",
    "    # Ensure Case_ID is removed from its current position\n",
    "    if 'Case_ID' in cols:\n",
    "        cols.remove('Case_ID')\n",
    "        # Insert Case_ID at index 0 (the first position)\n",
    "        cols.insert(0, 'Case_ID')\n",
    "    \n",
    "    df_master_all = df_master_all[cols]\n",
    "    print(\"✅ Columns reordered: 'Case_ID' is now the first column.\")\n",
    "\n",
    "    # --- 2. Row Sorting: Sort by Case_ID then Chronologically ---\n",
    "    # Sorting by Case_ID first groups the data by case, making it easier to navigate.\n",
    "    df_master_all.sort_values(by=['Case_ID', 'timestamp_primary'], inplace=True)\n",
    "    print(\"✅ Rows sorted by 'Case_ID' then 'timestamp_primary'.\")\n",
    "    \n",
    "    # --- Ensure output directory exists ---\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Export the Master Timeline ---\n",
    "    df_master_all.to_csv(\n",
    "        OUTPUT_FILEPATH, \n",
    "        index=False, \n",
    "        encoding='utf-8', \n",
    "        date_format='%Y-%m-%d %H:%M:%S'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
