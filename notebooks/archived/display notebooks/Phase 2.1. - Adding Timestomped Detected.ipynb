{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9af0d5",
   "metadata": {},
   "source": [
    "# Phase 2.1 - Adding Timestomped Detected Columns \n",
    "From the merged LogFile and UsnJrnl, we now add another column that depicts if a certain row is timestomped or not based on the extracted results from Oh et. al's tool: NTFS Log Tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0709f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 3.2.5: Labeling Subfolder Timeline (01-PE-Merged.csv) ---\n",
      "1. Loading Timeline and Ground Truth...\n",
      "Detected Case ID: 1\n",
      "Timeline loaded with 325,420 records.\n",
      "Ground Truth loaded with 2 confirmed 'Timestamp Manipulation' records.\n",
      "Filtered Ground Truth Keys (Actionable Set Size): 2\n",
      "\n",
      "Successfully integrated labels. Total confirmed anomalies found: 1\n",
      "✅ Final labeled dataset saved to: data/processed/phase 2.1 - data labeled/01-PE-Labeled.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os # NEW: Import os for directory creation\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input 1: The intermediate merged timeline file for a single case.\n",
    "MERGED_INPUT_FILEPATH = Path('data/processed/phase 2 - data merged/01-PE-Merged.csv')\n",
    "\n",
    "# Input 2: The case-specific ground truth file.\n",
    "GROUND_TRUTH_FILEPATH = Path('data/raw/suspicious/01-PE-Suspicious.csv')\n",
    "\n",
    "# Output Directory: New directory as requested by the user\n",
    "NEW_OUTPUT_DIR = Path('data/processed/phase 2.1 - data labeled')\n",
    "\n",
    "# Output: The labeled file, ready for final master timeline aggregation.\n",
    "LABELED_OUTPUT_FILEPATH = NEW_OUTPUT_DIR / MERGED_INPUT_FILEPATH.name.replace('-Merged', '-Labeled')\n",
    "\n",
    "# --- Helper Function for ID Standardization (v12) ---\n",
    "\n",
    "def to_int_str(s):\n",
    "    \"\"\"\n",
    "    Converts a string (or float) ID into its clean decimal integer string representation. \n",
    "    Handles hexadecimal ('0x') prefixes, cleans up floating point notation,\n",
    "    and aggressively removes non-standard characters. Returns '' if invalid or NaN.\n",
    "    \"\"\"\n",
    "    if pd.isna(s) or (isinstance(s, str) and str(s).strip() == ''):\n",
    "        return ''\n",
    "    \n",
    "    # Handle direct numeric types (like floats/ints)\n",
    "    if isinstance(s, (int, float)):\n",
    "        return str(int(s))\n",
    "\n",
    "    s = str(s).strip()\n",
    "    \n",
    "    # Aggressively remove control characters or hidden punctuation.\n",
    "    s_clean = re.sub(r'[^\\w]', '', s).upper()\n",
    "        \n",
    "    if not s_clean:\n",
    "        return ''\n",
    "        \n",
    "    s = s_clean\n",
    "\n",
    "    try:\n",
    "        # Check for hex prefix and convert accordingly\n",
    "        if s.startswith('0X'):\n",
    "            return str(int(s, 16))\n",
    "        # Otherwise, treat as decimal\n",
    "        return str(int(s))\n",
    "    except ValueError:\n",
    "        return '' # Return empty string for unparseable values\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "def run_label_subfolder():\n",
    "    \"\"\"Main function to execute the case-specific ground truth integration.\"\"\"\n",
    "    print(f\"\\n--- Phase 3.2.5: Labeling Subfolder Timeline ({MERGED_INPUT_FILEPATH.name}) ---\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load DataFrames and Determine Case ID\n",
    "        print(\"1. Loading Timeline and Ground Truth...\")\n",
    "        \n",
    "        # Determine Case_ID from the filename (e.g., '01-PE-Merged.csv' -> '1')\n",
    "        file_prefix = MERGED_INPUT_FILEPATH.name.split('-')[0]\n",
    "        # Standardize Case ID format (e.g., '01' -> '1')\n",
    "        CASE_ID = str(int(file_prefix)) \n",
    "        print(f\"Detected Case ID: {CASE_ID}\")\n",
    "        \n",
    "        # Load the Merged Timeline\n",
    "        timeline_df = pd.read_csv(\n",
    "            MERGED_INPUT_FILEPATH, \n",
    "            dtype={'lsn': str, 'usn': str}, # LSN/USN must be strings\n",
    "            low_memory=False\n",
    "        )\n",
    "        print(f\"Timeline loaded with {len(timeline_df):,} records.\")\n",
    "        \n",
    "        # Add the necessary Case_ID column if it's missing (often happens in phase 2)\n",
    "        timeline_df['Case_ID'] = CASE_ID\n",
    "        \n",
    "        # Load the Ground Truth (specific to this case)\n",
    "        gt_df = pd.read_csv(GROUND_TRUTH_FILEPATH, low_memory=False)\n",
    "        \n",
    "        # 2. Prepare Ground Truth for Merging\n",
    "        # Robust Column Name Standardization for Ground Truth\n",
    "        gt_df.columns = gt_df.columns.str.strip().str.lower()\n",
    "        gt_df.rename(columns={'lsn/usn': 'id', 'source': 'source'}, inplace=True, errors='ignore')\n",
    "        \n",
    "        # Impute the missing Case_ID, as this GT file is case-specific\n",
    "        gt_df['Case_ID'] = CASE_ID\n",
    "        gt_df['Case_ID'] = gt_df['Case_ID'].astype(str).str.strip().apply(lambda x: str(int(x)) if str(x).isdigit() else x)\n",
    "        gt_df['id'] = gt_df['id'].astype(str).str.strip()\n",
    "        \n",
    "        # Filter for confirmed 'Timestamp Manipulation'\n",
    "        gt_df_filtered = gt_df[\n",
    "            gt_df['category'].str.contains('Timestamp Manipulation', case=False, na=False)\n",
    "        ].copy()\n",
    "        \n",
    "        gt_df_filtered['Is_Timestomped'] = 1\n",
    "\n",
    "        # Drop rows where the ID is missing\n",
    "        gt_keys = gt_df_filtered[['Case_ID', 'id']].dropna().drop_duplicates()\n",
    "        \n",
    "        print(f\"Ground Truth loaded with {len(gt_keys):,} confirmed 'Timestamp Manipulation' records.\")\n",
    "\n",
    "        # CRITICAL CLEANING: Standardize LSN/USN in both DataFrames\n",
    "        timeline_df['lsn'] = timeline_df['lsn'].apply(to_int_str)\n",
    "        timeline_df['usn'] = timeline_df['usn'].apply(to_int_str)\n",
    "        gt_keys['id'] = gt_keys['id'].apply(to_int_str)\n",
    "        \n",
    "        # 3. Create Merge Keys for Brute-Force Matching\n",
    "        \n",
    "        # Master Timeline Keys\n",
    "        timeline_df['Merge_Key_L'] = timeline_df['Case_ID'] + '_' + timeline_df['lsn'].fillna('')\n",
    "        timeline_df['Merge_Key_U'] = timeline_df['Case_ID'] + '_' + timeline_df['usn'].fillna('')\n",
    "        \n",
    "        # Ground Truth Keys (Brute-Force)\n",
    "        # Create one comprehensive set of keys from the GT: Case_ID + LSN/USN ID\n",
    "        gt_keys['Merge_Key_GT'] = gt_keys['Case_ID'] + '_' + gt_keys['id']\n",
    "        \n",
    "        # Final set of actionable keys (excluding keys with missing ID)\n",
    "        gt_match_set = {key for key in gt_keys['Merge_Key_GT'].unique() if not key.endswith('_')}\n",
    "\n",
    "        final_actionable_keys = len(gt_match_set)\n",
    "        print(f\"Filtered Ground Truth Keys (Actionable Set Size): {final_actionable_keys:,}\")\n",
    "\n",
    "        # 4. Optimized Labeling\n",
    "        timeline_df['Is_Timestomped'] = 0\n",
    "\n",
    "        # Match 1: Master LSN key is in the comprehensive GT match set\n",
    "        is_lsn_match = timeline_df['Merge_Key_L'].isin(gt_match_set)\n",
    "        # Match 2: Master USN key is in the comprehensive GT match set\n",
    "        is_usn_match = timeline_df['Merge_Key_U'].isin(gt_match_set)\n",
    "\n",
    "        # Flag the records: if EITHER the LSN key OR the USN key matches the GT key set, set Is_Timestomped to 1\n",
    "        timeline_df.loc[is_lsn_match | is_usn_match, 'Is_Timestomped'] = 1\n",
    "\n",
    "        # Drop temporary merge columns and the temporary Case_ID column before saving\n",
    "        timeline_df.drop(columns=['Merge_Key_L', 'Merge_Key_U'], inplace=True)\n",
    "        \n",
    "        # 5. Final Count and Save\n",
    "        total_anomalies = timeline_df['Is_Timestomped'].sum()\n",
    "        print(f\"\\nSuccessfully integrated labels. Total confirmed anomalies found: {total_anomalies:,}\")\n",
    "        \n",
    "        # CRITICAL ADDITION: Ensure the new output directory exists\n",
    "        os.makedirs(LABELED_OUTPUT_FILEPATH.parent, exist_ok=True)\n",
    "        \n",
    "        # Save the final labeled dataset\n",
    "        timeline_df.to_csv(LABELED_OUTPUT_FILEPATH, index=False)\n",
    "        print(f\"✅ Final labeled dataset saved to: {LABELED_OUTPUT_FILEPATH}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n❌ ERROR: One or more input files not found.\")\n",
    "        print(f\"Ensure the timeline exists at: {MERGED_INPUT_FILEPATH}\")\n",
    "        print(f\"Ensure the ground truth exists at: {GROUND_TRUTH_FILEPATH}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n❌ A data format error occurred during labeling: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An unexpected error occurred during labeling: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_label_subfolder()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
