{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749665c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 4 & 5: Anomaly Detection Pipeline ---\n",
      "Loading data from: data/processed/phase 3 - feature engineered/MASTER_TIMELINE_FEATURES.csv\n",
      "Data loaded successfully with 2,239,418 rows.\n",
      "Converting potential hex strings to decimal in columns: ['targetvcn', 'filereferencenumber', 'parentfilereferencenumber', 'lsn', 'usn']\n",
      "  Hex conversion complete.\n",
      "\n",
      "1. Dropping non-feature columns (IDs, raw timestamps, path text)...\n",
      "\n",
      "2. Applying Frequency Encoding to categorical features...\n",
      "  Encoded and dropped original column: 'event'\n",
      "  Encoded and dropped original column: 'source'\n",
      "\n",
      "Final Feature Matrix (X) shape: (2239418, 15)\n",
      "\n",
      "3. Applying Standard Scaling...\n",
      "\n",
      "❌ An unexpected error occurred during processing: could not convert string to float: 'Archive / Repasre_Point / Sparse'\n"
     ]
    }
   ],
   "source": [
    "# Phase 4 & 5: Final Feature Preparation, Scaling, and Isolation Forest Training\n",
    "# FIX: Includes robust conversion of hexadecimal strings (e.g., '0x393') to decimal \n",
    "# integers for columns like 'targetvcn' to prevent the ValueError during scaling.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set display options for better data visibility\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 30)\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = 'data/processed/phase 3 - feature engineered'\n",
    "INPUT_FILENAME = 'MASTER_TIMELINE_FEATURES.csv'\n",
    "INPUT_FILEPATH = Path(INPUT_DIR) / INPUT_FILENAME\n",
    "\n",
    "# Model Configuration\n",
    "CONTAMINATION_RATE = 0.01 \n",
    "\n",
    "# Columns that are NOT features and should be dropped (IDs, raw timestamps, unused text)\n",
    "COLUMNS_TO_DROP = [\n",
    "    'Case_ID', \n",
    "    'creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime', 'timestamp_primary',\n",
    "    'fullpath', 'filedirectoryname', 'eventinfo', 'redo' \n",
    "]\n",
    "\n",
    "# Categorical columns that need Frequency Encoding\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'event', \n",
    "    'source'\n",
    "]\n",
    "\n",
    "# Columns that are expected to be numeric but often contain hex strings\n",
    "HEX_TO_NUMERIC_COLS = [\n",
    "    'targetvcn', \n",
    "    'filereferencenumber', \n",
    "    'parentfilereferencenumber',\n",
    "    'lsn',\n",
    "    'usn'\n",
    "]\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"Loads the CSV and converts necessary columns to datetime objects.\"\"\"\n",
    "    print(f\"Loading data from: {filepath}\")\n",
    "    df = pd.read_csv(filepath, dtype={'Case_ID': str}, low_memory=False)\n",
    "    time_cols = ['timestamp_primary', 'creationtime', 'modifiedtime', 'mftmodifiedtime', 'accessedtime']\n",
    "    for col in time_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    print(f\"Data loaded successfully with {len(df):,} rows.\")\n",
    "    return df\n",
    "\n",
    "def convert_hex_to_decimal(df, cols):\n",
    "    \"\"\"Converts columns containing hex strings (like '0x393') to decimal integers.\"\"\"\n",
    "    print(f\"Converting potential hex strings to decimal in columns: {cols}\")\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    for col in cols:\n",
    "        if col in df_temp.columns:\n",
    "            # First, fill NaNs with 0 to make conversion simpler\n",
    "            df_temp[col] = df_temp[col].fillna(0)\n",
    "            \n",
    "            # Apply conversion logic\n",
    "            def hex_to_dec(val):\n",
    "                if isinstance(val, (int, float)):\n",
    "                    # Already numeric\n",
    "                    return val\n",
    "                try:\n",
    "                    s = str(val).strip().upper()\n",
    "                    if s.startswith('0X'):\n",
    "                        return int(s, 16)\n",
    "                    return float(s) # Tries to convert non-hex string to float\n",
    "                except ValueError:\n",
    "                    return 0 # Default to 0 if conversion fails\n",
    "                except TypeError:\n",
    "                    return 0\n",
    "\n",
    "            df_temp[col] = df_temp[col].apply(hex_to_dec).astype(float)\n",
    "            \n",
    "    print(\"  Hex conversion complete.\")\n",
    "    return df_temp\n",
    "\n",
    "def preprocess_features(df):\n",
    "    \"\"\"Executes the final feature preparation steps: column dropping, encoding, and cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Pre-step: Convert hex strings to decimal where necessary\n",
    "    df_processed = convert_hex_to_decimal(df_processed, HEX_TO_NUMERIC_COLS)\n",
    "    \n",
    "    # 1. Drop Non-Feature Columns\n",
    "    print(\"\\n1. Dropping non-feature columns (IDs, raw timestamps, path text)...\")\n",
    "    df_processed = df_processed.drop(columns=[col for col in COLUMNS_TO_DROP if col in df_processed.columns])\n",
    "    \n",
    "    # 2. Frequency Encoding\n",
    "    print(\"\\n2. Applying Frequency Encoding to categorical features...\")\n",
    "    for col in CATEGORICAL_FEATURES:\n",
    "        if col in df_processed.columns:\n",
    "            freq_map = df_processed[col].value_counts(normalize=True).to_dict()\n",
    "            df_processed[f'{col}_freq_encoded'] = df_processed[col].map(freq_map).fillna(0)\n",
    "            df_processed = df_processed.drop(columns=[col])\n",
    "            print(f\"  Encoded and dropped original column: '{col}'\")\n",
    "\n",
    "    # 3. Handle remaining NaNs in numeric columns (Impute with 0)\n",
    "    numeric_cols = df_processed.select_dtypes(include=np.number).columns\n",
    "    df_processed[numeric_cols] = df_processed[numeric_cols].fillna(0)\n",
    "    \n",
    "    # Identify the final feature matrix X and the key columns\n",
    "    key_cols = ['lsn', 'usn']\n",
    "    X = df_processed.drop(columns=[col for col in key_cols if col in df_processed.columns], errors='ignore')\n",
    "    \n",
    "    print(f\"\\nFinal Feature Matrix (X) shape: {X.shape}\")\n",
    "    return X, df_processed[key_cols].fillna(np.nan).astype(str)\n",
    "\n",
    "def train_and_score_isolation_forest(X, keys):\n",
    "    \"\"\"Applies scaling, trains the IF model, and generates anomaly scores.\"\"\"\n",
    "\n",
    "    # 1. Feature Scaling\n",
    "    print(\"\\n3. Applying Standard Scaling...\")\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the data and transform it\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "    # 2. Isolation Forest Training\n",
    "    print(f\"\\n4. Training Isolation Forest with contamination rate: {CONTAMINATION_RATE}...\")\n",
    "    model = IsolationForest(\n",
    "        n_estimators=100, \n",
    "        contamination=CONTAMINATION_RATE, \n",
    "        random_state=42,\n",
    "        max_samples='auto',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    model.fit(X_scaled)\n",
    "    print(\"  Isolation Forest training complete.\")\n",
    "\n",
    "    # 3. Generate Anomaly Scores and Predictions\n",
    "    X_scaled_df['anomaly_score'] = model.decision_function(X_scaled)\n",
    "    X_scaled_df['anomaly_label'] = model.predict(X_scaled) # -1 is anomaly, 1 is normal\n",
    "    \n",
    "    # Merge keys back for identification\n",
    "    X_scaled_df = pd.concat([keys.reset_index(drop=True), X_scaled_df], axis=1)\n",
    "\n",
    "    return X_scaled_df\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Phase 4 & 5: Anomaly Detection Pipeline ---\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load Data\n",
    "        df_master = load_data(INPUT_FILEPATH)\n",
    "\n",
    "        # 2. Preprocess Features (Fixed Hex Conversion, Drop metadata and Encode)\n",
    "        X_features, key_identifiers = preprocess_features(df_master)\n",
    "        \n",
    "        # 3. Train and Score Model\n",
    "        results_df = train_and_score_isolation_forest(X_features, key_identifiers)\n",
    "        \n",
    "        # 4. Display Final Results\n",
    "        anomalies_detected = (results_df['anomaly_label'] == -1).sum()\n",
    "        print(\"\\n--- Results Summary ---\")\n",
    "        print(f\"Total records analyzed: {len(results_df):,}\")\n",
    "        print(f\"Records classified as anomalies (-1) based on contamination={CONTAMINATION_RATE}: {anomalies_detected:,}\")\n",
    "        \n",
    "        print(\"\\nTop 5 Most Anomalous Records (Lowest Score):\")\n",
    "        top_anomalies = results_df.sort_values(by='anomaly_score', ascending=True).head(5)\n",
    "        # Display the key identifiers, score, and the delta features that likely caused the anomaly\n",
    "        print(top_anomalies[['lsn', 'usn', 'anomaly_score', 'anomaly_label', 'Delta_M_vs_C', 'Delta_MFTM_vs_M', 'event_freq_encoded']])\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n❌ ERROR: Input file not found. Ensure '{INPUT_FILEPATH}' exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An unexpected error occurred during processing: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
